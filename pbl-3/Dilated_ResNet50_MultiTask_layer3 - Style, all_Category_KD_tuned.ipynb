{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2c261d0-8022-4151-9f41-246cbca67d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Dilated_ResNet50_MultiTask_layer3 - Style, all_Category_KD</strong> at: <a href='https://wandb.ai/hh0804352-hanyang-university/office-home-classification/runs/vice484n' target=\"_blank\">https://wandb.ai/hh0804352-hanyang-university/office-home-classification/runs/vice484n</a><br> View project at: <a href='https://wandb.ai/hh0804352-hanyang-university/office-home-classification' target=\"_blank\">https://wandb.ai/hh0804352-hanyang-university/office-home-classification</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250528_060042-vice484n/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# WandB 설정 \u001b[39;00m\n\u001b[1;32m     26\u001b[0m wandb\u001b[38;5;241m.\u001b[39mlogin(key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mef091b9abcea3186341ddf8995d62bde62d7469e\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 27\u001b[0m \u001b[43mwandb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproject\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moffice-home-classification\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDilated_ResNet50_MultiTask_layer3 - Style, all_Category_KD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mentity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhh0804352-hanyang-university\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     31\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# wandb run name을 체크포인트 경로에 사용\u001b[39;00m\n\u001b[1;32m     34\u001b[0m run_name \u001b[38;5;241m=\u001b[39m wandb\u001b[38;5;241m.\u001b[39mrun\u001b[38;5;241m.\u001b[39mname\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/wandb/sdk/wandb_init.py:1530\u001b[0m, in \u001b[0;36minit\u001b[0;34m(entity, project, dir, id, name, notes, tags, config, config_exclude_keys, config_include_keys, allow_val_change, group, job_type, mode, force, anonymous, reinit, resume, resume_from, fork_from, save_code, tensorboard, sync_tensorboard, monitor_gym, settings)\u001b[0m\n\u001b[1;32m   1527\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_settings\u001b[38;5;241m.\u001b[39mx_server_side_derived_summary:\n\u001b[1;32m   1528\u001b[0m             init_telemetry\u001b[38;5;241m.\u001b[39mfeature\u001b[38;5;241m.\u001b[39mserver_side_derived_summary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m-> 1530\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_settings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1532\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1533\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m wl:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/wandb/sdk/wandb_init.py:855\u001b[0m, in \u001b[0;36m_WandbInit.init\u001b[0;34m(self, settings, config)\u001b[0m\n\u001b[1;32m    852\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m settings\u001b[38;5;241m.\u001b[39mlaunch:\n\u001b[1;32m    853\u001b[0m     tel\u001b[38;5;241m.\u001b[39mfeature\u001b[38;5;241m.\u001b[39mlaunch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 855\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module_name \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtelemetry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist_telemetry_imports\u001b[49m\u001b[43m(\u001b[49m\u001b[43monly_imported\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m:\n\u001b[1;32m    856\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(tel\u001b[38;5;241m.\u001b[39mimports_init, module_name, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    858\u001b[0m \u001b[38;5;66;03m# probe the active start method\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/wandb/sdk/lib/telemetry.py:90\u001b[0m, in \u001b[0;36mlist_telemetry_imports\u001b[0;34m(only_imported)\u001b[0m\n\u001b[1;32m     84\u001b[0m import_telemetry_set \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     85\u001b[0m     desc\u001b[38;5;241m.\u001b[39mname\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m desc \u001b[38;5;129;01min\u001b[39;00m TelemetryImports\u001b[38;5;241m.\u001b[39mDESCRIPTOR\u001b[38;5;241m.\u001b[39mfields\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m desc\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m desc\u001b[38;5;241m.\u001b[39mTYPE_BOOL\n\u001b[1;32m     88\u001b[0m }\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m only_imported:\n\u001b[0;32m---> 90\u001b[0m     imported_modules_set \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(sys\u001b[38;5;241m.\u001b[39mmodules)\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m imported_modules_set\u001b[38;5;241m.\u001b[39mintersection(import_telemetry_set)\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m import_telemetry_set\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchmetrics\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "from utility.utils import DualMAPEarlyStopping\n",
    "from torchmetrics.classification import MulticlassAveragePrecision\n",
    "from Dataset.data import OfficeHomeDataset\n",
    "from models.resnet_dilated import ResnetDilated\n",
    "import timm\n",
    "from models.efficientnet_teacher import EfficientNetTeacher, MultiTaskKDLoss, pretrain_teacher, train_with_kd\n",
    "\n",
    "# 체크포인트 디렉토리 생성\n",
    "os.makedirs('checkpoints', exist_ok=True)\n",
    "\n",
    "# WandB 설정 \n",
    "wandb.login(key=\"ef091b9abcea3186341ddf8995d62bde62d7469e\")\n",
    "wandb.init(\n",
    "    project=\"office-home-classification\", \n",
    "    name=\"Dilated_ResNet50_MultiTask_layer3 - Style, all_Category_KD\",\n",
    "    entity=\"hh0804352-hanyang-university\"\n",
    ")\n",
    "\n",
    "# wandb run name을 체크포인트 경로에 사용\n",
    "run_name = wandb.run.name\n",
    "CHECKPOINT_PATH = os.path.join('checkpoints', f'{run_name}_checkpoint.pth')\n",
    "\n",
    "# 설정\n",
    "config = {\n",
    "    # 기존 설정 유지\n",
    "    \"model\": \"resnet50\",\n",
    "    \"batch_size\": 128,\n",
    "    \"num_epochs\": 300,\n",
    "    \"learning_rate\": 0.0001,\n",
    "    \"optimizer\": \"Adam\",\n",
    "    \"seed\": 2025,\n",
    "    \"deterministic\": False,\n",
    "    \"patience\": 20,\n",
    "    \"max_epochs_wait\": float('inf'),\n",
    "    \"num_domains\": 4,\n",
    "    \"num_classes\": 65,\n",
    "    \n",
    "    # KD 설정\n",
    "    \"domain_weight\": 0.2,\n",
    "    \"class_weight\": 0.8,\n",
    "    \n",
    "    # Teacher 설정\n",
    "    \"teacher_epochs\": 20,\n",
    "    \n",
    "    # 기존 설정들\n",
    "    \"num_workers\": 20,\n",
    "    \"pin_memory\": True,\n",
    "    \"save_every\": 5,\n",
    "    \"scheduler\": \"ReduceLROnPlateau\",\n",
    "    \"scheduler_mode\": \"max\",\n",
    "    \"scheduler_factor\": 0.1,\n",
    "    \"scheduler_patience\": 5,\n",
    "    \"scheduler_verbose\": True,\n",
    "}\n",
    "\n",
    "wandb.config.update(config)\n",
    "\n",
    "class ImprovedMultiTaskModel(nn.Module):\n",
    "    def __init__(self, num_domains=4, num_classes=65):\n",
    "        super(ImprovedMultiTaskModel, self).__init__()\n",
    "        \n",
    "        # 공유 백본 - ResnetDilated 사용\n",
    "        pretrained_resnet = models.resnet50(pretrained=True)\n",
    "        self.backbone = ResnetDilated(pretrained_resnet, dilate_scale=8)\n",
    "        \n",
    "        # 백본 동결 (선택적)\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        # 도메인 분류를 위한 분기점 (layer3 출력에서)\n",
    "        self.domain_branch = nn.Sequential(\n",
    "            nn.Conv2d(1024, 128, kernel_size=1),\n",
    "            nn.GroupNorm(8, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        \n",
    "        # 클래스 분류를 위한 분기점 (layer4 출력에서)\n",
    "        self.class_branch = nn.Sequential(\n",
    "            nn.Conv2d(2048, 256, kernel_size=1),\n",
    "            nn.GroupNorm(8, 256),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # 풀링 레이어\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        # 분류 헤드\n",
    "        self.domain_classifier = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.7),\n",
    "            nn.Linear(64, num_domains)\n",
    "        )\n",
    "        \n",
    "        self.class_classifier = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.7),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 백본의 각 스테이지 출력 추출\n",
    "        x = self.backbone.conv1(x)\n",
    "        x = self.backbone.bn1(x)\n",
    "        x = self.backbone.relu1(x)\n",
    "        x = self.backbone.maxpool(x)\n",
    "        \n",
    "        x = self.backbone.layer1(x)\n",
    "        x = self.backbone.layer2(x)\n",
    "        \n",
    "        # Layer3 출력 저장 (도메인 분류용)\n",
    "        layer3_output = self.backbone.layer3(x)\n",
    "        \n",
    "        # Layer4 출력 (클래스 분류용)\n",
    "        layer4_output = self.backbone.layer4(layer3_output)\n",
    "        \n",
    "        # 도메인 분류 경로\n",
    "        domain_features = self.domain_branch(layer3_output)\n",
    "        domain_features = self.avgpool(domain_features)\n",
    "        domain_features = torch.flatten(domain_features, 1)\n",
    "        domain_out = self.domain_classifier(domain_features)\n",
    "        \n",
    "        # 클래스 분류 경로\n",
    "        class_features = self.class_branch(layer4_output)\n",
    "        class_features = self.avgpool(class_features)\n",
    "        class_features = torch.flatten(class_features, 1)\n",
    "        class_out = self.class_classifier(class_features)\n",
    "        \n",
    "        return domain_out, class_out\n",
    "\n",
    "def save_kd_checkpoint(student, teacher, optimizer, scheduler, epoch, best_class_map, best_domain_map, early_stopping, filename=\"checkpoint_kd.pt\"):\n",
    "    \"\"\"KD 훈련 체크포인트 저장 함수\"\"\"\n",
    "    # 모델 상태 추출\n",
    "    student_state_dict = student.module.state_dict() if isinstance(student, nn.DataParallel) else student.state_dict()\n",
    "    teacher_state_dict = teacher.module.state_dict() if isinstance(teacher, nn.DataParallel) else teacher.state_dict()\n",
    "    \n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'student_state_dict': student_state_dict,\n",
    "        'teacher_state_dict': teacher_state_dict,\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'best_class_map': best_class_map,\n",
    "        'best_domain_map': best_domain_map,\n",
    "        'early_stopping_counter': early_stopping.counter,\n",
    "        'early_stopping_domain_best_score': early_stopping.domain_best_score,\n",
    "        'early_stopping_class_best_score': early_stopping.class_best_score,\n",
    "        'early_stopping_best_epoch': early_stopping.best_epoch,\n",
    "        'early_stopping_domain_best_epoch': early_stopping.domain_best_epoch,\n",
    "        'early_stopping_class_best_epoch': early_stopping.class_best_epoch,\n",
    "        'early_stopping_early_stop': early_stopping.early_stop,\n",
    "        'early_stopping_domain_map_max': early_stopping.domain_map_max,\n",
    "        'early_stopping_class_map_max': early_stopping.class_map_max,\n",
    "        'config': config,\n",
    "    }\n",
    "    torch.save(checkpoint, filename)\n",
    "    print(f\"KD 체크포인트가 {filename}에 저장되었습니다.\")\n",
    "\n",
    "def evaluate(model, dataloader, domain_criterion, class_criterion, device, epoch):\n",
    "    \"\"\"멀티태스크 평가 함수\"\"\"\n",
    "    model.eval()\n",
    "    start_time = time.time()\n",
    "    running_loss = 0.0\n",
    "    running_domain_loss = 0.0\n",
    "    running_class_loss = 0.0\n",
    "    domain_correct = 0\n",
    "    class_correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # mAP 계산기 초기화\n",
    "    domain_map = MulticlassAveragePrecision(num_classes=config[\"num_domains\"], average='macro')\n",
    "    class_map = MulticlassAveragePrecision(num_classes=config[\"num_classes\"], average='macro')\n",
    "    \n",
    "    domain_map = domain_map.to(device)\n",
    "    class_map = class_map.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, domain_labels, class_labels in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            domain_labels = domain_labels.to(device)\n",
    "            class_labels = class_labels.to(device)\n",
    "            \n",
    "            # 순전파\n",
    "            domain_outputs, class_outputs = model(inputs)\n",
    "            \n",
    "            # 손실 계산\n",
    "            domain_loss = domain_criterion(domain_outputs, domain_labels)\n",
    "            class_loss = class_criterion(class_outputs, class_labels)\n",
    "            loss = config[\"domain_weight\"] * domain_loss + config[\"class_weight\"] * class_loss\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            running_domain_loss += domain_loss.item()\n",
    "            running_class_loss += class_loss.item()\n",
    "            \n",
    "            # 정확도 계산\n",
    "            _, domain_preds = domain_outputs.max(1)\n",
    "            domain_correct += domain_preds.eq(domain_labels).sum().item()\n",
    "            \n",
    "            _, class_preds = class_outputs.max(1)\n",
    "            class_correct += class_preds.eq(class_labels).sum().item()\n",
    "            \n",
    "            total += inputs.size(0)\n",
    "            \n",
    "            # mAP 업데이트\n",
    "            domain_map.update(domain_outputs, domain_labels)\n",
    "            class_map.update(class_outputs, class_labels)\n",
    "    \n",
    "    # 평균 손실 및 정확도 계산\n",
    "    eval_loss = running_loss / len(dataloader)\n",
    "    eval_domain_loss = running_domain_loss / len(dataloader)\n",
    "    eval_class_loss = running_class_loss / len(dataloader)\n",
    "    domain_accuracy = 100.0 * domain_correct / total\n",
    "    class_accuracy = 100.0 * class_correct / total\n",
    "    \n",
    "    # mAP 계산\n",
    "    domain_map_value = domain_map.compute().item()\n",
    "    class_map_value = class_map.compute().item()\n",
    "    \n",
    "    eval_time = time.time() - start_time\n",
    "    \n",
    "    print(f'Test set: Epoch: {epoch+1}, Avg loss: {eval_loss:.4f}, '\n",
    "          f'Domain Loss: {eval_domain_loss:.4f}, Class Loss: {eval_class_loss:.4f}, '\n",
    "          f'Domain Acc: {domain_accuracy:.2f}%, Class Acc: {class_accuracy:.2f}%, '\n",
    "          f'Domain mAP: {domain_map_value:.4f}, Class mAP: {class_map_value:.4f}, '\n",
    "          f'Time: {eval_time:.2f}s')\n",
    "    print()\n",
    "    \n",
    "    return eval_loss, eval_domain_loss, eval_class_loss, domain_accuracy, class_accuracy, domain_map_value, class_map_value\n",
    "\n",
    "# 데이터 변환\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((255,255)),\n",
    "    transforms.RandomCrop((224,224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# 메인 실행 코드\n",
    "if __name__ == \"__main__\":\n",
    "    # 시드 설정\n",
    "    seed = config[\"seed\"]\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "    # 결정적 알고리즘 사용 여부\n",
    "    if config[\"deterministic\"]:\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "    else:\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "    # 디바이스 설정\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # 데이터셋 로드\n",
    "    trainset = OfficeHomeDataset(root_dir='./Dataset/train', transform=transform_train)\n",
    "    testset = OfficeHomeDataset(root_dir='./Dataset/test', transform=transform_test)\n",
    "\n",
    "    # DataLoader 생성\n",
    "    trainloader = DataLoader(\n",
    "        trainset, \n",
    "        batch_size=config[\"batch_size\"], \n",
    "        shuffle=True, \n",
    "        pin_memory=config[\"pin_memory\"], \n",
    "        num_workers=config[\"num_workers\"]\n",
    "    )\n",
    "\n",
    "    testloader = DataLoader(\n",
    "        testset, \n",
    "        batch_size=config[\"batch_size\"], \n",
    "        shuffle=False, \n",
    "        pin_memory=config[\"pin_memory\"], \n",
    "        num_workers=config[\"num_workers\"]\n",
    "    )\n",
    "\n",
    "    print(f\"Train set size: {len(trainset)}\")\n",
    "    print(f\"Test set size: {len(testset)}\")\n",
    "\n",
    "    # Teacher 모델 로드\n",
    "    teacher = EfficientNetTeacher(\n",
    "        num_domains=config[\"num_domains\"],\n",
    "        num_classes=config[\"num_classes\"]\n",
    "    ).to(device)\n",
    "\n",
    "    teacher_checkpoint_path = \"efficientnet_teacher_best.pth\"\n",
    "    if os.path.exists(teacher_checkpoint_path):\n",
    "        print(f\" Teacher 모델 로드: {teacher_checkpoint_path}\")\n",
    "        teacher.load_state_dict(torch.load(teacher_checkpoint_path, map_location=device))\n",
    "    else:\n",
    "        print(\" Teacher 모델이 없습니다. Teacher를 훈련합니다...\")\n",
    "        teacher = pretrain_teacher(teacher, trainloader, testloader, device, epochs=config[\"teacher_epochs\"])\n",
    "\n",
    "    # Student 모델 생성\n",
    "    student = ImprovedMultiTaskModel(\n",
    "        num_domains=config[\"num_domains\"], \n",
    "        num_classes=config[\"num_classes\"]\n",
    "    ).to(device)\n",
    "\n",
    "    # KD Loss\n",
    "    kd_loss_fn = MultiTaskKDLoss(\n",
    "        domain_alpha=0.8,\n",
    "        class_alpha=0.6,\n",
    "        domain_temp=5.0,\n",
    "        class_temp=3.0\n",
    "    )\n",
    "    \n",
    "    # 옵티마이저 및 스케줄러\n",
    "    optimizer = optim.Adam([\n",
    "        {'params': student.domain_branch.parameters(), 'lr': config[\"learning_rate\"]},\n",
    "        {'params': student.class_branch.parameters(), 'lr': config[\"learning_rate\"]},\n",
    "        {'params': student.domain_classifier.parameters(), 'lr': config[\"learning_rate\"]},\n",
    "        {'params': student.class_classifier.parameters(), 'lr': config[\"learning_rate\"]}\n",
    "    ])\n",
    "\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, \n",
    "        mode=config[\"scheduler_mode\"], \n",
    "        factor=config[\"scheduler_factor\"], \n",
    "        patience=config[\"scheduler_patience\"], \n",
    "        verbose=config[\"scheduler_verbose\"]\n",
    "    )\n",
    "\n",
    "    # GPU 병렬 처리\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(f\"{torch.cuda.device_count()}개의 GPU를 사용합니다.\")\n",
    "        student = nn.DataParallel(student)\n",
    "        teacher = nn.DataParallel(teacher)\n",
    "\n",
    "    # Early stopping 초기화\n",
    "    early_stopping = DualMAPEarlyStopping(\n",
    "        patience=config[\"patience\"], \n",
    "        verbose=True, \n",
    "        path='checkpoint_kd.pt', \n",
    "        max_epochs=config[\"max_epochs_wait\"]\n",
    "    )\n",
    "\n",
    "    # 학습 시작 (체크포인트 로드 없이 처음부터)\n",
    "    start_epoch = 0\n",
    "    best_class_map = 0.0\n",
    "    best_domain_map = 0.0\n",
    "\n",
    "    # WandB watch\n",
    "    wandb.watch(student, log=\"all\")\n",
    "    \n",
    "    print(f\"Knowledge Distillation !\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        for epoch in range(start_epoch, config[\"num_epochs\"]):\n",
    "            print(f\"\\n 에폭 {epoch+1}/{config['num_epochs']} 시작\")\n",
    "            \n",
    "            # KD 훈련\n",
    "            train_results = train_with_kd(\n",
    "                student, teacher, trainloader, kd_loss_fn, optimizer, device, epoch, config\n",
    "            )\n",
    "            \n",
    "            train_loss, train_domain_loss, train_class_loss, train_domain_acc, train_class_acc, train_domain_map, train_class_map, detailed_losses = train_results\n",
    "            \n",
    "            # 평가\n",
    "            test_loss, test_domain_loss, test_class_loss, test_domain_acc, test_class_acc, test_domain_map, test_class_map = evaluate(\n",
    "                student, testloader, nn.CrossEntropyLoss(), nn.CrossEntropyLoss(), device, epoch\n",
    "            )\n",
    "            \n",
    "            # 스케줄러 업데이트\n",
    "            avg_map = (test_domain_map + test_class_map) / 2\n",
    "            scheduler.step(avg_map)\n",
    "            \n",
    "            # WandB 로깅\n",
    "            wandb.log({\n",
    "                \"epoch\": epoch + 1,\n",
    "                \"learning_rate\": optimizer.param_groups[0]['lr'],\n",
    "                \"train_loss\": train_loss,\n",
    "                \"train_domain_accuracy\": train_domain_acc,\n",
    "                \"train_class_accuracy\": train_class_acc,\n",
    "                \"train_domain_map\": train_domain_map,\n",
    "                \"train_class_map\": train_class_map,\n",
    "                \"test_loss\": test_loss,\n",
    "                \"test_domain_accuracy\": test_domain_acc,\n",
    "                \"test_class_accuracy\": test_class_acc,\n",
    "                \"test_domain_map\": test_domain_map,\n",
    "                \"test_class_map\": test_class_map,\n",
    "                \"kd_domain_kd_loss\": detailed_losses['domain_kd'],\n",
    "                \"kd_domain_hard_loss\": detailed_losses['domain_hard'],\n",
    "                \"kd_class_kd_loss\": detailed_losses['class_kd'],\n",
    "                \"kd_class_hard_loss\": detailed_losses['class_hard'],\n",
    "            })\n",
    "            \n",
    "            # 최고 성능 모델 저장\n",
    "            if test_class_map > best_class_map:\n",
    "                best_class_map = test_class_map\n",
    "                model_to_save = student.module if isinstance(student, nn.DataParallel) else student\n",
    "                torch.save(model_to_save.state_dict(), f'best_student_class_kd_{wandb.run.name}.pth')\n",
    "                print(f' 새로운 최고 Class mAP: {best_class_map:.4f}')\n",
    "            \n",
    "            if test_domain_map > best_domain_map:\n",
    "                best_domain_map = test_domain_map\n",
    "                model_to_save = student.module if isinstance(student, nn.DataParallel) else student\n",
    "                torch.save(model_to_save.state_dict(), f'best_student_domain_kd_{wandb.run.name}.pth')\n",
    "                print(f' 새로운 최고 Domain mAP: {best_domain_map:.4f}')\n",
    "        \n",
    "            # 체크포인트 주기적 저장\n",
    "            if (epoch + 1) % config[\"save_every\"] == 0:\n",
    "                save_kd_checkpoint(student, teacher, optimizer, scheduler, epoch, best_class_map, best_domain_map, early_stopping)\n",
    "            \n",
    "            # Early stopping\n",
    "            early_stopping(test_domain_map, test_class_map, student, epoch)\n",
    "            if early_stopping.early_stop:\n",
    "                print(f\" KD 훈련 조기 종료 - 에폭 {epoch+1}\")\n",
    "                break\n",
    "        \n",
    "        print(f' KD 훈련 완료!')\n",
    "        print(f'최고 Class mAP: {best_class_map:.4f}')\n",
    "        print(f'최고 Domain mAP: {best_domain_map:.4f}')\n",
    "        \n",
    "        # WandB 최종 결과 기록\n",
    "        wandb.run.summary[\"best_class_map_kd\"] = best_class_map\n",
    "        wandb.run.summary[\"best_domain_map_kd\"] = best_domain_map\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"KD 훈련이 중단되었습니다. 체크포인트를 저장합니다.\")\n",
    "        save_kd_checkpoint(student, teacher, optimizer, scheduler, epoch, best_class_map, best_domain_map, early_stopping)\n",
    "    except Exception as e:\n",
    "        print(f\"훈련 중 오류 발생: {e}\")\n",
    "        print(\"체크포인트를 저장하고 종료합니다.\")\n",
    "        save_kd_checkpoint(student, teacher, optimizer, scheduler, epoch if 'epoch' in locals() else start_epoch, \n",
    "                          best_class_map, best_domain_map, early_stopping)\n",
    "    \n",
    "    # 최종 체크포인트 저장\n",
    "    save_kd_checkpoint(student, teacher, optimizer, scheduler, epoch, best_class_map, best_domain_map, early_stopping, \n",
    "                      filename=\"final_checkpoint_kd.pt\")\n",
    "    \n",
    "    # 훈련 종료 시간\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    wandb.log({\"total_training_time\": total_time})\n",
    "    print(f\"전체 학습 시간: {total_time:.2f} 초\")\n",
    "    \n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7d0f34-01c7-4f72-9853-140d1528e1e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
