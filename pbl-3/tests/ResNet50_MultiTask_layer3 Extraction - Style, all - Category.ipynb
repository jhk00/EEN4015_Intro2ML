{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c261d0-8022-4151-9f41-246cbca67d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/guswls/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/guswls/EEN4015_Intro2ML/pbl-3/wandb/run-20250517_062631-qgzwk4ub</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hh0804352-hanyang-university/office-home-classification/runs/qgzwk4ub' target=\"_blank\">ResNet50_MultiTask_layer3 Extraction - Style, all - Category</a></strong> to <a href='https://wandb.ai/hh0804352-hanyang-university/office-home-classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hh0804352-hanyang-university/office-home-classification' target=\"_blank\">https://wandb.ai/hh0804352-hanyang-university/office-home-classification</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hh0804352-hanyang-university/office-home-classification/runs/qgzwk4ub' target=\"_blank\">https://wandb.ai/hh0804352-hanyang-university/office-home-classification/runs/qgzwk4ub</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 11113 이미지, 65 클래스, 4 도메인을 로드했습니다.\n",
      "총 3213 이미지, 65 클래스, 4 도메인을 로드했습니다.\n",
      "Train set size: 11113\n",
      "Test set size: 3213\n",
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 가능한 파라미터: 1,345,861 / 26,902,893 (5.00%)\n",
      "2개의 GPU를 사용합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                       | 0/300 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1], Batch [20/44], Loss: 1.9314, Domain Loss: 1.2300, Class Loss: 2.6328, LR: 0.001000\n",
      "Epoch [1], Batch [40/44], Loss: 1.3973, Domain Loss: 0.9719, Class Loss: 1.8228, LR: 0.001000\n",
      "Train set: Epoch: 1, Avg loss: 1.9138, Domain Loss: 1.1782, Class Loss: 2.6493, Domain Acc: 52.68%, Class Acc: 38.80%, Domain mAP: 0.5320, Class mAP: 0.3419, Time: 74.77s\n",
      "Test set: Epoch: 1, Avg loss: 1.2668, Domain Loss: 1.0102, Class Loss: 1.5233, Domain Acc: 60.97%, Class Acc: 63.90%, Domain mAP: 0.6613, Class mAP: 0.7282, Time: 23.86s\n",
      "\n",
      "새로운 최고 Class mAP: 0.7282, Domain mAP: 0.6613\n",
      "새로운 최고 Domain mAP: 0.6613\n",
      "Accuracy improved (-inf% --> 0.73%). Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▎                                                                                            | 1/300 [01:39<8:16:25, 99.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "체크포인트가 checkpoints/latest_checkpoint.pth에 저장되었습니다.\n",
      "Epoch [2], Batch [20/44], Loss: 1.1512, Domain Loss: 0.8358, Class Loss: 1.4667, LR: 0.001000\n",
      "Epoch [2], Batch [40/44], Loss: 1.0923, Domain Loss: 0.7778, Class Loss: 1.4069, LR: 0.001000\n",
      "Train set: Epoch: 2, Avg loss: 1.1306, Domain Loss: 0.8470, Class Loss: 1.4141, Domain Acc: 68.18%, Class Acc: 62.96%, Domain mAP: 0.6671, Class mAP: 0.6378, Time: 69.23s\n",
      "Test set: Epoch: 2, Avg loss: 0.9854, Domain Loss: 0.8214, Class Loss: 1.1493, Domain Acc: 67.29%, Class Acc: 70.37%, Domain mAP: 0.7116, Class mAP: 0.7778, Time: 23.39s\n",
      "\n",
      "새로운 최고 Class mAP: 0.7778, Domain mAP: 0.7116\n",
      "새로운 최고 Domain mAP: 0.7116\n",
      "Accuracy improved (0.73% --> 0.78%). Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▌                                                                                            | 2/300 [03:13<7:57:54, 96.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "체크포인트가 checkpoints/latest_checkpoint.pth에 저장되었습니다.\n",
      "Epoch [3], Batch [20/44], Loss: 0.9164, Domain Loss: 0.7076, Class Loss: 1.1252, LR: 0.001000\n",
      "Epoch [3], Batch [40/44], Loss: 0.9141, Domain Loss: 0.7086, Class Loss: 1.1196, LR: 0.001000\n",
      "Train set: Epoch: 3, Avg loss: 0.9569, Domain Loss: 0.7372, Class Loss: 1.1767, Domain Acc: 71.25%, Class Acc: 68.01%, Domain mAP: 0.7076, Class mAP: 0.7099, Time: 69.24s\n",
      "Test set: Epoch: 3, Avg loss: 0.8989, Domain Loss: 0.7581, Class Loss: 1.0397, Domain Acc: 68.72%, Class Acc: 71.68%, Domain mAP: 0.7450, Class mAP: 0.8022, Time: 22.96s\n",
      "\n",
      "새로운 최고 Class mAP: 0.8022, Domain mAP: 0.7450\n",
      "새로운 최고 Domain mAP: 0.7450\n",
      "Accuracy improved (0.78% --> 0.80%). Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▉                                                                                            | 3/300 [04:46<7:49:42, 94.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "체크포인트가 checkpoints/latest_checkpoint.pth에 저장되었습니다.\n",
      "Epoch [4], Batch [20/44], Loss: 0.8314, Domain Loss: 0.6730, Class Loss: 0.9898, LR: 0.001000\n",
      "Epoch [4], Batch [40/44], Loss: 0.8215, Domain Loss: 0.6415, Class Loss: 1.0016, LR: 0.001000\n",
      "Train set: Epoch: 4, Avg loss: 0.8659, Domain Loss: 0.6839, Class Loss: 1.0479, Domain Acc: 73.68%, Class Acc: 71.18%, Domain mAP: 0.7352, Class mAP: 0.7526, Time: 73.12s\n",
      "Test set: Epoch: 4, Avg loss: 0.8324, Domain Loss: 0.7239, Class Loss: 0.9408, Domain Acc: 71.12%, Class Acc: 74.14%, Domain mAP: 0.7656, Class mAP: 0.8172, Time: 22.55s\n",
      "\n",
      "새로운 최고 Class mAP: 0.8172, Domain mAP: 0.7656\n",
      "새로운 최고 Domain mAP: 0.7656\n",
      "Accuracy improved (0.80% --> 0.82%). Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|█▏                                                                                           | 4/300 [06:23<7:51:56, 95.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "체크포인트가 checkpoints/latest_checkpoint.pth에 저장되었습니다.\n",
      "Epoch [5], Batch [20/44], Loss: 0.8112, Domain Loss: 0.7446, Class Loss: 0.8778, LR: 0.001000\n",
      "Epoch [5], Batch [40/44], Loss: 0.8253, Domain Loss: 0.7110, Class Loss: 0.9397, LR: 0.001000\n",
      "Train set: Epoch: 5, Avg loss: 0.7907, Domain Loss: 0.6423, Class Loss: 0.9391, Domain Acc: 75.49%, Class Acc: 74.01%, Domain mAP: 0.7606, Class mAP: 0.7822, Time: 69.34s\n",
      "Test set: Epoch: 5, Avg loss: 0.8127, Domain Loss: 0.7036, Class Loss: 0.9218, Domain Acc: 71.37%, Class Acc: 74.14%, Domain mAP: 0.7807, Class mAP: 0.8231, Time: 23.13s\n",
      "\n",
      "새로운 최고 Class mAP: 0.8231, Domain mAP: 0.7807\n",
      "새로운 최고 Domain mAP: 0.7807\n",
      "체크포인트가 checkpoints/ResNet50_MultiTask_layer3 Extraction - Style, all - Category_checkpoint.pth에 저장되었습니다.\n",
      "Accuracy improved (0.82% --> 0.82%). Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|█▌                                                                                           | 5/300 [07:57<7:46:59, 94.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "체크포인트가 checkpoints/latest_checkpoint.pth에 저장되었습니다.\n",
      "Epoch [6], Batch [20/44], Loss: 0.7708, Domain Loss: 0.6634, Class Loss: 0.8782, LR: 0.001000\n",
      "Epoch [6], Batch [40/44], Loss: 0.6892, Domain Loss: 0.5349, Class Loss: 0.8436, LR: 0.001000\n",
      "Train set: Epoch: 6, Avg loss: 0.7424, Domain Loss: 0.6133, Class Loss: 0.8714, Domain Acc: 76.59%, Class Acc: 75.35%, Domain mAP: 0.7767, Class mAP: 0.8054, Time: 70.42s\n",
      "Test set: Epoch: 6, Avg loss: 0.7848, Domain Loss: 0.6681, Class Loss: 0.9015, Domain Acc: 73.61%, Class Acc: 74.76%, Domain mAP: 0.7915, Class mAP: 0.8287, Time: 23.38s\n",
      "\n",
      "새로운 최고 Class mAP: 0.8287, Domain mAP: 0.7915\n",
      "새로운 최고 Domain mAP: 0.7915\n",
      "Accuracy improved (0.82% --> 0.83%). Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|█▊                                                                                           | 6/300 [09:32<7:45:26, 94.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "체크포인트가 checkpoints/latest_checkpoint.pth에 저장되었습니다.\n",
      "Epoch [22], Batch [40/44], Loss: 0.4889, Domain Loss: 0.4825, Class Loss: 0.4953, LR: 0.001000\n",
      "Train set: Epoch: 22, Avg loss: 0.4365, Domain Loss: 0.4592, Class Loss: 0.4138, Domain Acc: 82.68%, Class Acc: 87.47%, Domain mAP: 0.8591, Class mAP: 0.9331, Time: 71.43s\n",
      "Test set: Epoch: 22, Avg loss: 0.7125, Domain Loss: 0.5985, Class Loss: 0.8265, Domain Acc: 77.81%, Class Acc: 78.24%, Domain mAP: 0.8353, Class mAP: 0.8494, Time: 24.13s\n",
      "\n",
      "EarlyStopping 카운터: 1 / 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|██████▋                                                                                     | 22/300 [34:46<7:19:46, 94.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "체크포인트가 checkpoints/latest_checkpoint.pth에 저장되었습니다.\n",
      "Epoch [23], Batch [20/44], Loss: 0.3949, Domain Loss: 0.4086, Class Loss: 0.3811, LR: 0.001000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchmetrics\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "from utility.utils import AccuracyEarlyStopping\n",
    "from torchmetrics.classification import MulticlassAveragePrecision\n",
    "from Dataset.data import OfficeHomeDataset  # 기존 데이터셋 클래스 임포트\n",
    "\n",
    "# 체크포인트 디렉토리 생성\n",
    "os.makedirs('checkpoints', exist_ok=True)\n",
    "\n",
    "# WandB 설정 \n",
    "wandb.login(key=\"ef091b9abcea3186341ddf8995d62bde62d7469e\")\n",
    "wandb.init(\n",
    "    project=\"office-home-classification\", \n",
    "    name=\"ResNet50_MultiTask_layer3 Extraction - Style, all - Category\",\n",
    "    entity=\"hh0804352-hanyang-university\"\n",
    ")\n",
    "\n",
    "# wandb run name을 체크포인트 경로에 사용\n",
    "run_name = wandb.run.name\n",
    "CHECKPOINT_PATH = os.path.join('checkpoints', f'{run_name}_checkpoint.pth')\n",
    "\n",
    "\n",
    "# 설정\n",
    "config = {\n",
    "    # 모델 설정\n",
    "    \"model\": \"resnet50\",\n",
    "    \"batch_size\": 256,\n",
    "    \"num_epochs\": 300,\n",
    "    \n",
    "    \"learning_rate\": 0.001,  # Adam 기본 학습률\n",
    "    \"optimizer\": \"Adam\",\n",
    "    \n",
    "    # 학습 과정 설정\n",
    "    \"seed\": 2025,\n",
    "    \"deterministic\": False,\n",
    "    \"patience\": 30,  # early stopping patience\n",
    "    \"max_epochs_wait\": float('inf'),\n",
    "    \n",
    "    # 멀티태스크 설정\n",
    "    \"num_domains\": 4,\n",
    "    \"num_classes\": 65,\n",
    "    \"domain_weight\": 0.5,  # 도메인 분류 손실 가중치\n",
    "    \"class_weight\": 0.5,   # 클래스 분류 손실 가중치\n",
    "    \n",
    "    # 시스템 설정\n",
    "    \"num_workers\": 32,\n",
    "    \"pin_memory\": True,\n",
    "    \n",
    "    # 체크포인트 설정\n",
    "    \"save_every\": 5,  # 몇 epoch마다 저장할지\n",
    "    \n",
    "    # 스케줄러 설정\n",
    "    \"scheduler\": \"ReduceLROnPlateau\",\n",
    "    \"scheduler_mode\": \"max\",\n",
    "    \"scheduler_factor\": 0.1,\n",
    "    \"scheduler_patience\": 5,\n",
    "    \"scheduler_verbose\": True,\n",
    "}\n",
    "\n",
    "wandb.config.update(config)\n",
    "\n",
    "# ResNet-50 기반 멀티태스크 모델 - 중간 활성화 사용\n",
    "class FeatureExtractingMultiTaskModel(nn.Module):\n",
    "    def __init__(self, num_domains=4, num_classes=65):\n",
    "        super(FeatureExtractingMultiTaskModel, self).__init__()\n",
    "        # 사전학습된 ResNet-50 로드\n",
    "        self.backbone = models.resnet50(pretrained=True)\n",
    "        \n",
    "        # 전체 백본 동결 (선택적)\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        # 2. FeatureExtractingMultiTaskModel 클래스에서 도메인 분류기 수정 (약 91-96 라인)\n",
    "        # 도메인 분류를 위한 추가 레이어 (Layer3 출력에 적용)\n",
    "        # Layer3 출력 크기: [B, 1024, H/16, W/16]\n",
    "        self.domain_avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.domain_classifier = nn.Sequential(\n",
    "            nn.Linear(1024, 256),  # Layer3의 채널 수는 1024\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, num_domains)\n",
    "        )\n",
    "        \n",
    "        # 클래스 분류를 위한 헤드 (최종 특징에 적용)\n",
    "        self.class_classifier = nn.Sequential(\n",
    "            nn.Linear(2048, 512),  # 최종 출력의 채널 수는 2048\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 중간 결과를 저장할 리스트\n",
    "        activations = []\n",
    "        \n",
    "        # ResNet 블록을 하나씩 통과시키며 중간 결과 저장\n",
    "        x = self.backbone.conv1(x)\n",
    "        x = self.backbone.bn1(x)\n",
    "        x = self.backbone.relu(x)\n",
    "        x = self.backbone.maxpool(x)\n",
    "        \n",
    "        x = self.backbone.layer1(x)\n",
    "        activations.append(x)  # 저수준 특징\n",
    "        \n",
    "        x = self.backbone.layer2(x)\n",
    "        activations.append(x)  # 중간 수준 특징 - 도메인 분류에 사용\n",
    "        \n",
    "        x = self.backbone.layer3(x)\n",
    "        activations.append(x)  # 중간-고수준 특징\n",
    "        \n",
    "        x = self.backbone.layer4(x)\n",
    "        activations.append(x)  # 고수준 특징\n",
    "        \n",
    "        x = self.backbone.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        \n",
    "        # 클래스 분류에는 전체 특징 사용\n",
    "        class_out = self.class_classifier(x)\n",
    "        \n",
    "        # 도메인 분류에는 중간 수준 특징(layer3 출력) 사용\n",
    "        domain_mid_feat = activations[2]  # Layer3 출력 \n",
    "        domain_mid_feat = self.domain_avgpool(domain_mid_feat)  # [B, 512, 1, 1]\n",
    "        domain_mid_feat = torch.flatten(domain_mid_feat, 1)  # [B, 512]\n",
    "        domain_out = self.domain_classifier(domain_mid_feat)\n",
    "        \n",
    "        return domain_out, class_out\n",
    "\n",
    "# 데이터 변환 (RandomResizedCrop 제거)\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((224,224)),  # \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# 데이터셋 로드\n",
    "trainset = OfficeHomeDataset(root_dir='./Dataset/train', transform=transform_train)\n",
    "testset = OfficeHomeDataset(root_dir='./Dataset/test', transform=transform_test)\n",
    "\n",
    "# DataLoader 생성\n",
    "trainloader = DataLoader(\n",
    "    trainset, \n",
    "    batch_size=config[\"batch_size\"], \n",
    "    shuffle=True, \n",
    "    pin_memory=config[\"pin_memory\"], \n",
    "    num_workers=config[\"num_workers\"]\n",
    ")\n",
    "\n",
    "testloader = DataLoader(\n",
    "    testset, \n",
    "    batch_size=config[\"batch_size\"], \n",
    "    shuffle=False, \n",
    "    pin_memory=config[\"pin_memory\"], \n",
    "    num_workers=config[\"num_workers\"]\n",
    ")\n",
    "\n",
    "print(f\"Train set size: {len(trainset)}\")\n",
    "print(f\"Test set size: {len(testset)}\")\n",
    "\n",
    "def train(model, trainloader, domain_criterion, class_criterion, optimizer, device, epoch):\n",
    "    \"\"\"\n",
    "    멀티태스크 학습 함수 (도메인 + 클래스 분류)\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    running_loss = 0.0\n",
    "    running_domain_loss = 0.0  # 도메인 손실 별도 추적\n",
    "    running_class_loss = 0.0   # 클래스 손실 별도 추적\n",
    "    domain_correct = 0\n",
    "    class_correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # mAP 계산기 초기화\n",
    "    domain_map = MulticlassAveragePrecision(num_classes=config[\"num_domains\"], average='macro')\n",
    "    class_map = MulticlassAveragePrecision(num_classes=config[\"num_classes\"], average='macro')\n",
    "    \n",
    "    domain_map = domain_map.to(device)\n",
    "    class_map = class_map.to(device)\n",
    "    \n",
    "    for i, (inputs, domain_labels, class_labels) in enumerate(trainloader):\n",
    "        inputs = inputs.to(device)\n",
    "        domain_labels = domain_labels.to(device)\n",
    "        class_labels = class_labels.to(device)\n",
    "        \n",
    "        # 그래디언트 초기화\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 모델 전방 전파\n",
    "        domain_outputs, class_outputs = model(inputs)\n",
    "        \n",
    "        # 손실 계산\n",
    "        domain_loss = domain_criterion(domain_outputs, domain_labels)\n",
    "        class_loss = class_criterion(class_outputs, class_labels)\n",
    "        loss = config[\"domain_weight\"] * domain_loss + config[\"class_weight\"] * class_loss\n",
    "        \n",
    "        # 역전파 및 최적화\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        running_domain_loss += domain_loss.item()  # 도메인 손실 누적\n",
    "        running_class_loss += class_loss.item()    # 클래스 손실 누적\n",
    "        \n",
    "        # 정확도 계산\n",
    "        _, domain_preds = domain_outputs.max(1)\n",
    "        domain_correct += domain_preds.eq(domain_labels).sum().item()\n",
    "        \n",
    "        _, class_preds = class_outputs.max(1)\n",
    "        class_correct += class_preds.eq(class_labels).sum().item()\n",
    "        \n",
    "        total += inputs.size(0)\n",
    "        \n",
    "        # mAP 업데이트\n",
    "        domain_map.update(domain_outputs, domain_labels)\n",
    "        class_map.update(class_outputs, class_labels)\n",
    "        \n",
    "        if (i + 1) % 20 == 0:\n",
    "            print(f'Epoch [{epoch+1}], Batch [{i+1}/{len(trainloader)}], Loss: {loss.item():.4f}, '\n",
    "                  f'Domain Loss: {domain_loss.item():.4f}, Class Loss: {class_loss.item():.4f}, '\n",
    "                  f'LR: {optimizer.param_groups[0][\"lr\"]:.6f}')\n",
    "    \n",
    "    # 에폭 통계\n",
    "    epoch_loss = running_loss / len(trainloader)\n",
    "    epoch_domain_loss = running_domain_loss / len(trainloader)  # 평균 도메인 손실\n",
    "    epoch_class_loss = running_class_loss / len(trainloader)    # 평균 클래스 손실\n",
    "    domain_accuracy = 100.0 * domain_correct / total\n",
    "    class_accuracy = 100.0 * class_correct / total\n",
    "    \n",
    "    # mAP 계산\n",
    "    domain_map_value = domain_map.compute().item()\n",
    "    class_map_value = class_map.compute().item()\n",
    "    \n",
    "    train_time = time.time() - start_time\n",
    "    \n",
    "    # 학습 세트에 대한 성능 출력\n",
    "    print(f'Train set: Epoch: {epoch+1}, Avg loss: {epoch_loss:.4f}, '\n",
    "          f'Domain Loss: {epoch_domain_loss:.4f}, Class Loss: {epoch_class_loss:.4f}, '\n",
    "          f'Domain Acc: {domain_accuracy:.2f}%, Class Acc: {class_accuracy:.2f}%, '\n",
    "          f'Domain mAP: {domain_map_value:.4f}, Class mAP: {class_map_value:.4f}, '\n",
    "          f'Time: {train_time:.2f}s')\n",
    "    \n",
    "    return epoch_loss, epoch_domain_loss, epoch_class_loss, domain_accuracy, class_accuracy, domain_map_value, class_map_value\n",
    "\n",
    "def evaluate(model, dataloader, domain_criterion, class_criterion, device, epoch):\n",
    "    \"\"\"\n",
    "    멀티태스크 평가 함수\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    start_time = time.time()\n",
    "    running_loss = 0.0\n",
    "    running_domain_loss = 0.0  # 도메인 손실 별도 추적\n",
    "    running_class_loss = 0.0   # 클래스 손실 별도 추적\n",
    "    domain_correct = 0\n",
    "    class_correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # mAP 계산기 초기화\n",
    "    domain_map = MulticlassAveragePrecision(num_classes=config[\"num_domains\"], average='macro')\n",
    "    class_map = MulticlassAveragePrecision(num_classes=config[\"num_classes\"], average='macro')\n",
    "    \n",
    "    domain_map = domain_map.to(device)\n",
    "    class_map = class_map.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, domain_labels, class_labels in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            domain_labels = domain_labels.to(device)\n",
    "            class_labels = class_labels.to(device)\n",
    "            \n",
    "            # 순전파\n",
    "            domain_outputs, class_outputs = model(inputs)\n",
    "            \n",
    "            # 손실 계산\n",
    "            domain_loss = domain_criterion(domain_outputs, domain_labels)\n",
    "            class_loss = class_criterion(class_outputs, class_labels)\n",
    "            loss = config[\"domain_weight\"] * domain_loss + config[\"class_weight\"] * class_loss\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            running_domain_loss += domain_loss.item()  # 도메인 손실 누적\n",
    "            running_class_loss += class_loss.item()    # 클래스 손실 누적\n",
    "            \n",
    "            # 정확도 계산\n",
    "            _, domain_preds = domain_outputs.max(1)\n",
    "            domain_correct += domain_preds.eq(domain_labels).sum().item()\n",
    "            \n",
    "            _, class_preds = class_outputs.max(1)\n",
    "            class_correct += class_preds.eq(class_labels).sum().item()\n",
    "            \n",
    "            total += inputs.size(0)\n",
    "            \n",
    "            # mAP 업데이트\n",
    "            domain_map.update(domain_outputs, domain_labels)\n",
    "            class_map.update(class_outputs, class_labels)\n",
    "    \n",
    "    # 평균 손실 및 정확도 계산\n",
    "    eval_loss = running_loss / len(dataloader)\n",
    "    eval_domain_loss = running_domain_loss / len(dataloader)  # 평균 도메인 손실\n",
    "    eval_class_loss = running_class_loss / len(dataloader)    # 평균 클래스 손실\n",
    "    domain_accuracy = 100.0 * domain_correct / total\n",
    "    class_accuracy = 100.0 * class_correct / total\n",
    "    \n",
    "    # mAP 계산\n",
    "    domain_map_value = domain_map.compute().item()\n",
    "    class_map_value = class_map.compute().item()\n",
    "    \n",
    "    # 평가 시간 계산\n",
    "    eval_time = time.time() - start_time\n",
    "    \n",
    "    # 테스트 세트에 대한 성능 출력\n",
    "    print(f'Test set: Epoch: {epoch+1}, Avg loss: {eval_loss:.4f}, '\n",
    "          f'Domain Loss: {eval_domain_loss:.4f}, Class Loss: {eval_class_loss:.4f}, '\n",
    "          f'Domain Acc: {domain_accuracy:.2f}%, Class Acc: {class_accuracy:.2f}%, '\n",
    "          f'Domain mAP: {domain_map_value:.4f}, Class mAP: {class_map_value:.4f}, '\n",
    "          f'Time: {eval_time:.2f}s')\n",
    "    print()\n",
    "    \n",
    "    return eval_loss, eval_domain_loss, eval_class_loss, domain_accuracy, class_accuracy, domain_map_value, class_map_value\n",
    "\n",
    "# 체크포인트 저장 함수 추가\n",
    "def save_checkpoint(model, optimizer, scheduler, epoch, best_class_map, best_domain_map, early_stopping, filename=CHECKPOINT_PATH):\n",
    "    \"\"\"\n",
    "    학습 상태 저장 함수\n",
    "    \"\"\"\n",
    "    # 모델이 DataParallel로 감싸져 있는 경우 처리\n",
    "    model_state_dict = model.module.state_dict() if isinstance(model, nn.DataParallel) else model.state_dict()\n",
    "    \n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model_state_dict,\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),  # 스케줄러 상태 저장 추가\n",
    "        'best_class_map': best_class_map,\n",
    "        'best_domain_map': best_domain_map,\n",
    "        'early_stopping_counter': early_stopping.counter,\n",
    "        'early_stopping_best_score': early_stopping.best_score,\n",
    "        'early_stopping_best_epoch': early_stopping.best_epoch,\n",
    "        'early_stopping_early_stop': early_stopping.early_stop,\n",
    "        'config': config,  # 설정값도 저장\n",
    "    }\n",
    "    torch.save(checkpoint, filename)\n",
    "    print(f\"체크포인트가 {filename}에 저장되었습니다.\")\n",
    "\n",
    "# 체크포인트 로드 함수 추가\n",
    "def load_checkpoint(model, optimizer, scheduler, early_stopping, filename=CHECKPOINT_PATH):\n",
    "    \"\"\"\n",
    "    학습 상태 로드 함수\n",
    "    \"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        print(f\"체크포인트 파일 {filename}이 존재하지 않습니다. 처음부터 학습을 시작합니다.\")\n",
    "        return model, optimizer, scheduler, early_stopping, 0, 0.0, 0.0\n",
    "    \n",
    "    print(f\"체크포인트 {filename}을 로드합니다.\")\n",
    "    checkpoint = torch.load(filename)\n",
    "    \n",
    "    # 모델이 DataParallel로 감싸져 있는 경우 처리\n",
    "    if isinstance(model, nn.DataParallel):\n",
    "        model.module.load_state_dict(checkpoint['model_state_dict'])\n",
    "    else:\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    \n",
    "    # 스케줄러 상태 로드 추가\n",
    "    if 'scheduler_state_dict' in checkpoint:\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    \n",
    "    # 조기 중단 상태 복원\n",
    "    early_stopping.counter = checkpoint['early_stopping_counter']\n",
    "    early_stopping.best_score = checkpoint['early_stopping_best_score']\n",
    "    early_stopping.best_epoch = checkpoint['early_stopping_best_epoch']\n",
    "    early_stopping.early_stop = checkpoint['early_stopping_early_stop']\n",
    "    \n",
    "    # 기타 학습 상태\n",
    "    start_epoch = checkpoint['epoch'] + 1  # 다음 에폭부터 시작\n",
    "    best_class_map = checkpoint['best_class_map']\n",
    "    best_domain_map = checkpoint['best_domain_map']\n",
    "    \n",
    "    print(f\"체크포인트에서 로드 완료: 에폭 {start_epoch}부터 시작합니다.\")\n",
    "    print(f\"이전 최고 성능: Class mAP: {best_class_map:.4f}, Domain mAP: {best_domain_map:.4f}\")\n",
    "    \n",
    "    return model, optimizer, scheduler, early_stopping, start_epoch, best_class_map, best_domain_map\n",
    "\n",
    "# 메인 학습 루프\n",
    "def main_training_loop(model, trainloader, testloader, domain_criterion, class_criterion, optimizer, scheduler, device, num_epochs=None, patience=None, max_epochs_wait=None):\n",
    "    \"\"\"\n",
    "    메인 학습 루프 (mAP 기준 early stopping)\n",
    "    \"\"\"\n",
    "    # config에서 값 가져오기\n",
    "    if num_epochs is None:\n",
    "        num_epochs = config[\"num_epochs\"]\n",
    "    if patience is None:\n",
    "        patience = config[\"patience\"]\n",
    "    if max_epochs_wait is None:\n",
    "        max_epochs_wait = config[\"max_epochs_wait\"]\n",
    "        \n",
    "    # mAP 기반 얼리 스토핑 초기화\n",
    "    early_stopping = AccuracyEarlyStopping(patience=patience, verbose=True, path='checkpoint.pt', max_epochs=max_epochs_wait)\n",
    "    \n",
    "\n",
    "    start_epoch = 0\n",
    "    best_class_map = 0.0\n",
    "    best_domain_map = 0.0\n",
    "    \n",
    "    # tqdm을 사용한 진행 상황 표시\n",
    "    for epoch in tqdm(range(start_epoch, num_epochs)):\n",
    "        # 학습\n",
    "        train_loss, train_domain_loss, train_class_loss, train_domain_acc, train_class_acc, train_domain_map, train_class_map = train(\n",
    "            model, \n",
    "            trainloader, \n",
    "            domain_criterion, \n",
    "            class_criterion, \n",
    "            optimizer, \n",
    "            device, \n",
    "            epoch\n",
    "        )\n",
    "        \n",
    "        # 테스트 데이터로 평가\n",
    "        test_loss, test_domain_loss, test_class_loss, test_domain_acc, test_class_acc, test_domain_map, test_class_map = evaluate(\n",
    "            model, \n",
    "            testloader, \n",
    "            domain_criterion, \n",
    "            class_criterion, \n",
    "            device, \n",
    "            epoch\n",
    "        )\n",
    "        \n",
    "        # 학습률 조정 - 검증 성능에 따라 스케줄러 업데이트\n",
    "        avg_map = (test_domain_map + test_class_map) / 2\n",
    "        scheduler.step(avg_map)  # 스케줄러 호출 추가\n",
    "        \n",
    "        # WandB에 로깅 (도메인 손실과 클래스 손실 별도 로깅)\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"learning_rate\": optimizer.param_groups[0]['lr'],\n",
    "            \"train_loss\": train_loss,\n",
    "            \"train_domain_loss\": train_domain_loss,\n",
    "            \"train_class_loss\": train_class_loss,\n",
    "            \"train_domain_accuracy\": train_domain_acc,\n",
    "            \"train_class_accuracy\": train_class_acc,\n",
    "            \"train_domain_map\": train_domain_map,\n",
    "            \"train_class_map\": train_class_map,\n",
    "            \"test_loss\": test_loss,\n",
    "            \"test_domain_loss\": test_domain_loss,\n",
    "            \"test_class_loss\": test_class_loss,\n",
    "            \"test_domain_accuracy\": test_domain_acc,\n",
    "            \"test_class_accuracy\": test_class_acc,\n",
    "            \"test_domain_map\": test_domain_map,\n",
    "            \"test_class_map\": test_class_map\n",
    "        })\n",
    "            \n",
    "        # 최고 클래스 mAP 모델 저장\n",
    "        if test_class_map > best_class_map:\n",
    "            best_class_map = test_class_map\n",
    "            best_domain_map_at_best_class = test_domain_map\n",
    "            print(f'새로운 최고 Class mAP: {best_class_map:.4f}, Domain mAP: {best_domain_map_at_best_class:.4f}')\n",
    "            # 모델 저장\n",
    "            model_path = f'best_model_class_{wandb.run.name}.pth'\n",
    "            # 모델이 DataParallel로 감싸져 있는 경우 처리\n",
    "            model_state_dict = model.module.state_dict() if isinstance(model, nn.DataParallel) else model.state_dict()\n",
    "            torch.save(model_state_dict, model_path)\n",
    "            wandb.save(model_path)\n",
    "        \n",
    "        # 최고 도메인 mAP 모델 저장\n",
    "        if test_domain_map > best_domain_map:\n",
    "            best_domain_map = test_domain_map\n",
    "            print(f'새로운 최고 Domain mAP: {best_domain_map:.4f}')\n",
    "            # 모델 저장\n",
    "            model_path = f'best_model_domain_{wandb.run.name}.pth'\n",
    "            # 모델이 DataParallel로 감싸져 있는 경우 처리\n",
    "            model_state_dict = model.module.state_dict() if isinstance(model, nn.DataParallel) else model.state_dict()\n",
    "            torch.save(model_state_dict, model_path)\n",
    "            wandb.save(model_path)\n",
    "\n",
    "        # 주기적으로 체크포인트 저장 (설정한 간격마다)\n",
    "        if (epoch + 1) % config[\"save_every\"] == 0:\n",
    "            save_checkpoint(model, optimizer, scheduler, epoch, best_class_map, best_domain_map, early_stopping)\n",
    "\n",
    "        # Early stopping 체크 (클래스 mAP 기준)\n",
    "        early_stopping(test_class_map, model, epoch)\n",
    "        \n",
    "        # 매 에폭 후에 체크포인트 저장 (가장 최신 상태)\n",
    "        save_checkpoint(model, optimizer, scheduler, epoch, best_class_map, best_domain_map, early_stopping, \n",
    "                       filename=os.path.join('checkpoints', 'latest_checkpoint.pth'))\n",
    "        \n",
    "        if early_stopping.early_stop:\n",
    "            print(f\"에폭 {epoch+1}에서 학습 조기 종료. 최고 성능 에폭: {early_stopping.best_epoch+1}\")\n",
    "            break\n",
    "    \n",
    "    # 훈련 완료 후 최고 모델 로드\n",
    "    print(\"최고 클래스 mAP 모델 로드 중...\")\n",
    "    model_path = f'best_model_class_{wandb.run.name}.pth'\n",
    "    if os.path.exists(model_path):\n",
    "        # 모델이 DataParallel로 감싸져 있는 경우 처리\n",
    "        if isinstance(model, nn.DataParallel):\n",
    "            model.module.load_state_dict(torch.load(model_path))\n",
    "        else:\n",
    "            model.load_state_dict(torch.load(model_path))\n",
    "    else:\n",
    "        print(f\"경고: {model_path} 파일이 존재하지 않습니다. 최종 모델을 사용합니다.\")\n",
    "\n",
    "    # 최종 테스트 평가\n",
    "    final_test_loss, final_test_domain_loss, final_test_class_loss, final_test_domain_acc, final_test_class_acc, final_test_domain_map, final_test_class_map = evaluate(\n",
    "        model, testloader, domain_criterion, class_criterion, device, num_epochs-1\n",
    "    )\n",
    "    \n",
    "    print(f'완료! 최고 Class mAP: {best_class_map:.4f}, 최고 Domain mAP: {best_domain_map:.4f}')\n",
    "    \n",
    "    # WandB에 최종 결과 기록\n",
    "    wandb.run.summary[\"best_class_map\"] = best_class_map\n",
    "    wandb.run.summary[\"best_domain_map\"] = best_domain_map\n",
    "    wandb.run.summary[\"final_test_class_map\"] = final_test_class_map\n",
    "    wandb.run.summary[\"final_test_domain_map\"] = final_test_domain_map\n",
    "\n",
    "    # Early stopping 정보 저장\n",
    "    if early_stopping.early_stop:\n",
    "        wandb.run.summary[\"early_stopped\"] = True\n",
    "        wandb.run.summary[\"early_stopped_epoch\"] = epoch+1\n",
    "        wandb.run.summary[\"best_epoch\"] = early_stopping.best_epoch+1\n",
    "    else:\n",
    "        wandb.run.summary[\"early_stopped\"] = False\n",
    "        \n",
    "    # 최종 체크포인트 저장\n",
    "    save_checkpoint(model, optimizer, scheduler, epoch, best_class_map, best_domain_map, early_stopping, \n",
    "                   filename=os.path.join('checkpoints', 'final_checkpoint.pth'))\n",
    "\n",
    "# 메인 실행 코드\n",
    "if __name__ == \"__main__\":\n",
    "    # 시드 설정\n",
    "    seed = config[\"seed\"]\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "    # 결정적 알고리즘 사용 여부\n",
    "    if config[\"deterministic\"]:\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "    else:\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "    # 디바이스 설정\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # 모델 초기화 - 새로운 중간 특징 추출 모델 사용\n",
    "    model = FeatureExtractingMultiTaskModel(\n",
    "        num_domains=config[\"num_domains\"], \n",
    "        num_classes=config[\"num_classes\"]\n",
    "    ).to(device)\n",
    "    \n",
    "    # 학습 가능한 파라미터 확인\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"학습 가능한 파라미터: {trainable_params:,} / {total_params:,} ({trainable_params / total_params:.2%})\")\n",
    "    \n",
    "    # 손실 함수\n",
    "    domain_criterion = nn.CrossEntropyLoss()\n",
    "    class_criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # 학습률 차별화 (선택적)\n",
    "    optimizer = optim.Adam([\n",
    "        {'params': model.domain_classifier.parameters(), 'lr': config[\"learning_rate\"]},  # 도메인 분류기\n",
    "        {'params': model.class_classifier.parameters(), 'lr': config[\"learning_rate\"]}    # 클래스 분류기\n",
    "    ])\n",
    "    \n",
    "    # 스케줄러 초기화\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, \n",
    "        mode=config[\"scheduler_mode\"], \n",
    "        factor=config[\"scheduler_factor\"], \n",
    "        patience=config[\"scheduler_patience\"], \n",
    "        verbose=config[\"scheduler_verbose\"]\n",
    "    )\n",
    "    \n",
    "    # WandB에 모델 구조 기록\n",
    "    wandb.watch(model, log=\"all\")\n",
    "    \n",
    "    # GPU 가속 \n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(f\"{torch.cuda.device_count()}개의 GPU를 사용합니다.\")\n",
    "        model = nn.DataParallel(model)\n",
    "    \n",
    "    # 훈련 시작 시간 기록\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # 메인 학습 루프 호출 \n",
    "    try:\n",
    "        main_training_loop(\n",
    "            model=model,\n",
    "            trainloader=trainloader,\n",
    "            testloader=testloader,\n",
    "            domain_criterion=domain_criterion,\n",
    "            class_criterion=class_criterion,\n",
    "            optimizer=optimizer,\n",
    "            scheduler=scheduler,  # 스케줄러 전달 추가\n",
    "            device=device\n",
    "        )\n",
    "    except KeyboardInterrupt:\n",
    "        # 사용자가 Ctrl+C로 중단한 경우, 현재 상태 저장\n",
    "        print(\"학습이 사용자에 의해 중단되었습니다. 현재 상태를 저장합니다.\")\n",
    "        early_stopping = AccuracyEarlyStopping(patience=config[\"patience\"], verbose=True)\n",
    "        save_checkpoint(model, optimizer, scheduler, 0, 0.0, 0.0, early_stopping, \n",
    "                       filename=os.path.join('checkpoints', 'interrupted_checkpoint.pth'))\n",
    "    \n",
    "    # 훈련 종료 시간 및 출력\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    wandb.log({\"total_training_time\": total_time})\n",
    "    \n",
    "    print(f\"전체 학습 시간: {total_time:.2f} 초\")\n",
    "    \n",
    "    # WandB 실행 종료\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7d0f34-01c7-4f72-9853-140d1528e1e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
