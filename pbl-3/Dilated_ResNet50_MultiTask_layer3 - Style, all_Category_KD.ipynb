{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c261d0-8022-4151-9f41-246cbca67d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Dilated_ResNet50_MultiTask_layer3 - Style, all_Category_KD</strong> at: <a href='https://wandb.ai/hh0804352-hanyang-university/office-home-classification/runs/vi7nskt1' target=\"_blank\">https://wandb.ai/hh0804352-hanyang-university/office-home-classification/runs/vi7nskt1</a><br> View project at: <a href='https://wandb.ai/hh0804352-hanyang-university/office-home-classification' target=\"_blank\">https://wandb.ai/hh0804352-hanyang-university/office-home-classification</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250525_085743-vi7nskt1/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/guswls/EEN4015_Intro2ML/pbl-3/wandb/run-20250525_093342-xiy2i83g</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hh0804352-hanyang-university/office-home-classification/runs/xiy2i83g' target=\"_blank\">Dilated_ResNet50_MultiTask_layer3 - Style, all_Category_KD</a></strong> to <a href='https://wandb.ai/hh0804352-hanyang-university/office-home-classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hh0804352-hanyang-university/office-home-classification' target=\"_blank\">https://wandb.ai/hh0804352-hanyang-university/office-home-classification</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hh0804352-hanyang-university/office-home-classification/runs/xiy2i83g' target=\"_blank\">https://wandb.ai/hh0804352-hanyang-university/office-home-classification/runs/xiy2i83g</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì´ 11113 ì´ë¯¸ì§€, 65 í´ëž˜ìŠ¤, 4 ë„ë©”ì¸ì„ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.\n",
      "ì´ 3213 ì´ë¯¸ì§€, 65 í´ëž˜ìŠ¤, 4 ë„ë©”ì¸ì„ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.\n",
      "Train set size: 11113\n",
      "Test set size: 3213\n",
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guswls/.local/lib/python3.11/site-packages/timm/models/_factory.py:126: UserWarning: Mapping deprecated model name tf_efficientnet_b2_ns to current tf_efficientnet_b2.ns_jft_in1k.\n",
      "  model = create_fn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teacher íŒŒë¼ë¯¸í„°: 10,489,543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student íŒŒë¼ë¯¸í„°: 24,846,149\n",
      "âœ… ê¸°ì¡´ Teacher ëª¨ë¸ ë°œê²¬: efficientnet_teacher_best.pth\n",
      "ðŸ“ ì €ìž¥ëœ Teacher ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤...\n",
      "ðŸŽ“ Teacher ëª¨ë¸ ë¡œë“œ ì™„ë£Œ! KD í•™ìŠµì„ ì‹œìž‘í•©ë‹ˆë‹¤.\n",
      "2ê°œì˜ GPUë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
      "Knowledge Distillation í›ˆë ¨ ì‹œìž‘!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KD Epoch [1], Batch [20/87]\n",
      "Total: 15.0850 | Domain KD: 17.6160 | Class KD: 23.3997\n",
      "KD Epoch [1], Batch [40/87]\n",
      "Total: 12.2148 | Domain KD: 16.2182 | Class KD: 18.1731\n",
      "KD Epoch [1], Batch [60/87]\n",
      "Total: 10.2639 | Domain KD: 12.7069 | Class KD: 15.6878\n",
      "KD Epoch [1], Batch [80/87]\n",
      "Total: 8.2902 | Domain KD: 10.6157 | Class KD: 12.5795\n",
      "KD Train Epoch 1:\n",
      "Total Loss: 12.1558 | Domain Acc: 54.44% | Class Acc: 39.13%\n",
      "Domain mAP: 0.5311 | Class mAP: 0.3531\n",
      "KD Losses - Domain: 15.1380 | Class: 18.5203\n",
      "Time: 307.98s\n",
      "\n",
      "Test set: Epoch: 1, Avg loss: 1.3692, Domain Loss: 1.0692, Class Loss: 1.4442, Domain Acc: 63.80%, Class Acc: 62.31%, Domain mAP: 0.7115, Class mAP: 0.7023, Time: 50.94s\n",
      "\n",
      "ðŸ† ìƒˆë¡œìš´ ìµœê³  Class mAP: 0.7023\n",
      "ðŸ† ìƒˆë¡œìš´ ìµœê³  Domain mAP: 0.7115\n",
      "Performance improved. Saving model ...\n",
      "Domain mAP: -inf --> 0.7115, Class mAP: -inf --> 0.7023\n",
      "KD Epoch [2], Batch [20/87]\n",
      "Total: 6.7091 | Domain KD: 8.0048 | Class KD: 10.1600\n",
      "KD Epoch [2], Batch [40/87]\n",
      "Total: 6.9191 | Domain KD: 10.4619 | Class KD: 9.8846\n",
      "KD Epoch [2], Batch [60/87]\n",
      "Total: 6.5121 | Domain KD: 9.0987 | Class KD: 9.5251\n",
      "KD Epoch [2], Batch [80/87]\n",
      "Total: 6.3454 | Domain KD: 8.8494 | Class KD: 9.2516\n",
      "KD Train Epoch 2:\n",
      "Total Loss: 6.9650 | Domain Acc: 70.76% | Class Acc: 63.46%\n",
      "Domain mAP: 0.6690 | Class mAP: 0.6374\n",
      "KD Losses - Domain: 9.6950 | Class: 10.2105\n",
      "Time: 302.53s\n",
      "\n",
      "Test set: Epoch: 2, Avg loss: 1.2190, Domain Loss: 1.2071, Class Loss: 1.2220, Domain Acc: 68.35%, Class Acc: 69.87%, Domain mAP: 0.7599, Class mAP: 0.7753, Time: 51.39s\n",
      "\n",
      "ðŸ† ìƒˆë¡œìš´ ìµœê³  Class mAP: 0.7753\n",
      "ðŸ† ìƒˆë¡œìš´ ìµœê³  Domain mAP: 0.7599\n",
      "Domain mAP improved (0.7115 --> 0.7599).\n",
      "Class mAP improved (0.7023 --> 0.7753).\n",
      "Performance improved. Saving model ...\n",
      "Domain mAP: 0.7115 --> 0.7599, Class mAP: 0.7023 --> 0.7753\n",
      "KD Epoch [3], Batch [20/87]\n",
      "Total: 6.0497 | Domain KD: 9.5859 | Class KD: 8.5833\n",
      "KD Epoch [3], Batch [40/87]\n",
      "Total: 5.4624 | Domain KD: 8.5570 | Class KD: 7.8417\n",
      "KD Epoch [3], Batch [60/87]\n",
      "Total: 5.6497 | Domain KD: 8.5535 | Class KD: 8.0054\n",
      "KD Epoch [3], Batch [80/87]\n",
      "Total: 5.5067 | Domain KD: 8.4245 | Class KD: 7.9226\n",
      "KD Train Epoch 3:\n",
      "Total Loss: 5.7546 | Domain Acc: 73.82% | Class Acc: 70.89%\n",
      "Domain mAP: 0.7153 | Class mAP: 0.7424\n",
      "KD Losses - Domain: 8.7561 | Class: 8.2083\n",
      "Time: 305.78s\n",
      "\n",
      "Test set: Epoch: 3, Avg loss: 1.1463, Domain Loss: 1.1608, Class Loss: 1.1427, Domain Acc: 70.77%, Class Acc: 72.61%, Domain mAP: 0.7895, Class mAP: 0.8000, Time: 50.69s\n",
      "\n",
      "ðŸ† ìƒˆë¡œìš´ ìµœê³  Class mAP: 0.8000\n",
      "ðŸ† ìƒˆë¡œìš´ ìµœê³  Domain mAP: 0.7895\n",
      "Domain mAP improved (0.7599 --> 0.7895).\n",
      "Class mAP improved (0.7753 --> 0.8000).\n",
      "Performance improved. Saving model ...\n",
      "Domain mAP: 0.7599 --> 0.7895, Class mAP: 0.7753 --> 0.8000\n",
      "KD Epoch [4], Batch [20/87]\n",
      "Total: 4.9424 | Domain KD: 7.4315 | Class KD: 7.1386\n",
      "KD Epoch [4], Batch [40/87]\n",
      "Total: 5.0645 | Domain KD: 9.1293 | Class KD: 6.8320\n",
      "KD Epoch [4], Batch [60/87]\n",
      "Total: 4.9261 | Domain KD: 6.7082 | Class KD: 7.1913\n",
      "KD Epoch [4], Batch [80/87]\n",
      "Total: 4.9160 | Domain KD: 8.8894 | Class KD: 6.5969\n",
      "KD Train Epoch 4:\n",
      "Total Loss: 5.1165 | Domain Acc: 76.13% | Class Acc: 75.40%\n",
      "Domain mAP: 0.7429 | Class mAP: 0.7937\n",
      "KD Losses - Domain: 8.1420 | Class: 7.1938\n",
      "Time: 304.36s\n",
      "\n",
      "Test set: Epoch: 4, Avg loss: 1.1359, Domain Loss: 1.1024, Class Loss: 1.1443, Domain Acc: 74.17%, Class Acc: 74.29%, Domain mAP: 0.8118, Class mAP: 0.8166, Time: 50.11s\n",
      "\n",
      "ðŸ† ìƒˆë¡œìš´ ìµœê³  Class mAP: 0.8166\n",
      "ðŸ† ìƒˆë¡œìš´ ìµœê³  Domain mAP: 0.8118\n",
      "Domain mAP improved (0.7895 --> 0.8118).\n",
      "Class mAP improved (0.8000 --> 0.8166).\n",
      "Performance improved. Saving model ...\n",
      "Domain mAP: 0.7895 --> 0.8118, Class mAP: 0.8000 --> 0.8166\n",
      "KD Epoch [5], Batch [20/87]\n",
      "Total: 4.7525 | Domain KD: 6.8361 | Class KD: 6.9376\n",
      "KD Epoch [5], Batch [40/87]\n",
      "Total: 4.5586 | Domain KD: 8.4959 | Class KD: 6.0755\n",
      "KD Epoch [5], Batch [60/87]\n",
      "Total: 4.9278 | Domain KD: 8.7101 | Class KD: 6.7395\n",
      "KD Epoch [5], Batch [80/87]\n",
      "Total: 4.8851 | Domain KD: 8.4152 | Class KD: 6.7586\n",
      "KD Train Epoch 5:\n",
      "Total Loss: 4.7206 | Domain Acc: 77.19% | Class Acc: 77.85%\n",
      "Domain mAP: 0.7673 | Class mAP: 0.8234\n",
      "KD Losses - Domain: 7.7529 | Class: 6.5825\n",
      "Time: 298.11s\n",
      "\n",
      "Test set: Epoch: 5, Avg loss: 1.0608, Domain Loss: 0.9433, Class Loss: 1.0901, Domain Acc: 76.10%, Class Acc: 75.57%, Domain mAP: 0.8195, Class mAP: 0.8275, Time: 51.32s\n",
      "\n",
      "ðŸ† ìƒˆë¡œìš´ ìµœê³  Class mAP: 0.8275\n",
      "ðŸ† ìƒˆë¡œìš´ ìµœê³  Domain mAP: 0.8195\n",
      "Domain mAP improved (0.8118 --> 0.8195).\n",
      "Class mAP improved (0.8166 --> 0.8275).\n",
      "Performance improved. Saving model ...\n",
      "Domain mAP: 0.8118 --> 0.8195, Class mAP: 0.8166 --> 0.8275\n",
      "KD Epoch [6], Batch [20/87]\n",
      "Total: 4.4420 | Domain KD: 7.3679 | Class KD: 6.1719\n",
      "KD Epoch [6], Batch [40/87]\n",
      "Total: 4.9188 | Domain KD: 7.7422 | Class KD: 6.9829\n",
      "KD Epoch [6], Batch [60/87]\n",
      "Total: 4.7786 | Domain KD: 6.9056 | Class KD: 6.9729\n",
      "KD Epoch [6], Batch [80/87]\n",
      "Total: 4.3142 | Domain KD: 8.2117 | Class KD: 5.6416\n",
      "KD Train Epoch 6:\n",
      "Total Loss: 4.3453 | Domain Acc: 78.28% | Class Acc: 80.64%\n",
      "Domain mAP: 0.7824 | Class mAP: 0.8595\n",
      "KD Losses - Domain: 7.5282 | Class: 5.9604\n",
      "Time: 307.92s\n",
      "\n",
      "Test set: Epoch: 6, Avg loss: 1.1344, Domain Loss: 1.2830, Class Loss: 1.0973, Domain Acc: 73.70%, Class Acc: 75.88%, Domain mAP: 0.8212, Class mAP: 0.8292, Time: 51.11s\n",
      "\n",
      "ðŸ† ìƒˆë¡œìš´ ìµœê³  Class mAP: 0.8292\n",
      "ðŸ† ìƒˆë¡œìš´ ìµœê³  Domain mAP: 0.8212\n",
      "Domain mAP improved (0.8195 --> 0.8212).\n",
      "Class mAP improved (0.8275 --> 0.8292).\n",
      "Performance improved. Saving model ...\n",
      "Domain mAP: 0.8195 --> 0.8212, Class mAP: 0.8275 --> 0.8292\n",
      "KD Epoch [7], Batch [20/87]\n",
      "Total: 3.3176 | Domain KD: 7.2194 | Class KD: 4.2006\n",
      "KD Epoch [7], Batch [40/87]\n",
      "Total: 4.1703 | Domain KD: 6.6178 | Class KD: 5.8655\n",
      "KD Epoch [7], Batch [60/87]\n",
      "Total: 4.1570 | Domain KD: 7.9082 | Class KD: 5.5691\n",
      "KD Epoch [7], Batch [80/87]\n",
      "Total: 4.9279 | Domain KD: 8.9516 | Class KD: 6.5306\n",
      "KD Train Epoch 7:\n",
      "Total Loss: 4.1182 | Domain Acc: 79.12% | Class Acc: 82.35%\n",
      "Domain mAP: 0.7937 | Class mAP: 0.8760\n",
      "KD Losses - Domain: 7.3823 | Class: 5.5885\n",
      "Time: 307.76s\n",
      "\n",
      "Test set: Epoch: 7, Avg loss: 1.1419, Domain Loss: 1.1744, Class Loss: 1.1338, Domain Acc: 76.69%, Class Acc: 75.79%, Domain mAP: 0.8226, Class mAP: 0.8304, Time: 50.38s\n",
      "\n",
      "ðŸ† ìƒˆë¡œìš´ ìµœê³  Class mAP: 0.8304\n",
      "ðŸ† ìƒˆë¡œìš´ ìµœê³  Domain mAP: 0.8226\n",
      "Domain mAP improved (0.8212 --> 0.8226).\n",
      "Class mAP improved (0.8292 --> 0.8304).\n",
      "Performance improved. Saving model ...\n",
      "Domain mAP: 0.8212 --> 0.8226, Class mAP: 0.8292 --> 0.8304\n",
      "KD Epoch [8], Batch [20/87]\n",
      "Total: 3.5874 | Domain KD: 6.1556 | Class KD: 5.0519\n",
      "KD Epoch [8], Batch [40/87]\n",
      "Total: 3.8575 | Domain KD: 6.5074 | Class KD: 5.3679\n",
      "KD Epoch [8], Batch [60/87]\n",
      "Total: 4.2435 | Domain KD: 8.2847 | Class KD: 5.5491\n",
      "KD Epoch [8], Batch [80/87]\n",
      "Total: 4.2713 | Domain KD: 6.6543 | Class KD: 6.1322\n",
      "KD Train Epoch 8:\n",
      "Total Loss: 3.9226 | Domain Acc: 79.22% | Class Acc: 83.13%\n",
      "Domain mAP: 0.7971 | Class mAP: 0.8891\n",
      "KD Losses - Domain: 7.2447 | Class: 5.2631\n",
      "Time: 300.91s\n",
      "\n",
      "Test set: Epoch: 8, Avg loss: 1.0974, Domain Loss: 0.9885, Class Loss: 1.1246, Domain Acc: 77.59%, Class Acc: 76.72%, Domain mAP: 0.8371, Class mAP: 0.8418, Time: 52.21s\n",
      "\n",
      "ðŸ† ìƒˆë¡œìš´ ìµœê³  Class mAP: 0.8418\n",
      "ðŸ† ìƒˆë¡œìš´ ìµœê³  Domain mAP: 0.8371\n",
      "Domain mAP improved (0.8226 --> 0.8371).\n",
      "Class mAP improved (0.8304 --> 0.8418).\n",
      "Performance improved. Saving model ...\n",
      "Domain mAP: 0.8226 --> 0.8371, Class mAP: 0.8304 --> 0.8418\n",
      "KD Epoch [9], Batch [20/87]\n",
      "Total: 3.1689 | Domain KD: 5.8335 | Class KD: 4.3771\n",
      "KD Epoch [13], Batch [60/87]\n",
      "Total: 2.9856 | Domain KD: 5.9775 | Class KD: 3.8920\n",
      "KD Epoch [13], Batch [80/87]\n",
      "Total: 3.0952 | Domain KD: 7.6031 | Class KD: 3.6435\n",
      "KD Train Epoch 13:\n",
      "Total Loss: 3.0994 | Domain Acc: 82.30% | Class Acc: 89.63%\n",
      "Domain mAP: 0.8350 | Class mAP: 0.9506\n",
      "KD Losses - Domain: 6.6169 | Class: 3.9519\n",
      "Time: 305.29s\n",
      "\n",
      "Test set: Epoch: 13, Avg loss: 1.0653, Domain Loss: 0.8491, Class Loss: 1.1193, Domain Acc: 79.93%, Class Acc: 77.84%, Domain mAP: 0.8513, Class mAP: 0.8515, Time: 50.64s\n",
      "\n",
      "ðŸ† ìƒˆë¡œìš´ ìµœê³  Class mAP: 0.8515\n",
      "ðŸ† ìƒˆë¡œìš´ ìµœê³  Domain mAP: 0.8513\n",
      "Domain mAP improved (0.8457 --> 0.8513).\n",
      "Class mAP improved (0.8473 --> 0.8515).\n",
      "Performance improved. Saving model ...\n",
      "Domain mAP: 0.8457 --> 0.8513, Class mAP: 0.8473 --> 0.8515\n",
      "KD Epoch [14], Batch [20/87]\n",
      "Total: 2.8547 | Domain KD: 6.8671 | Class KD: 3.4605\n",
      "KD Epoch [14], Batch [40/87]\n",
      "Total: 3.0756 | Domain KD: 7.2108 | Class KD: 3.7084\n",
      "KD Epoch [14], Batch [60/87]\n",
      "Total: 3.2106 | Domain KD: 6.8134 | Class KD: 4.0884\n",
      "KD Epoch [14], Batch [80/87]\n",
      "Total: 3.1398 | Domain KD: 6.0326 | Class KD: 4.2657\n",
      "KD Train Epoch 14:\n",
      "Total Loss: 2.9710 | Domain Acc: 82.29% | Class Acc: 90.70%\n",
      "Domain mAP: 0.8428 | Class mAP: 0.9579\n",
      "KD Losses - Domain: 6.4170 | Class: 3.7796\n",
      "Time: 303.84s\n",
      "\n",
      "Test set: Epoch: 14, Avg loss: 1.1141, Domain Loss: 0.9235, Class Loss: 1.1618, Domain Acc: 79.37%, Class Acc: 77.90%, Domain mAP: 0.8514, Class mAP: 0.8494, Time: 50.62s\n",
      "\n",
      "ðŸ† ìƒˆë¡œìš´ ìµœê³  Domain mAP: 0.8514\n",
      "Domain mAP improved (0.8513 --> 0.8514).\n",
      "Performance improved. Saving model ...\n",
      "Domain mAP: 0.8513 --> 0.8514, Class mAP: 0.8515 --> 0.8494\n",
      "KD Epoch [15], Batch [20/87]\n",
      "Total: 2.8224 | Domain KD: 5.8386 | Class KD: 3.6466\n",
      "KD Epoch [15], Batch [40/87]\n",
      "Total: 2.9578 | Domain KD: 6.3776 | Class KD: 3.7367\n",
      "KD Epoch [15], Batch [60/87]\n",
      "Total: 3.5846 | Domain KD: 8.1143 | Class KD: 4.3869\n",
      "KD Epoch [15], Batch [80/87]\n",
      "Total: 2.7388 | Domain KD: 5.7893 | Class KD: 3.5327\n",
      "KD Train Epoch 15:\n",
      "Total Loss: 2.8968 | Domain Acc: 82.51% | Class Acc: 91.24%\n",
      "Domain mAP: 0.8430 | Class mAP: 0.9616\n",
      "KD Losses - Domain: 6.4911 | Class: 3.6172\n",
      "Time: 302.62s\n",
      "\n",
      "Test set: Epoch: 15, Avg loss: 1.1688, Domain Loss: 1.0095, Class Loss: 1.2086, Domain Acc: 79.68%, Class Acc: 78.03%, Domain mAP: 0.8481, Class mAP: 0.8526, Time: 50.83s\n",
      "\n",
      "ðŸ† ìƒˆë¡œìš´ ìµœê³  Class mAP: 0.8526\n",
      "Class mAP improved (0.8515 --> 0.8526).\n",
      "Performance improved. Saving model ...\n",
      "Domain mAP: 0.8514 --> 0.8481, Class mAP: 0.8515 --> 0.8526\n",
      "KD Epoch [16], Batch [20/87]\n",
      "Total: 2.6659 | Domain KD: 6.1712 | Class KD: 3.2768\n",
      "KD Epoch [16], Batch [40/87]\n",
      "Total: 2.9840 | Domain KD: 7.2681 | Class KD: 3.4955\n",
      "KD Epoch [16], Batch [60/87]\n",
      "Total: 3.1882 | Domain KD: 6.4984 | Class KD: 4.1574\n",
      "KD Epoch [16], Batch [80/87]\n",
      "Total: 2.8154 | Domain KD: 5.7923 | Class KD: 3.7240\n",
      "KD Train Epoch 16:\n",
      "Total Loss: 2.8003 | Domain Acc: 82.83% | Class Acc: 91.89%\n",
      "Domain mAP: 0.8439 | Class mAP: 0.9679\n",
      "KD Losses - Domain: 6.3356 | Class: 3.4919\n",
      "Time: 303.56s\n",
      "\n",
      "Test set: Epoch: 16, Avg loss: 1.0319, Domain Loss: 0.8782, Class Loss: 1.0703, Domain Acc: 79.71%, Class Acc: 78.84%, Domain mAP: 0.8529, Class mAP: 0.8589, Time: 51.22s\n",
      "\n",
      "ðŸ† ìƒˆë¡œìš´ ìµœê³  Class mAP: 0.8589\n",
      "ðŸ† ìƒˆë¡œìš´ ìµœê³  Domain mAP: 0.8529\n",
      "Domain mAP improved (0.8514 --> 0.8529).\n",
      "Class mAP improved (0.8526 --> 0.8589).\n",
      "Performance improved. Saving model ...\n",
      "Domain mAP: 0.8514 --> 0.8529, Class mAP: 0.8526 --> 0.8589\n",
      "KD Epoch [17], Batch [20/87]\n",
      "Total: 2.5036 | Domain KD: 6.5603 | Class KD: 2.8359\n",
      "KD Epoch [17], Batch [40/87]\n",
      "Total: 2.3610 | Domain KD: 5.8980 | Class KD: 2.8092\n",
      "KD Epoch [17], Batch [60/87]\n",
      "Total: 2.7816 | Domain KD: 6.2220 | Class KD: 3.5771\n",
      "KD Epoch [17], Batch [80/87]\n",
      "Total: 2.8681 | Domain KD: 6.5424 | Class KD: 3.5900\n",
      "KD Train Epoch 17:\n",
      "Total Loss: 2.6912 | Domain Acc: 83.00% | Class Acc: 92.81%\n",
      "Domain mAP: 0.8533 | Class mAP: 0.9728\n",
      "KD Losses - Domain: 6.2397 | Class: 3.3130\n",
      "Time: 302.50s\n",
      "\n",
      "Test set: Epoch: 17, Avg loss: 1.0951, Domain Loss: 0.9141, Class Loss: 1.1403, Domain Acc: 79.55%, Class Acc: 78.80%, Domain mAP: 0.8527, Class mAP: 0.8557, Time: 50.98s\n",
      "\n",
      "EarlyStopping ì¹´ìš´í„°: 1 / 20 (Domain ìµœê³ : 0.8529, Class ìµœê³ : 0.8589)\n",
      "KD Epoch [18], Batch [20/87]\n",
      "Total: 2.5662 | Domain KD: 5.5439 | Class KD: 3.2522\n",
      "KD Epoch [18], Batch [40/87]\n",
      "Total: 2.6876 | Domain KD: 6.8459 | Class KD: 3.1089\n",
      "KD Epoch [18], Batch [60/87]\n",
      "Total: 2.9996 | Domain KD: 6.1342 | Class KD: 3.8778\n",
      "KD Epoch [18], Batch [80/87]\n",
      "Total: 2.6503 | Domain KD: 6.9436 | Class KD: 2.9880\n",
      "KD Train Epoch 18:\n",
      "Total Loss: 2.5973 | Domain Acc: 83.20% | Class Acc: 93.38%\n",
      "Domain mAP: 0.8572 | Class mAP: 0.9759\n",
      "KD Losses - Domain: 6.2211 | Class: 3.1383\n",
      "Time: 303.61s\n",
      "\n",
      "Test set: Epoch: 18, Avg loss: 1.0987, Domain Loss: 0.9468, Class Loss: 1.1367, Domain Acc: 80.39%, Class Acc: 78.65%, Domain mAP: 0.8548, Class mAP: 0.8553, Time: 50.98s\n",
      "\n",
      "ðŸ† ìƒˆë¡œìš´ ìµœê³  Domain mAP: 0.8548\n",
      "Domain mAP improved (0.8529 --> 0.8548).\n",
      "Performance improved. Saving model ...\n",
      "Domain mAP: 0.8529 --> 0.8548, Class mAP: 0.8589 --> 0.8553\n",
      "KD Epoch [19], Batch [20/87]\n",
      "Total: 2.4940 | Domain KD: 6.0542 | Class KD: 2.9600\n",
      "KD Epoch [19], Batch [40/87]\n",
      "Total: 2.3154 | Domain KD: 6.5742 | Class KD: 2.5233\n",
      "KD Epoch [19], Batch [60/87]\n",
      "Total: 2.3402 | Domain KD: 5.4382 | Class KD: 2.9148\n",
      "KD Epoch [19], Batch [80/87]\n",
      "Total: 2.3074 | Domain KD: 5.7384 | Class KD: 2.7780\n",
      "KD Train Epoch 19:\n",
      "Total Loss: 2.5352 | Domain Acc: 83.65% | Class Acc: 93.81%\n",
      "Domain mAP: 0.8538 | Class mAP: 0.9800\n",
      "KD Losses - Domain: 6.1660 | Class: 3.0446\n",
      "Time: 301.74s\n",
      "\n",
      "Test set: Epoch: 19, Avg loss: 1.1767, Domain Loss: 1.1365, Class Loss: 1.1868, Domain Acc: 79.30%, Class Acc: 78.96%, Domain mAP: 0.8527, Class mAP: 0.8555, Time: 52.04s\n",
      "\n",
      "EarlyStopping ì¹´ìš´í„°: 1 / 20 (Domain ìµœê³ : 0.8548, Class ìµœê³ : 0.8589)\n",
      "KD Epoch [20], Batch [20/87]\n",
      "Total: 2.1497 | Domain KD: 5.5873 | Class KD: 2.5135\n",
      "KD Epoch [20], Batch [40/87]\n",
      "Total: 2.4339 | Domain KD: 6.8459 | Class KD: 2.6557\n",
      "KD Epoch [20], Batch [60/87]\n",
      "Total: 2.6144 | Domain KD: 5.9118 | Class KD: 3.2739\n",
      "KD Epoch [20], Batch [80/87]\n",
      "Total: 2.2093 | Domain KD: 6.2069 | Class KD: 2.4192\n",
      "KD Train Epoch 20:\n",
      "Total Loss: 2.4745 | Domain Acc: 83.28% | Class Acc: 94.24%\n",
      "Domain mAP: 0.8550 | Class mAP: 0.9825\n",
      "KD Losses - Domain: 6.0989 | Class: 2.9477\n",
      "Time: 301.00s\n",
      "\n",
      "Test set: Epoch: 20, Avg loss: 1.1255, Domain Loss: 1.1291, Class Loss: 1.1246, Domain Acc: 77.90%, Class Acc: 79.37%, Domain mAP: 0.8523, Class mAP: 0.8608, Time: 50.29s\n",
      "\n",
      "ðŸ† ìƒˆë¡œìš´ ìµœê³  Class mAP: 0.8608\n",
      "Class mAP improved (0.8589 --> 0.8608).\n",
      "Performance improved. Saving model ...\n",
      "Domain mAP: 0.8548 --> 0.8523, Class mAP: 0.8589 --> 0.8608\n",
      "KD Epoch [21], Batch [20/87]\n",
      "Total: 2.2860 | Domain KD: 6.5816 | Class KD: 2.4151\n",
      "KD Epoch [21], Batch [40/87]\n",
      "Total: 2.2091 | Domain KD: 5.6999 | Class KD: 2.5738\n",
      "KD Epoch [21], Batch [60/87]\n",
      "Total: 2.2863 | Domain KD: 6.3710 | Class KD: 2.5382\n",
      "KD Epoch [21], Batch [80/87]\n",
      "Total: 2.2518 | Domain KD: 5.7216 | Class KD: 2.6795\n",
      "KD Train Epoch 21:\n",
      "Total Loss: 2.3832 | Domain Acc: 84.17% | Class Acc: 95.05%\n",
      "Domain mAP: 0.8628 | Class mAP: 0.9864\n",
      "KD Losses - Domain: 5.9719 | Class: 2.8158\n",
      "Time: 309.51s\n",
      "\n",
      "Test set: Epoch: 21, Avg loss: 1.1739, Domain Loss: 1.2905, Class Loss: 1.1448, Domain Acc: 74.88%, Class Acc: 79.40%, Domain mAP: 0.8487, Class mAP: 0.8579, Time: 50.30s\n",
      "\n",
      "EarlyStopping ì¹´ìš´í„°: 1 / 20 (Domain ìµœê³ : 0.8548, Class ìµœê³ : 0.8608)\n",
      "KD Epoch [22], Batch [20/87]\n",
      "Total: 2.3358 | Domain KD: 6.2085 | Class KD: 2.6474\n",
      "KD Epoch [22], Batch [40/87]\n",
      "Total: 2.3346 | Domain KD: 5.9369 | Class KD: 2.6981\n",
      "KD Epoch [22], Batch [60/87]\n",
      "Total: 2.2804 | Domain KD: 5.3461 | Class KD: 2.7688\n",
      "KD Epoch [22], Batch [80/87]\n",
      "Total: 2.5353 | Domain KD: 6.8090 | Class KD: 2.8591\n",
      "KD Train Epoch 22:\n",
      "Total Loss: 2.3659 | Domain Acc: 84.60% | Class Acc: 95.17%\n",
      "Domain mAP: 0.8684 | Class mAP: 0.9854\n",
      "KD Losses - Domain: 5.9996 | Class: 2.7705\n",
      "Time: 302.79s\n",
      "\n",
      "Test set: Epoch: 22, Avg loss: 1.0927, Domain Loss: 0.8643, Class Loss: 1.1498, Domain Acc: 80.77%, Class Acc: 78.71%, Domain mAP: 0.8557, Class mAP: 0.8580, Time: 50.88s\n",
      "\n",
      "ðŸ† ìƒˆë¡œìš´ ìµœê³  Domain mAP: 0.8557\n",
      "Domain mAP improved (0.8548 --> 0.8557).\n",
      "Performance improved. Saving model ...\n",
      "Domain mAP: 0.8548 --> 0.8557, Class mAP: 0.8608 --> 0.8580\n",
      "KD Epoch [23], Batch [20/87]\n",
      "Total: 2.2187 | Domain KD: 6.1909 | Class KD: 2.4447\n",
      "KD Epoch [23], Batch [40/87]\n",
      "Total: 2.2155 | Domain KD: 5.5376 | Class KD: 2.6318\n",
      "KD Epoch [23], Batch [60/87]\n",
      "Total: 2.4406 | Domain KD: 6.4365 | Class KD: 2.8037\n",
      "KD Epoch [23], Batch [80/87]\n",
      "Total: 2.4212 | Domain KD: 5.7264 | Class KD: 2.9472\n",
      "KD Train Epoch 23:\n",
      "Total Loss: 2.3024 | Domain Acc: 84.40% | Class Acc: 95.62%\n",
      "Domain mAP: 0.8695 | Class mAP: 0.9885\n",
      "KD Losses - Domain: 6.0104 | Class: 2.6492\n",
      "Time: 314.11s\n",
      "\n",
      "Test set: Epoch: 23, Avg loss: 1.1567, Domain Loss: 1.0555, Class Loss: 1.1820, Domain Acc: 79.93%, Class Acc: 79.05%, Domain mAP: 0.8569, Class mAP: 0.8597, Time: 50.34s\n",
      "\n",
      "ðŸ† ìƒˆë¡œìš´ ìµœê³  Domain mAP: 0.8569\n",
      "Domain mAP improved (0.8557 --> 0.8569).\n",
      "Performance improved. Saving model ...\n",
      "Domain mAP: 0.8557 --> 0.8569, Class mAP: 0.8608 --> 0.8597\n",
      "KD Epoch [24], Batch [20/87]\n",
      "Total: 2.2026 | Domain KD: 5.2151 | Class KD: 2.7431\n",
      "KD Epoch [24], Batch [40/87]\n",
      "Total: 2.4382 | Domain KD: 6.3070 | Class KD: 2.8309\n",
      "KD Epoch [24], Batch [60/87]\n",
      "Total: 2.2933 | Domain KD: 6.4634 | Class KD: 2.4808\n",
      "KD Epoch [24], Batch [80/87]\n",
      "Total: 2.3685 | Domain KD: 6.6393 | Class KD: 2.5452\n",
      "KD Train Epoch 24:\n",
      "Total Loss: 2.2427 | Domain Acc: 84.98% | Class Acc: 95.51%\n",
      "Domain mAP: 0.8726 | Class mAP: 0.9887\n",
      "KD Losses - Domain: 5.7861 | Class: 2.6024\n",
      "Time: 311.88s\n",
      "\n",
      "Test set: Epoch: 24, Avg loss: 1.0868, Domain Loss: 0.9652, Class Loss: 1.1172, Domain Acc: 79.86%, Class Acc: 78.80%, Domain mAP: 0.8576, Class mAP: 0.8560, Time: 50.39s\n",
      "\n",
      "ðŸ† ìƒˆë¡œìš´ ìµœê³  Domain mAP: 0.8576\n",
      "Domain mAP improved (0.8569 --> 0.8576).\n",
      "Performance improved. Saving model ...\n",
      "Domain mAP: 0.8569 --> 0.8576, Class mAP: 0.8608 --> 0.8560\n",
      "KD Epoch [25], Batch [20/87]\n",
      "Total: 2.0544 | Domain KD: 4.2968 | Class KD: 2.6983\n",
      "KD Epoch [25], Batch [40/87]\n",
      "Total: 2.4462 | Domain KD: 6.3606 | Class KD: 2.7865\n",
      "KD Epoch [25], Batch [60/87]\n",
      "Total: 2.3175 | Domain KD: 6.6432 | Class KD: 2.5400\n",
      "KD Epoch [25], Batch [80/87]\n",
      "Total: 2.5257 | Domain KD: 6.6091 | Class KD: 2.8558\n",
      "KD Train Epoch 25:\n",
      "Total Loss: 2.1726 | Domain Acc: 85.65% | Class Acc: 96.01%\n",
      "Domain mAP: 0.8826 | Class mAP: 0.9909\n",
      "KD Losses - Domain: 5.6414 | Class: 2.5171\n",
      "Time: 307.36s\n",
      "\n",
      "Test set: Epoch: 25, Avg loss: 1.0423, Domain Loss: 0.9557, Class Loss: 1.0639, Domain Acc: 80.86%, Class Acc: 79.83%, Domain mAP: 0.8607, Class mAP: 0.8632, Time: 50.97s\n",
      "\n",
      "ðŸ† ìƒˆë¡œìš´ ìµœê³  Class mAP: 0.8632\n",
      "ðŸ† ìƒˆë¡œìš´ ìµœê³  Domain mAP: 0.8607\n",
      "Domain mAP improved (0.8576 --> 0.8607).\n",
      "Class mAP improved (0.8608 --> 0.8632).\n",
      "Performance improved. Saving model ...\n",
      "Domain mAP: 0.8576 --> 0.8607, Class mAP: 0.8608 --> 0.8632\n",
      "KD Epoch [26], Batch [20/87]\n",
      "Total: 1.8326 | Domain KD: 4.7516 | Class KD: 2.1522\n",
      "KD Epoch [26], Batch [40/87]\n",
      "Total: 1.8699 | Domain KD: 5.2086 | Class KD: 2.0355\n",
      "KD Epoch [26], Batch [60/87]\n",
      "Total: 1.9690 | Domain KD: 6.0557 | Class KD: 1.9896\n",
      "KD Epoch [26], Batch [80/87]\n",
      "Total: 2.1983 | Domain KD: 6.5696 | Class KD: 2.2758\n",
      "KD Train Epoch 26:\n",
      "Total Loss: 2.1261 | Domain Acc: 85.31% | Class Acc: 96.65%\n",
      "Domain mAP: 0.8813 | Class mAP: 0.9924\n",
      "KD Losses - Domain: 5.7042 | Class: 2.4083\n",
      "Time: 305.68s\n",
      "\n",
      "Test set: Epoch: 26, Avg loss: 1.2455, Domain Loss: 1.5009, Class Loss: 1.1816, Domain Acc: 75.97%, Class Acc: 79.21%, Domain mAP: 0.8464, Class mAP: 0.8566, Time: 50.52s\n",
      "\n",
      "EarlyStopping ì¹´ìš´í„°: 1 / 20 (Domain ìµœê³ : 0.8607, Class ìµœê³ : 0.8632)\n",
      "KD Epoch [27], Batch [20/87]\n",
      "Total: 2.3101 | Domain KD: 6.6346 | Class KD: 2.4938\n",
      "KD Epoch [27], Batch [40/87]\n",
      "Total: 1.9624 | Domain KD: 4.8948 | Class KD: 2.3606\n",
      "KD Epoch [27], Batch [60/87]\n",
      "Total: 2.2225 | Domain KD: 5.9673 | Class KD: 2.5253\n",
      "KD Epoch [27], Batch [80/87]\n",
      "Total: 2.1785 | Domain KD: 6.4880 | Class KD: 2.2761\n",
      "KD Train Epoch 27:\n",
      "Total Loss: 2.1081 | Domain Acc: 85.32% | Class Acc: 96.55%\n",
      "Domain mAP: 0.8795 | Class mAP: 0.9924\n",
      "KD Losses - Domain: 5.6421 | Class: 2.3893\n",
      "Time: 301.74s\n",
      "\n",
      "Test set: Epoch: 27, Avg loss: 1.1162, Domain Loss: 0.8901, Class Loss: 1.1728, Domain Acc: 81.01%, Class Acc: 79.27%, Domain mAP: 0.8613, Class mAP: 0.8579, Time: 50.17s\n",
      "\n",
      "ðŸ† ìƒˆë¡œìš´ ìµœê³  Domain mAP: 0.8613\n",
      "Domain mAP improved (0.8607 --> 0.8613).\n",
      "Performance improved. Saving model ...\n",
      "Domain mAP: 0.8607 --> 0.8613, Class mAP: 0.8632 --> 0.8579\n",
      "KD Epoch [28], Batch [20/87]\n",
      "Total: 1.7863 | Domain KD: 5.2878 | Class KD: 1.8913\n",
      "KD Epoch [28], Batch [40/87]\n",
      "Total: 2.0587 | Domain KD: 5.8997 | Class KD: 2.2552\n",
      "KD Epoch [28], Batch [60/87]\n",
      "Total: 2.2654 | Domain KD: 7.0135 | Class KD: 2.2812\n",
      "KD Epoch [28], Batch [80/87]\n",
      "Total: 2.3023 | Domain KD: 4.7215 | Class KD: 3.0181\n",
      "KD Train Epoch 28:\n",
      "Total Loss: 2.0707 | Domain Acc: 86.10% | Class Acc: 96.95%\n",
      "Domain mAP: 0.8830 | Class mAP: 0.9936\n",
      "KD Losses - Domain: 5.6091 | Class: 2.3312\n",
      "Time: 301.95s\n",
      "\n",
      "Test set: Epoch: 28, Avg loss: 1.1070, Domain Loss: 1.0999, Class Loss: 1.1088, Domain Acc: 79.96%, Class Acc: 79.46%, Domain mAP: 0.8578, Class mAP: 0.8583, Time: 50.67s\n",
      "\n",
      "EarlyStopping ì¹´ìš´í„°: 1 / 20 (Domain ìµœê³ : 0.8613, Class ìµœê³ : 0.8632)\n",
      "KD Epoch [29], Batch [20/87]\n",
      "Total: 1.9956 | Domain KD: 5.4563 | Class KD: 2.2418\n",
      "KD Epoch [29], Batch [40/87]\n",
      "Total: 2.2536 | Domain KD: 6.3989 | Class KD: 2.4392\n",
      "KD Epoch [29], Batch [60/87]\n",
      "Total: 2.3940 | Domain KD: 5.8731 | Class KD: 2.8962\n",
      "KD Epoch [29], Batch [80/87]\n",
      "Total: 2.1522 | Domain KD: 5.8399 | Class KD: 2.4056\n",
      "KD Train Epoch 29:\n",
      "Total Loss: 2.0433 | Domain Acc: 85.91% | Class Acc: 96.75%\n",
      "Domain mAP: 0.8838 | Class mAP: 0.9942\n",
      "KD Losses - Domain: 5.5938 | Class: 2.2809\n",
      "Time: 301.78s\n",
      "\n",
      "Test set: Epoch: 29, Avg loss: 1.1677, Domain Loss: 0.9417, Class Loss: 1.2242, Domain Acc: 81.01%, Class Acc: 78.65%, Domain mAP: 0.8653, Class mAP: 0.8550, Time: 52.75s\n",
      "\n",
      "ðŸ† ìƒˆë¡œìš´ ìµœê³  Domain mAP: 0.8653\n",
      "Domain mAP improved (0.8613 --> 0.8653).\n",
      "Performance improved. Saving model ...\n",
      "Domain mAP: 0.8613 --> 0.8653, Class mAP: 0.8632 --> 0.8550\n",
      "KD Epoch [30], Batch [20/87]\n",
      "Total: 2.0966 | Domain KD: 4.7246 | Class KD: 2.6661\n",
      "KD Epoch [30], Batch [40/87]\n",
      "Total: 2.1221 | Domain KD: 5.5710 | Class KD: 2.4579\n",
      "KD Epoch [30], Batch [60/87]\n",
      "Total: 1.7472 | Domain KD: 4.5444 | Class KD: 2.0491\n",
      "KD Epoch [30], Batch [80/87]\n",
      "Total: 2.3023 | Domain KD: 6.7828 | Class KD: 2.4193\n",
      "KD Train Epoch 30:\n",
      "Total Loss: 1.9866 | Domain Acc: 86.23% | Class Acc: 97.15%\n",
      "Domain mAP: 0.8904 | Class mAP: 0.9945\n",
      "KD Losses - Domain: 5.4646 | Class: 2.2122\n",
      "Time: 302.38s\n",
      "\n",
      "Test set: Epoch: 30, Avg loss: 1.0666, Domain Loss: 1.1915, Class Loss: 1.0353, Domain Acc: 77.90%, Class Acc: 80.02%, Domain mAP: 0.8577, Class mAP: 0.8641, Time: 51.44s\n",
      "\n",
      "ðŸ† ìƒˆë¡œìš´ ìµœê³  Class mAP: 0.8641\n",
      "Class mAP improved (0.8632 --> 0.8641).\n",
      "Performance improved. Saving model ...\n",
      "Domain mAP: 0.8653 --> 0.8577, Class mAP: 0.8632 --> 0.8641\n",
      "KD Epoch [31], Batch [20/87]\n",
      "Total: 1.9641 | Domain KD: 5.7087 | Class KD: 2.1184\n",
      "KD Epoch [31], Batch [40/87]\n",
      "Total: 1.8025 | Domain KD: 5.1820 | Class KD: 1.9542\n",
      "KD Epoch [31], Batch [60/87]\n",
      "Total: 1.8533 | Domain KD: 4.9986 | Class KD: 2.1255\n",
      "KD Epoch [31], Batch [80/87]\n",
      "Total: 2.0800 | Domain KD: 5.2691 | Class KD: 2.4290\n",
      "KD Train Epoch 31:\n",
      "Total Loss: 1.9961 | Domain Acc: 85.63% | Class Acc: 97.42%\n",
      "Domain mAP: 0.8807 | Class mAP: 0.9946\n",
      "KD Losses - Domain: 5.6299 | Class: 2.1756\n",
      "Time: 301.95s\n",
      "\n",
      "Test set: Epoch: 31, Avg loss: 1.1374, Domain Loss: 0.9602, Class Loss: 1.1817, Domain Acc: 81.85%, Class Acc: 79.30%, Domain mAP: 0.8635, Class mAP: 0.8599, Time: 51.75s\n",
      "\n",
      "EarlyStopping ì¹´ìš´í„°: 1 / 20 (Domain ìµœê³ : 0.8653, Class ìµœê³ : 0.8641)\n",
      "KD Epoch [32], Batch [20/87]\n",
      "Total: 1.8693 | Domain KD: 5.0640 | Class KD: 2.0929\n",
      "KD Epoch [32], Batch [40/87]\n",
      "Total: 1.9639 | Domain KD: 5.4431 | Class KD: 2.1987\n",
      "KD Epoch [32], Batch [60/87]\n",
      "Total: 1.8873 | Domain KD: 5.1203 | Class KD: 2.1359\n",
      "KD Epoch [32], Batch [80/87]\n",
      "Total: 1.6356 | Domain KD: 4.4599 | Class KD: 1.8823\n",
      "KD Train Epoch 32:\n",
      "Total Loss: 1.7622 | Domain Acc: 87.36% | Class Acc: 97.85%\n",
      "Domain mAP: 0.9056 | Class mAP: 0.9970\n",
      "KD Losses - Domain: 5.0416 | Class: 1.9082\n",
      "Time: 311.97s\n",
      "\n",
      "Test set: Epoch: 32, Avg loss: 1.0517, Domain Loss: 1.1038, Class Loss: 1.0387, Domain Acc: 79.89%, Class Acc: 80.83%, Domain mAP: 0.8626, Class mAP: 0.8692, Time: 50.03s\n",
      "\n",
      "ðŸ† ìƒˆë¡œìš´ ìµœê³  Class mAP: 0.8692\n",
      "Class mAP improved (0.8641 --> 0.8692).\n",
      "Performance improved. Saving model ...\n",
      "Domain mAP: 0.8653 --> 0.8626, Class mAP: 0.8641 --> 0.8692\n",
      "KD Epoch [33], Batch [20/87]\n",
      "Total: 1.7478 | Domain KD: 4.8626 | Class KD: 1.9553\n",
      "KD Epoch [33], Batch [40/87]\n",
      "Total: 1.5411 | Domain KD: 4.9967 | Class KD: 1.4788\n",
      "KD Epoch [33], Batch [60/87]\n",
      "Total: 1.8261 | Domain KD: 6.1269 | Class KD: 1.6754\n",
      "KD Epoch [33], Batch [80/87]\n",
      "Total: 1.4760 | Domain KD: 4.6889 | Class KD: 1.4664\n",
      "KD Train Epoch 33:\n",
      "Total Loss: 1.7038 | Domain Acc: 87.87% | Class Acc: 98.14%\n",
      "Domain mAP: 0.9087 | Class mAP: 0.9977\n",
      "KD Losses - Domain: 5.0042 | Class: 1.8075\n",
      "Time: 297.26s\n",
      "\n",
      "Test set: Epoch: 33, Avg loss: 1.0093, Domain Loss: 0.9836, Class Loss: 1.0158, Domain Acc: 81.08%, Class Acc: 81.17%, Domain mAP: 0.8645, Class mAP: 0.8711, Time: 50.21s\n",
      "\n",
      "ðŸ† ìƒˆë¡œìš´ ìµœê³  Class mAP: 0.8711\n",
      "Class mAP improved (0.8692 --> 0.8711).\n",
      "Performance improved. Saving model ...\n",
      "Domain mAP: 0.8653 --> 0.8645, Class mAP: 0.8692 --> 0.8711\n",
      "KD Epoch [34], Batch [20/87]\n",
      "Total: 1.8066 | Domain KD: 5.3924 | Class KD: 1.9048\n",
      "KD Epoch [34], Batch [40/87]\n",
      "Total: 1.4484 | Domain KD: 4.1190 | Class KD: 1.5961\n",
      "KD Epoch [34], Batch [60/87]\n",
      "Total: 1.5458 | Domain KD: 4.9070 | Class KD: 1.5116\n",
      "KD Epoch [34], Batch [80/87]\n",
      "Total: 1.5453 | Domain KD: 4.2873 | Class KD: 1.6997\n",
      "KD Train Epoch 34:\n",
      "Total Loss: 1.6777 | Domain Acc: 87.49% | Class Acc: 98.44%\n",
      "Domain mAP: 0.9062 | Class mAP: 0.9981\n",
      "KD Losses - Domain: 4.9686 | Class: 1.7678\n",
      "Time: 309.50s\n",
      "\n",
      "Test set: Epoch: 34, Avg loss: 1.0158, Domain Loss: 1.0138, Class Loss: 1.0163, Domain Acc: 80.77%, Class Acc: 81.14%, Domain mAP: 0.8642, Class mAP: 0.8698, Time: 50.05s\n",
      "\n",
      "EarlyStopping ì¹´ìš´í„°: 1 / 20 (Domain ìµœê³ : 0.8653, Class ìµœê³ : 0.8711)\n",
      "KD Epoch [35], Batch [20/87]\n",
      "Total: 1.6361 | Domain KD: 5.0594 | Class KD: 1.6653\n",
      "KD Epoch [35], Batch [40/87]\n",
      "Total: 1.7361 | Domain KD: 4.9426 | Class KD: 1.8580\n",
      "KD Epoch [35], Batch [60/87]\n",
      "Total: 1.7374 | Domain KD: 5.2555 | Class KD: 1.7857\n",
      "KD Epoch [35], Batch [80/87]\n",
      "Total: 1.4933 | Domain KD: 4.1202 | Class KD: 1.6984\n",
      "KD Train Epoch 35:\n",
      "Total Loss: 1.6752 | Domain Acc: 87.63% | Class Acc: 98.58%\n",
      "Domain mAP: 0.9067 | Class mAP: 0.9982\n",
      "KD Losses - Domain: 5.0235 | Class: 1.7458\n",
      "Time: 300.32s\n",
      "\n",
      "Test set: Epoch: 35, Avg loss: 1.0155, Domain Loss: 0.9870, Class Loss: 1.0226, Domain Acc: 81.01%, Class Acc: 81.29%, Domain mAP: 0.8646, Class mAP: 0.8704, Time: 50.38s\n",
      "\n",
      "EarlyStopping ì¹´ìš´í„°: 2 / 20 (Domain ìµœê³ : 0.8653, Class ìµœê³ : 0.8711)\n",
      "KD Epoch [36], Batch [20/87]\n",
      "Total: 1.6758 | Domain KD: 4.8640 | Class KD: 1.7937\n",
      "KD Epoch [36], Batch [40/87]\n",
      "Total: 1.4851 | Domain KD: 4.2331 | Class KD: 1.6259\n",
      "KD Epoch [36], Batch [60/87]\n",
      "Total: 1.8079 | Domain KD: 5.7341 | Class KD: 1.7806\n",
      "KD Epoch [36], Batch [80/87]\n",
      "Total: 1.5428 | Domain KD: 4.4300 | Class KD: 1.6619\n",
      "KD Train Epoch 36:\n",
      "Total Loss: 1.6606 | Domain Acc: 87.86% | Class Acc: 98.50%\n",
      "Domain mAP: 0.9079 | Class mAP: 0.9982\n",
      "KD Losses - Domain: 5.0002 | Class: 1.7233\n",
      "Time: 299.40s\n",
      "\n",
      "Test set: Epoch: 36, Avg loss: 1.0202, Domain Loss: 1.1103, Class Loss: 0.9977, Domain Acc: 79.93%, Class Acc: 81.23%, Domain mAP: 0.8622, Class mAP: 0.8705, Time: 50.12s\n",
      "\n",
      "EarlyStopping ì¹´ìš´í„°: 3 / 20 (Domain ìµœê³ : 0.8653, Class ìµœê³ : 0.8711)\n",
      "KD Epoch [37], Batch [20/87]\n",
      "Total: 1.5378 | Domain KD: 5.2080 | Class KD: 1.4222\n",
      "KD Epoch [37], Batch [40/87]\n",
      "Total: 1.6090 | Domain KD: 4.8492 | Class KD: 1.6726\n",
      "KD Epoch [37], Batch [60/87]\n",
      "Total: 1.6284 | Domain KD: 5.3802 | Class KD: 1.5415\n",
      "KD Epoch [37], Batch [80/87]\n",
      "Total: 1.4767 | Domain KD: 4.4641 | Class KD: 1.5245\n",
      "KD Train Epoch 37:\n",
      "Total Loss: 1.6336 | Domain Acc: 88.07% | Class Acc: 98.60%\n",
      "Domain mAP: 0.9115 | Class mAP: 0.9985\n",
      "KD Losses - Domain: 4.9475 | Class: 1.6873\n",
      "Time: 300.39s\n",
      "\n",
      "Test set: Epoch: 37, Avg loss: 1.0116, Domain Loss: 1.0501, Class Loss: 1.0020, Domain Acc: 80.42%, Class Acc: 81.11%, Domain mAP: 0.8631, Class mAP: 0.8713, Time: 49.85s\n",
      "\n",
      "ðŸ† ìƒˆë¡œìš´ ìµœê³  Class mAP: 0.8713\n",
      "Class mAP improved (0.8711 --> 0.8713).\n",
      "Performance improved. Saving model ...\n",
      "Domain mAP: 0.8653 --> 0.8631, Class mAP: 0.8711 --> 0.8713\n",
      "KD Epoch [38], Batch [20/87]\n",
      "Total: 1.7671 | Domain KD: 5.2344 | Class KD: 1.8304\n",
      "KD Epoch [38], Batch [40/87]\n",
      "Total: 1.5417 | Domain KD: 4.7931 | Class KD: 1.5733\n",
      "KD Epoch [38], Batch [60/87]\n",
      "Total: 1.8568 | Domain KD: 6.0221 | Class KD: 1.7938\n",
      "KD Epoch [38], Batch [80/87]\n",
      "Total: 1.8546 | Domain KD: 6.1417 | Class KD: 1.7566\n",
      "KD Train Epoch 38:\n",
      "Total Loss: 1.6300 | Domain Acc: 88.49% | Class Acc: 98.71%\n",
      "Domain mAP: 0.9104 | Class mAP: 0.9985\n",
      "KD Losses - Domain: 4.9019 | Class: 1.6955\n",
      "Time: 297.45s\n",
      "\n",
      "Test set: Epoch: 38, Avg loss: 1.0103, Domain Loss: 0.9753, Class Loss: 1.0190, Domain Acc: 81.29%, Class Acc: 81.51%, Domain mAP: 0.8642, Class mAP: 0.8716, Time: 51.32s\n",
      "\n",
      "ðŸ† ìƒˆë¡œìš´ ìµœê³  Class mAP: 0.8716\n",
      "Class mAP improved (0.8713 --> 0.8716).\n",
      "Performance improved. Saving model ...\n",
      "Domain mAP: 0.8653 --> 0.8642, Class mAP: 0.8713 --> 0.8716\n",
      "KD Epoch [39], Batch [20/87]\n",
      "Total: 1.6309 | Domain KD: 5.1970 | Class KD: 1.5826\n",
      "KD Epoch [39], Batch [40/87]\n",
      "Total: 1.5767 | Domain KD: 4.9011 | Class KD: 1.6164\n",
      "KD Epoch [39], Batch [60/87]\n",
      "Total: 1.4670 | Domain KD: 4.0166 | Class KD: 1.6515\n",
      "KD Epoch [39], Batch [80/87]\n",
      "Total: 1.7729 | Domain KD: 4.9907 | Class KD: 1.9338\n",
      "KD Train Epoch 39:\n",
      "Total Loss: 1.6203 | Domain Acc: 88.14% | Class Acc: 98.60%\n",
      "Domain mAP: 0.9136 | Class mAP: 0.9984\n",
      "KD Losses - Domain: 4.9127 | Class: 1.6718\n",
      "Time: 310.99s\n",
      "\n",
      "Test set: Epoch: 39, Avg loss: 1.0138, Domain Loss: 1.0511, Class Loss: 1.0044, Domain Acc: 80.67%, Class Acc: 81.08%, Domain mAP: 0.8628, Class mAP: 0.8703, Time: 50.65s\n",
      "\n",
      "EarlyStopping ì¹´ìš´í„°: 1 / 20 (Domain ìµœê³ : 0.8653, Class ìµœê³ : 0.8716)\n",
      "KD Epoch [40], Batch [20/87]\n",
      "Total: 1.4743 | Domain KD: 4.2885 | Class KD: 1.6139\n",
      "KD Epoch [40], Batch [40/87]\n",
      "Total: 1.6374 | Domain KD: 4.8003 | Class KD: 1.7667\n",
      "KD Epoch [40], Batch [60/87]\n",
      "Total: 1.5202 | Domain KD: 4.6011 | Class KD: 1.5857\n",
      "KD Epoch [40], Batch [80/87]\n",
      "Total: 1.6390 | Domain KD: 4.9211 | Class KD: 1.7138\n",
      "KD Train Epoch 40:\n",
      "Total Loss: 1.6300 | Domain Acc: 87.84% | Class Acc: 98.70%\n",
      "Domain mAP: 0.9098 | Class mAP: 0.9986\n",
      "KD Losses - Domain: 4.9394 | Class: 1.6836\n",
      "Time: 302.90s\n",
      "\n",
      "Test set: Epoch: 40, Avg loss: 1.0023, Domain Loss: 1.0084, Class Loss: 1.0008, Domain Acc: 81.14%, Class Acc: 81.29%, Domain mAP: 0.8642, Class mAP: 0.8709, Time: 50.59s\n",
      "\n",
      "EarlyStopping ì¹´ìš´í„°: 2 / 20 (Domain ìµœê³ : 0.8653, Class ìµœê³ : 0.8716)\n",
      "KD Epoch [41], Batch [20/87]\n",
      "Total: 1.5242 | Domain KD: 4.3377 | Class KD: 1.6689\n",
      "KD Epoch [41], Batch [40/87]\n",
      "Total: 1.5080 | Domain KD: 4.7186 | Class KD: 1.5351\n",
      "KD Epoch [41], Batch [60/87]\n",
      "Total: 1.6134 | Domain KD: 4.7983 | Class KD: 1.7072\n",
      "KD Epoch [41], Batch [80/87]\n",
      "Total: 1.6010 | Domain KD: 4.6979 | Class KD: 1.7162\n",
      "KD Train Epoch 41:\n",
      "Total Loss: 1.6105 | Domain Acc: 88.17% | Class Acc: 98.63%\n",
      "Domain mAP: 0.9121 | Class mAP: 0.9987\n",
      "KD Losses - Domain: 4.9408 | Class: 1.6438\n",
      "Time: 309.21s\n",
      "\n",
      "Test set: Epoch: 41, Avg loss: 1.0016, Domain Loss: 1.0116, Class Loss: 0.9991, Domain Acc: 80.83%, Class Acc: 81.33%, Domain mAP: 0.8633, Class mAP: 0.8716, Time: 50.72s\n",
      "\n",
      "ðŸ† ìƒˆë¡œìš´ ìµœê³  Class mAP: 0.8716\n",
      "Class mAP improved (0.8716 --> 0.8716).\n",
      "Performance improved. Saving model ...\n",
      "Domain mAP: 0.8653 --> 0.8633, Class mAP: 0.8716 --> 0.8716\n",
      "KD Epoch [42], Batch [20/87]\n",
      "Total: 1.5048 | Domain KD: 4.7877 | Class KD: 1.4860\n",
      "KD Epoch [42], Batch [40/87]\n",
      "Total: 1.6473 | Domain KD: 5.2963 | Class KD: 1.5998\n",
      "KD Epoch [42], Batch [60/87]\n",
      "Total: 1.6824 | Domain KD: 5.3276 | Class KD: 1.6746\n",
      "KD Epoch [42], Batch [80/87]\n",
      "Total: 1.6627 | Domain KD: 5.3533 | Class KD: 1.6044\n",
      "KD Train Epoch 42:\n",
      "Total Loss: 1.6053 | Domain Acc: 88.43% | Class Acc: 98.58%\n",
      "Domain mAP: 0.9107 | Class mAP: 0.9987\n",
      "KD Losses - Domain: 4.9277 | Class: 1.6373\n",
      "Time: 298.50s\n",
      "\n",
      "Test set: Epoch: 42, Avg loss: 0.9848, Domain Loss: 1.0189, Class Loss: 0.9762, Domain Acc: 80.95%, Class Acc: 81.42%, Domain mAP: 0.8635, Class mAP: 0.8729, Time: 51.25s\n",
      "\n",
      "ðŸ† ìƒˆë¡œìš´ ìµœê³  Class mAP: 0.8729\n",
      "Class mAP improved (0.8716 --> 0.8729).\n",
      "Performance improved. Saving model ...\n",
      "Domain mAP: 0.8653 --> 0.8635, Class mAP: 0.8716 --> 0.8729\n",
      "KD Epoch [43], Batch [20/87]\n",
      "Total: 1.6609 | Domain KD: 5.6999 | Class KD: 1.4907\n",
      "KD Epoch [43], Batch [40/87]\n",
      "Total: 1.6701 | Domain KD: 5.8186 | Class KD: 1.4745\n",
      "KD Epoch [43], Batch [60/87]\n",
      "Total: 1.6064 | Domain KD: 5.0007 | Class KD: 1.6035\n",
      "KD Epoch [43], Batch [80/87]\n",
      "Total: 1.4527 | Domain KD: 4.1625 | Class KD: 1.5960\n",
      "KD Train Epoch 43:\n",
      "Total Loss: 1.6074 | Domain Acc: 88.01% | Class Acc: 98.72%\n",
      "Domain mAP: 0.9098 | Class mAP: 0.9986\n",
      "KD Losses - Domain: 4.9396 | Class: 1.6369\n",
      "Time: 298.19s\n",
      "\n",
      "Test set: Epoch: 43, Avg loss: 1.0110, Domain Loss: 1.0009, Class Loss: 1.0136, Domain Acc: 81.39%, Class Acc: 81.26%, Domain mAP: 0.8635, Class mAP: 0.8695, Time: 49.98s\n",
      "\n",
      "EarlyStopping ì¹´ìš´í„°: 1 / 20 (Domain ìµœê³ : 0.8653, Class ìµœê³ : 0.8729)\n",
      "KD Epoch [44], Batch [20/87]\n",
      "Total: 1.5155 | Domain KD: 4.5851 | Class KD: 1.5682\n",
      "KD Epoch [44], Batch [40/87]\n",
      "Total: 1.4249 | Domain KD: 4.1823 | Class KD: 1.4878\n",
      "KD Epoch [44], Batch [60/87]\n",
      "Total: 1.6473 | Domain KD: 4.9083 | Class KD: 1.7293\n",
      "KD Epoch [44], Batch [80/87]\n",
      "Total: 1.5596 | Domain KD: 4.9039 | Class KD: 1.5653\n",
      "KD Train Epoch 44:\n",
      "Total Loss: 1.5960 | Domain Acc: 88.29% | Class Acc: 98.69%\n",
      "Domain mAP: 0.9128 | Class mAP: 0.9986\n",
      "KD Losses - Domain: 4.9342 | Class: 1.6146\n",
      "Time: 296.23s\n",
      "\n",
      "Test set: Epoch: 44, Avg loss: 1.0159, Domain Loss: 1.0403, Class Loss: 1.0098, Domain Acc: 80.80%, Class Acc: 81.23%, Domain mAP: 0.8637, Class mAP: 0.8699, Time: 50.61s\n",
      "\n",
      "EarlyStopping ì¹´ìš´í„°: 2 / 20 (Domain ìµœê³ : 0.8653, Class ìµœê³ : 0.8729)\n",
      "KD Epoch [45], Batch [20/87]\n",
      "Total: 1.4438 | Domain KD: 5.0718 | Class KD: 1.2644\n",
      "KD Epoch [45], Batch [40/87]\n",
      "Total: 1.4741 | Domain KD: 4.2274 | Class KD: 1.6107\n",
      "KD Epoch [45], Batch [60/87]\n",
      "Total: 1.6387 | Domain KD: 5.2146 | Class KD: 1.5901\n",
      "KD Epoch [45], Batch [80/87]\n",
      "Total: 1.6135 | Domain KD: 5.6505 | Class KD: 1.4240\n",
      "KD Train Epoch 45:\n",
      "Total Loss: 1.5910 | Domain Acc: 88.59% | Class Acc: 98.46%\n",
      "Domain mAP: 0.9127 | Class mAP: 0.9984\n",
      "KD Losses - Domain: 4.8664 | Class: 1.6269\n",
      "Time: 297.08s\n",
      "\n",
      "Test set: Epoch: 45, Avg loss: 1.0225, Domain Loss: 1.0346, Class Loss: 1.0195, Domain Acc: 81.17%, Class Acc: 81.33%, Domain mAP: 0.8639, Class mAP: 0.8710, Time: 50.02s\n",
      "\n",
      "EarlyStopping ì¹´ìš´í„°: 3 / 20 (Domain ìµœê³ : 0.8653, Class ìµœê³ : 0.8729)\n",
      "KD Epoch [46], Batch [20/87]\n",
      "Total: 1.5947 | Domain KD: 5.2111 | Class KD: 1.5208\n",
      "KD Epoch [46], Batch [40/87]\n",
      "Total: 1.5921 | Domain KD: 4.3812 | Class KD: 1.7946\n",
      "KD Epoch [46], Batch [60/87]\n",
      "Total: 1.6011 | Domain KD: 4.8247 | Class KD: 1.6717\n",
      "KD Epoch [46], Batch [80/87]\n",
      "Total: 1.6892 | Domain KD: 5.4522 | Class KD: 1.6272\n",
      "KD Train Epoch 46:\n",
      "Total Loss: 1.5708 | Domain Acc: 88.77% | Class Acc: 98.70%\n",
      "Domain mAP: 0.9201 | Class mAP: 0.9988\n",
      "KD Losses - Domain: 4.7605 | Class: 1.6245\n",
      "Time: 297.75s\n",
      "\n",
      "Test set: Epoch: 46, Avg loss: 1.0220, Domain Loss: 1.0721, Class Loss: 1.0095, Domain Acc: 80.45%, Class Acc: 81.17%, Domain mAP: 0.8636, Class mAP: 0.8712, Time: 49.96s\n",
      "\n",
      "EarlyStopping ì¹´ìš´í„°: 4 / 20 (Domain ìµœê³ : 0.8653, Class ìµœê³ : 0.8729)\n",
      "KD Epoch [47], Batch [20/87]\n",
      "Total: 1.7564 | Domain KD: 5.9212 | Class KD: 1.6256\n",
      "KD Epoch [47], Batch [40/87]\n",
      "Total: 1.4597 | Domain KD: 4.3854 | Class KD: 1.5256\n",
      "KD Epoch [47], Batch [60/87]\n",
      "Total: 1.7603 | Domain KD: 5.8719 | Class KD: 1.6345\n",
      "KD Epoch [47], Batch [80/87]\n",
      "Total: 1.6705 | Domain KD: 4.4046 | Class KD: 1.8972\n",
      "KD Train Epoch 47:\n",
      "Total Loss: 1.5712 | Domain Acc: 88.45% | Class Acc: 98.76%\n",
      "Domain mAP: 0.9149 | Class mAP: 0.9986\n",
      "KD Losses - Domain: 4.8658 | Class: 1.5892\n",
      "Time: 297.83s\n",
      "\n",
      "Test set: Epoch: 47, Avg loss: 1.0158, Domain Loss: 1.0450, Class Loss: 1.0085, Domain Acc: 81.17%, Class Acc: 81.48%, Domain mAP: 0.8646, Class mAP: 0.8716, Time: 51.38s\n",
      "\n",
      "EarlyStopping ì¹´ìš´í„°: 5 / 20 (Domain ìµœê³ : 0.8653, Class ìµœê³ : 0.8729)\n",
      "KD Epoch [48], Batch [20/87]\n",
      "Total: 1.6074 | Domain KD: 5.0077 | Class KD: 1.6445\n",
      "KD Epoch [48], Batch [40/87]\n",
      "Total: 1.6075 | Domain KD: 5.0707 | Class KD: 1.6106\n",
      "KD Epoch [48], Batch [60/87]\n",
      "Total: 1.4842 | Domain KD: 4.8647 | Class KD: 1.4305\n",
      "KD Epoch [48], Batch [80/87]\n",
      "Total: 1.6192 | Domain KD: 5.2892 | Class KD: 1.5313\n",
      "KD Train Epoch 48:\n",
      "Total Loss: 1.5806 | Domain Acc: 88.30% | Class Acc: 98.55%\n",
      "Domain mAP: 0.9127 | Class mAP: 0.9986\n",
      "KD Losses - Domain: 4.8910 | Class: 1.5974\n",
      "Time: 309.87s\n",
      "\n",
      "Test set: Epoch: 48, Avg loss: 1.0312, Domain Loss: 1.0896, Class Loss: 1.0166, Domain Acc: 80.30%, Class Acc: 81.29%, Domain mAP: 0.8638, Class mAP: 0.8702, Time: 51.17s\n",
      "\n",
      "EarlyStopping ì¹´ìš´í„°: 6 / 20 (Domain ìµœê³ : 0.8653, Class ìµœê³ : 0.8729)\n",
      "KD Epoch [49], Batch [20/87]\n",
      "Total: 1.3578 | Domain KD: 4.1698 | Class KD: 1.3923\n",
      "KD Epoch [49], Batch [40/87]\n",
      "Total: 1.4437 | Domain KD: 4.6821 | Class KD: 1.4039\n",
      "KD Epoch [49], Batch [60/87]\n",
      "Total: 1.3717 | Domain KD: 4.1284 | Class KD: 1.4199\n",
      "KD Epoch [49], Batch [80/87]\n",
      "Total: 1.4695 | Domain KD: 4.5959 | Class KD: 1.4909\n",
      "KD Train Epoch 49:\n",
      "Total Loss: 1.5393 | Domain Acc: 88.67% | Class Acc: 98.88%\n",
      "Domain mAP: 0.9166 | Class mAP: 0.9990\n",
      "KD Losses - Domain: 4.8170 | Class: 1.5434\n",
      "Time: 306.85s\n",
      "\n",
      "Test set: Epoch: 49, Avg loss: 1.0150, Domain Loss: 1.0369, Class Loss: 1.0095, Domain Acc: 80.64%, Class Acc: 81.17%, Domain mAP: 0.8643, Class mAP: 0.8701, Time: 50.49s\n",
      "\n",
      "EarlyStopping ì¹´ìš´í„°: 7 / 20 (Domain ìµœê³ : 0.8653, Class ìµœê³ : 0.8729)\n",
      "KD Epoch [50], Batch [20/87]\n",
      "Total: 1.6673 | Domain KD: 5.7807 | Class KD: 1.4892\n",
      "KD Epoch [50], Batch [40/87]\n",
      "Total: 1.5260 | Domain KD: 4.6860 | Class KD: 1.5423\n",
      "KD Epoch [50], Batch [60/87]\n",
      "Total: 1.7041 | Domain KD: 6.2496 | Class KD: 1.3964\n",
      "KD Epoch [50], Batch [80/87]\n",
      "Total: 1.4881 | Domain KD: 4.6182 | Class KD: 1.4968\n",
      "KD Train Epoch 50:\n",
      "Total Loss: 1.5487 | Domain Acc: 88.58% | Class Acc: 98.86%\n",
      "Domain mAP: 0.9140 | Class mAP: 0.9988\n",
      "KD Losses - Domain: 4.8345 | Class: 1.5539\n",
      "Time: 303.46s\n",
      "\n",
      "Test set: Epoch: 50, Avg loss: 1.0086, Domain Loss: 1.0387, Class Loss: 1.0010, Domain Acc: 81.01%, Class Acc: 81.33%, Domain mAP: 0.8644, Class mAP: 0.8707, Time: 50.52s\n",
      "\n",
      "EarlyStopping ì¹´ìš´í„°: 8 / 20 (Domain ìµœê³ : 0.8653, Class ìµœê³ : 0.8729)\n",
      "KD Epoch [51], Batch [20/87]\n",
      "Total: 1.5586 | Domain KD: 4.8385 | Class KD: 1.5768\n",
      "KD Epoch [51], Batch [40/87]\n",
      "Total: 1.6363 | Domain KD: 5.2961 | Class KD: 1.5727\n",
      "KD Epoch [51], Batch [60/87]\n",
      "Total: 1.6770 | Domain KD: 6.0096 | Class KD: 1.4110\n",
      "KD Epoch [51], Batch [80/87]\n",
      "Total: 1.5891 | Domain KD: 5.4554 | Class KD: 1.4245\n",
      "KD Train Epoch 51:\n",
      "Total Loss: 1.5656 | Domain Acc: 88.49% | Class Acc: 98.71%\n",
      "Domain mAP: 0.9136 | Class mAP: 0.9988\n",
      "KD Losses - Domain: 4.8800 | Class: 1.5731\n",
      "Time: 301.81s\n",
      "\n",
      "Test set: Epoch: 51, Avg loss: 0.9999, Domain Loss: 1.0091, Class Loss: 0.9976, Domain Acc: 81.17%, Class Acc: 81.05%, Domain mAP: 0.8650, Class mAP: 0.8715, Time: 50.31s\n",
      "\n",
      "EarlyStopping ì¹´ìš´í„°: 9 / 20 (Domain ìµœê³ : 0.8653, Class ìµœê³ : 0.8729)\n",
      "KD Epoch [52], Batch [20/87]\n",
      "Total: 1.5180 | Domain KD: 4.4714 | Class KD: 1.6139\n",
      "KD Epoch [52], Batch [40/87]\n",
      "Total: 1.5967 | Domain KD: 4.8027 | Class KD: 1.6402\n",
      "KD Epoch [52], Batch [60/87]\n",
      "Total: 1.6441 | Domain KD: 5.3697 | Class KD: 1.5661\n",
      "KD Epoch [52], Batch [80/87]\n",
      "Total: 1.4792 | Domain KD: 4.6522 | Class KD: 1.4892\n",
      "KD Train Epoch 52:\n",
      "Total Loss: 1.5476 | Domain Acc: 88.96% | Class Acc: 98.76%\n",
      "Domain mAP: 0.9168 | Class mAP: 0.9988\n",
      "KD Losses - Domain: 4.8097 | Class: 1.5599\n",
      "Time: 302.32s\n",
      "\n",
      "Test set: Epoch: 52, Avg loss: 0.9994, Domain Loss: 1.0037, Class Loss: 0.9983, Domain Acc: 81.08%, Class Acc: 81.48%, Domain mAP: 0.8651, Class mAP: 0.8716, Time: 51.18s\n",
      "\n",
      "EarlyStopping ì¹´ìš´í„°: 10 / 20 (Domain ìµœê³ : 0.8653, Class ìµœê³ : 0.8729)\n",
      "KD Epoch [53], Batch [20/87]\n",
      "Total: 1.5031 | Domain KD: 4.5849 | Class KD: 1.5254\n",
      "KD Epoch [53], Batch [40/87]\n",
      "Total: 1.4216 | Domain KD: 4.1952 | Class KD: 1.5076\n",
      "KD Epoch [53], Batch [60/87]\n",
      "Total: 1.4774 | Domain KD: 4.3165 | Class KD: 1.5909\n",
      "KD Epoch [53], Batch [80/87]\n",
      "Total: 1.5224 | Domain KD: 4.3056 | Class KD: 1.6857\n",
      "KD Train Epoch 53:\n",
      "Total Loss: 1.5525 | Domain Acc: 88.43% | Class Acc: 98.71%\n",
      "Domain mAP: 0.9148 | Class mAP: 0.9989\n",
      "KD Losses - Domain: 4.8259 | Class: 1.5650\n",
      "Time: 302.08s\n",
      "\n",
      "Test set: Epoch: 53, Avg loss: 1.0008, Domain Loss: 1.0118, Class Loss: 0.9981, Domain Acc: 81.17%, Class Acc: 81.33%, Domain mAP: 0.8646, Class mAP: 0.8712, Time: 51.08s\n",
      "\n",
      "EarlyStopping ì¹´ìš´í„°: 11 / 20 (Domain ìµœê³ : 0.8653, Class ìµœê³ : 0.8729)\n",
      "KD Epoch [54], Batch [20/87]\n",
      "Total: 1.5660 | Domain KD: 4.7826 | Class KD: 1.6178\n",
      "KD Epoch [54], Batch [40/87]\n",
      "Total: 1.6701 | Domain KD: 5.0931 | Class KD: 1.7275\n",
      "KD Epoch [54], Batch [60/87]\n",
      "Total: 1.4535 | Domain KD: 4.4498 | Class KD: 1.5002\n",
      "KD Epoch [54], Batch [80/87]\n",
      "Total: 1.5587 | Domain KD: 4.4198 | Class KD: 1.6722\n",
      "KD Train Epoch 54:\n",
      "Total Loss: 1.5531 | Domain Acc: 88.72% | Class Acc: 98.79%\n",
      "Domain mAP: 0.9167 | Class mAP: 0.9989\n",
      "KD Losses - Domain: 4.8695 | Class: 1.5518\n",
      "Time: 302.83s\n",
      "\n",
      "Test set: Epoch: 54, Avg loss: 1.0062, Domain Loss: 1.0324, Class Loss: 0.9997, Domain Acc: 80.80%, Class Acc: 81.42%, Domain mAP: 0.8636, Class mAP: 0.8718, Time: 50.75s\n",
      "\n",
      "EarlyStopping ì¹´ìš´í„°: 12 / 20 (Domain ìµœê³ : 0.8653, Class ìµœê³ : 0.8729)\n",
      "KD Epoch [55], Batch [20/87]\n",
      "Total: 1.4840 | Domain KD: 5.3826 | Class KD: 1.2296\n",
      "KD Epoch [55], Batch [40/87]\n",
      "Total: 1.5715 | Domain KD: 4.6480 | Class KD: 1.6525\n",
      "KD Epoch [55], Batch [60/87]\n",
      "Total: 1.5964 | Domain KD: 4.8842 | Class KD: 1.6433\n",
      "KD Epoch [55], Batch [80/87]\n",
      "Total: 1.5915 | Domain KD: 5.6200 | Class KD: 1.3675\n",
      "KD Train Epoch 55:\n",
      "Total Loss: 1.5476 | Domain Acc: 88.86% | Class Acc: 98.79%\n",
      "Domain mAP: 0.9177 | Class mAP: 0.9988\n",
      "KD Losses - Domain: 4.8293 | Class: 1.5533\n",
      "Time: 301.98s\n",
      "\n",
      "Test set: Epoch: 55, Avg loss: 0.9969, Domain Loss: 1.0248, Class Loss: 0.9900, Domain Acc: 81.01%, Class Acc: 81.20%, Domain mAP: 0.8647, Class mAP: 0.8717, Time: 50.58s\n",
      "\n",
      "EarlyStopping ì¹´ìš´í„°: 13 / 20 (Domain ìµœê³ : 0.8653, Class ìµœê³ : 0.8729)\n",
      "KD Epoch [56], Batch [20/87]\n",
      "Total: 1.4853 | Domain KD: 4.6564 | Class KD: 1.4892\n",
      "KD Epoch [56], Batch [40/87]\n",
      "Total: 1.4453 | Domain KD: 4.6698 | Class KD: 1.3699\n",
      "KD Epoch [56], Batch [60/87]\n",
      "Total: 1.4352 | Domain KD: 4.3002 | Class KD: 1.4660\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchmetrics\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "from utility.utils import DualMAPEarlyStopping  # DualMAPEarlyStoppingìœ¼ë¡œ ë³€ê²½\n",
    "from torchmetrics.classification import MulticlassAveragePrecision\n",
    "from Dataset.data import OfficeHomeDataset  # ê¸°ì¡´ ë°ì´í„°ì…‹ í´ëž˜ìŠ¤ ìž„í¬íŠ¸\n",
    "from models.resnet_dilated import ResnetDilated  # ResnetDilated ìž„í¬íŠ¸\n",
    "import timm\n",
    "from models.efficientnet_teacher import EfficientNetTeacher, MultiTaskKDLoss, pretrain_teacher, train_with_kd\n",
    "\n",
    "\n",
    "# ì²´í¬í¬ì¸íŠ¸ ë””ë ‰í† ë¦¬ ìƒì„±\n",
    "os.makedirs('checkpoints', exist_ok=True)\n",
    "\n",
    "# WandB ì„¤ì • \n",
    "wandb.login(key=\"ef091b9abcea3186341ddf8995d62bde62d7469e\")\n",
    "wandb.init(\n",
    "    project=\"office-home-classification\", \n",
    "    name=\"Dilated_ResNet50_MultiTask_layer3 - Style, all_Category_KD\",\n",
    "    entity=\"hh0804352-hanyang-university\"\n",
    ")\n",
    "\n",
    "# wandb run nameì„ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œì— ì‚¬ìš©\n",
    "run_name = wandb.run.name\n",
    "CHECKPOINT_PATH = os.path.join('checkpoints', f'{run_name}_checkpoint.pth')\n",
    "\n",
    "\n",
    "# ì„¤ì •\n",
    "config = {\n",
    "    # ê¸°ì¡´ ì„¤ì • ìœ ì§€\n",
    "    \"model\": \"resnet50\",\n",
    "    \"batch_size\": 128,\n",
    "    \"num_epochs\": 300,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"optimizer\": \"Adam\",\n",
    "    \"seed\": 2025,\n",
    "    \"deterministic\": False,\n",
    "    \"patience\": 20,  # ê¸°ì¡´ 30 â†’ 20\n",
    "    \"max_epochs_wait\": float('inf'),\n",
    "    \"num_domains\": 4,\n",
    "    \"num_classes\": 65,\n",
    "    \n",
    "    #  KD ìµœì í™” ì„¤ì •\n",
    "    \"domain_weight\": 0.2,  # 0.5 â†’ 0.2 (KDê°€ ì˜¤ë²„í”¼íŒ… í•´ê²°)\n",
    "    \"class_weight\": 0.8,   # 0.5 â†’ 0.8 (í´ëž˜ìŠ¤ì— ë” ì§‘ì¤‘)\n",
    "    \n",
    "    # Teacher ì„¤ì •\n",
    "    \"teacher_epochs\": 20,   # Teacher ì‚¬ì „ í›ˆë ¨ ì—í­\n",
    "    \n",
    "    # ê¸°ì¡´ ì„¤ì •ë“¤\n",
    "    \"num_workers\": 20,\n",
    "    \"pin_memory\": True,\n",
    "    \"save_every\": 5,\n",
    "    \"scheduler\": \"ReduceLROnPlateau\",\n",
    "    \"scheduler_mode\": \"max\",\n",
    "    \"scheduler_factor\": 0.1,\n",
    "    \"scheduler_patience\": 5,\n",
    "    \"scheduler_verbose\": True,\n",
    "}\n",
    "\n",
    "wandb.config.update(config)\n",
    "\n",
    "class ImprovedMultiTaskModel(nn.Module):\n",
    "    def __init__(self, num_domains=4, num_classes=65):\n",
    "        super(ImprovedMultiTaskModel, self).__init__()\n",
    "        \n",
    "        # ê³µìœ  ë°±ë³¸ - ResnetDilated ì‚¬ìš©\n",
    "        pretrained_resnet = models.resnet50(pretrained=True)\n",
    "        self.backbone = ResnetDilated(pretrained_resnet, dilate_scale=8)\n",
    "        \n",
    "        # ë°±ë³¸ ë™ê²° (ì„ íƒì )\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        # ë„ë©”ì¸ ë¶„ë¥˜ë¥¼ ìœ„í•œ ë¶„ê¸°ì  (layer3 ì¶œë ¥ì—ì„œ)\n",
    "        self.domain_branch = nn.Sequential(\n",
    "            nn.Conv2d(1024, 128, kernel_size=1),  # ì±„ë„ ìˆ˜ ê°ì†Œ\n",
    "            nn.GroupNorm(8, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        \n",
    "        # í´ëž˜ìŠ¤ ë¶„ë¥˜ë¥¼ ìœ„í•œ ë¶„ê¸°ì  (layer4 ì¶œë ¥ì—ì„œ)\n",
    "        self.class_branch = nn.Sequential(\n",
    "            nn.Conv2d(2048, 512, kernel_size=1),  # ì±„ë„ ìˆ˜ ê°ì†Œ\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # í’€ë§ ë ˆì´ì–´\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        # ë¶„ë¥˜ í—¤ë“œ\n",
    "        self.domain_classifier = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.7),\n",
    "            nn.Linear(64, num_domains)\n",
    "        )\n",
    "        \n",
    "        self.class_classifier = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # ë°±ë³¸ì˜ ê° ìŠ¤í…Œì´ì§€ ì¶œë ¥ ì¶”ì¶œ\n",
    "        x = self.backbone.conv1(x)\n",
    "        x = self.backbone.bn1(x)\n",
    "        x = self.backbone.relu1(x)  # relu1ìœ¼ë¡œ ìˆ˜ì • (ResnetDilatedì—ì„œ ì •ì˜ëœ ì´ë¦„ê³¼ ì¼ì¹˜)\n",
    "        x = self.backbone.maxpool(x)\n",
    "        \n",
    "        x = self.backbone.layer1(x)\n",
    "        x = self.backbone.layer2(x)\n",
    "        \n",
    "        # Layer3 ì¶œë ¥ ì €ìž¥ (ë„ë©”ì¸ ë¶„ë¥˜ìš©)\n",
    "        layer3_output = self.backbone.layer3(x)\n",
    "        \n",
    "        # Layer4 ì¶œë ¥ (í´ëž˜ìŠ¤ ë¶„ë¥˜ìš©)\n",
    "        layer4_output = self.backbone.layer4(layer3_output)\n",
    "        \n",
    "        # ë„ë©”ì¸ ë¶„ë¥˜ ê²½ë¡œ\n",
    "        domain_features = self.domain_branch(layer3_output)\n",
    "        domain_features = self.avgpool(domain_features)\n",
    "        domain_features = torch.flatten(domain_features, 1)\n",
    "        domain_out = self.domain_classifier(domain_features)\n",
    "        \n",
    "        # í´ëž˜ìŠ¤ ë¶„ë¥˜ ê²½ë¡œ\n",
    "        class_features = self.class_branch(layer4_output)\n",
    "        class_features = self.avgpool(class_features)\n",
    "        class_features = torch.flatten(class_features, 1)\n",
    "        class_out = self.class_classifier(class_features)\n",
    "        \n",
    "        return domain_out, class_out\n",
    "\n",
    "\n",
    "# ë°ì´í„° ë³€í™˜ (RandomResizedCrop ì œê±°)\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((224,224)),  # \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# ë°ì´í„°ì…‹ ë¡œë“œ\n",
    "trainset = OfficeHomeDataset(root_dir='./Dataset/train', transform=transform_train)\n",
    "testset = OfficeHomeDataset(root_dir='./Dataset/test', transform=transform_test)\n",
    "\n",
    "# DataLoader ìƒì„±\n",
    "trainloader = DataLoader(\n",
    "    trainset, \n",
    "    batch_size=config[\"batch_size\"], \n",
    "    shuffle=True, \n",
    "    pin_memory=config[\"pin_memory\"], \n",
    "    num_workers=config[\"num_workers\"]\n",
    ")\n",
    "\n",
    "testloader = DataLoader(\n",
    "    testset, \n",
    "    batch_size=config[\"batch_size\"], \n",
    "    shuffle=False, \n",
    "    pin_memory=config[\"pin_memory\"], \n",
    "    num_workers=config[\"num_workers\"]\n",
    ")\n",
    "\n",
    "print(f\"Train set size: {len(trainset)}\")\n",
    "print(f\"Test set size: {len(testset)}\")\n",
    "\n",
    "def train(model, trainloader, domain_criterion, class_criterion, optimizer, device, epoch):\n",
    "    \"\"\"\n",
    "    ë©€í‹°íƒœìŠ¤í¬ í•™ìŠµ í•¨ìˆ˜ (ë„ë©”ì¸ + í´ëž˜ìŠ¤ ë¶„ë¥˜)\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    running_loss = 0.0\n",
    "    running_domain_loss = 0.0  # ë„ë©”ì¸ ì†ì‹¤ ë³„ë„ ì¶”ì \n",
    "    running_class_loss = 0.0   # í´ëž˜ìŠ¤ ì†ì‹¤ ë³„ë„ ì¶”ì \n",
    "    domain_correct = 0\n",
    "    class_correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # mAP ê³„ì‚°ê¸° ì´ˆê¸°í™”\n",
    "    domain_map = MulticlassAveragePrecision(num_classes=config[\"num_domains\"], average='macro')\n",
    "    class_map = MulticlassAveragePrecision(num_classes=config[\"num_classes\"], average='macro')\n",
    "    \n",
    "    domain_map = domain_map.to(device)\n",
    "    class_map = class_map.to(device)\n",
    "    \n",
    "    for i, (inputs, domain_labels, class_labels) in enumerate(trainloader):\n",
    "        inputs = inputs.to(device)\n",
    "        domain_labels = domain_labels.to(device)\n",
    "        class_labels = class_labels.to(device)\n",
    "        \n",
    "        # ê·¸ëž˜ë””ì–¸íŠ¸ ì´ˆê¸°í™”\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # ëª¨ë¸ ì „ë°© ì „íŒŒ\n",
    "        domain_outputs, class_outputs = model(inputs)\n",
    "        \n",
    "        # ì†ì‹¤ ê³„ì‚°\n",
    "        domain_loss = domain_criterion(domain_outputs, domain_labels)\n",
    "        class_loss = class_criterion(class_outputs, class_labels)\n",
    "        loss = config[\"domain_weight\"] * domain_loss + config[\"class_weight\"] * class_loss\n",
    "        \n",
    "        # ì—­ì „íŒŒ ë° ìµœì í™”\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        running_domain_loss += domain_loss.item()  # ë„ë©”ì¸ ì†ì‹¤ ëˆ„ì \n",
    "        running_class_loss += class_loss.item()    # í´ëž˜ìŠ¤ ì†ì‹¤ ëˆ„ì \n",
    "        \n",
    "        # ì •í™•ë„ ê³„ì‚°\n",
    "        _, domain_preds = domain_outputs.max(1)\n",
    "        domain_correct += domain_preds.eq(domain_labels).sum().item()\n",
    "        \n",
    "        _, class_preds = class_outputs.max(1)\n",
    "        class_correct += class_preds.eq(class_labels).sum().item()\n",
    "        \n",
    "        total += inputs.size(0)\n",
    "        \n",
    "        # mAP ì—…ë°ì´íŠ¸\n",
    "        domain_map.update(domain_outputs, domain_labels)\n",
    "        class_map.update(class_outputs, class_labels)\n",
    "        \n",
    "        if (i + 1) % 20 == 0:\n",
    "            print(f'Epoch [{epoch+1}], Batch [{i+1}/{len(trainloader)}], Loss: {loss.item():.4f}, '\n",
    "                  f'Domain Loss: {domain_loss.item():.4f}, Class Loss: {class_loss.item():.4f}, '\n",
    "                  f'LR: {optimizer.param_groups[0][\"lr\"]:.6f}')\n",
    "    \n",
    "    # ì—í­ í†µê³„\n",
    "    epoch_loss = running_loss / len(trainloader)\n",
    "    epoch_domain_loss = running_domain_loss / len(trainloader)  # í‰ê·  ë„ë©”ì¸ ì†ì‹¤\n",
    "    epoch_class_loss = running_class_loss / len(trainloader)    # í‰ê·  í´ëž˜ìŠ¤ ì†ì‹¤\n",
    "    domain_accuracy = 100.0 * domain_correct / total\n",
    "    class_accuracy = 100.0 * class_correct / total\n",
    "    \n",
    "    # mAP ê³„ì‚°\n",
    "    domain_map_value = domain_map.compute().item()\n",
    "    class_map_value = class_map.compute().item()\n",
    "    \n",
    "    train_time = time.time() - start_time\n",
    "    \n",
    "    # í•™ìŠµ ì„¸íŠ¸ì— ëŒ€í•œ ì„±ëŠ¥ ì¶œë ¥\n",
    "    print(f'Train set: Epoch: {epoch+1}, Avg loss: {epoch_loss:.4f}, '\n",
    "          f'Domain Loss: {epoch_domain_loss:.4f}, Class Loss: {epoch_class_loss:.4f}, '\n",
    "          f'Domain Acc: {domain_accuracy:.2f}%, Class Acc: {class_accuracy:.2f}%, '\n",
    "          f'Domain mAP: {domain_map_value:.4f}, Class mAP: {class_map_value:.4f}, '\n",
    "          f'Time: {train_time:.2f}s')\n",
    "    \n",
    "    return epoch_loss, epoch_domain_loss, epoch_class_loss, domain_accuracy, class_accuracy, domain_map_value, class_map_value\n",
    "\n",
    "def evaluate(model, dataloader, domain_criterion, class_criterion, device, epoch):\n",
    "    \"\"\"\n",
    "    ë©€í‹°íƒœìŠ¤í¬ í‰ê°€ í•¨ìˆ˜\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    start_time = time.time()\n",
    "    running_loss = 0.0\n",
    "    running_domain_loss = 0.0  # ë„ë©”ì¸ ì†ì‹¤ ë³„ë„ ì¶”ì \n",
    "    running_class_loss = 0.0   # í´ëž˜ìŠ¤ ì†ì‹¤ ë³„ë„ ì¶”ì \n",
    "    domain_correct = 0\n",
    "    class_correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # mAP ê³„ì‚°ê¸° ì´ˆê¸°í™”\n",
    "    domain_map = MulticlassAveragePrecision(num_classes=config[\"num_domains\"], average='macro')\n",
    "    class_map = MulticlassAveragePrecision(num_classes=config[\"num_classes\"], average='macro')\n",
    "    \n",
    "    domain_map = domain_map.to(device)\n",
    "    class_map = class_map.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, domain_labels, class_labels in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            domain_labels = domain_labels.to(device)\n",
    "            class_labels = class_labels.to(device)\n",
    "            \n",
    "            # ìˆœì „íŒŒ\n",
    "            domain_outputs, class_outputs = model(inputs)\n",
    "            \n",
    "            # ì†ì‹¤ ê³„ì‚°\n",
    "            domain_loss = domain_criterion(domain_outputs, domain_labels)\n",
    "            class_loss = class_criterion(class_outputs, class_labels)\n",
    "            loss = config[\"domain_weight\"] * domain_loss + config[\"class_weight\"] * class_loss\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            running_domain_loss += domain_loss.item()  # ë„ë©”ì¸ ì†ì‹¤ ëˆ„ì \n",
    "            running_class_loss += class_loss.item()    # í´ëž˜ìŠ¤ ì†ì‹¤ ëˆ„ì \n",
    "            \n",
    "            # ì •í™•ë„ ê³„ì‚°\n",
    "            _, domain_preds = domain_outputs.max(1)\n",
    "            domain_correct += domain_preds.eq(domain_labels).sum().item()\n",
    "            \n",
    "            _, class_preds = class_outputs.max(1)\n",
    "            class_correct += class_preds.eq(class_labels).sum().item()\n",
    "            \n",
    "            total += inputs.size(0)\n",
    "            \n",
    "            # mAP ì—…ë°ì´íŠ¸\n",
    "            domain_map.update(domain_outputs, domain_labels)\n",
    "            class_map.update(class_outputs, class_labels)\n",
    "    \n",
    "    # í‰ê·  ì†ì‹¤ ë° ì •í™•ë„ ê³„ì‚°\n",
    "    eval_loss = running_loss / len(dataloader)\n",
    "    eval_domain_loss = running_domain_loss / len(dataloader)  # í‰ê·  ë„ë©”ì¸ ì†ì‹¤\n",
    "    eval_class_loss = running_class_loss / len(dataloader)    # í‰ê·  í´ëž˜ìŠ¤ ì†ì‹¤\n",
    "    domain_accuracy = 100.0 * domain_correct / total\n",
    "    class_accuracy = 100.0 * class_correct / total\n",
    "    \n",
    "    # mAP ê³„ì‚°\n",
    "    domain_map_value = domain_map.compute().item()\n",
    "    class_map_value = class_map.compute().item()\n",
    "    \n",
    "    # í‰ê°€ ì‹œê°„ ê³„ì‚°\n",
    "    eval_time = time.time() - start_time\n",
    "    \n",
    "    # í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì— ëŒ€í•œ ì„±ëŠ¥ ì¶œë ¥\n",
    "    print(f'Test set: Epoch: {epoch+1}, Avg loss: {eval_loss:.4f}, '\n",
    "          f'Domain Loss: {eval_domain_loss:.4f}, Class Loss: {eval_class_loss:.4f}, '\n",
    "          f'Domain Acc: {domain_accuracy:.2f}%, Class Acc: {class_accuracy:.2f}%, '\n",
    "          f'Domain mAP: {domain_map_value:.4f}, Class mAP: {class_map_value:.4f}, '\n",
    "          f'Time: {eval_time:.2f}s')\n",
    "    print()\n",
    "    \n",
    "    return eval_loss, eval_domain_loss, eval_class_loss, domain_accuracy, class_accuracy, domain_map_value, class_map_value\n",
    "\n",
    "# ì²´í¬í¬ì¸íŠ¸ ì €ìž¥ í•¨ìˆ˜ ì¶”ê°€\n",
    "def save_checkpoint(model, optimizer, scheduler, epoch, best_class_map, best_domain_map, early_stopping, filename=CHECKPOINT_PATH):\n",
    "    \"\"\"\n",
    "    í•™ìŠµ ìƒíƒœ ì €ìž¥ í•¨ìˆ˜\n",
    "    \"\"\"\n",
    "    # ëª¨ë¸ì´ DataParallelë¡œ ê°ì‹¸ì ¸ ìžˆëŠ” ê²½ìš° ì²˜ë¦¬\n",
    "    model_state_dict = model.module.state_dict() if isinstance(model, nn.DataParallel) else model.state_dict()\n",
    "    \n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model_state_dict,\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),  # ìŠ¤ì¼€ì¤„ëŸ¬ ìƒíƒœ ì €ìž¥ ì¶”ê°€\n",
    "        'best_class_map': best_class_map,\n",
    "        'best_domain_map': best_domain_map,\n",
    "        'early_stopping_counter': early_stopping.counter,\n",
    "        'early_stopping_domain_best_score': early_stopping.domain_best_score,  # DualMAP ë§žê²Œ ìˆ˜ì •\n",
    "        'early_stopping_class_best_score': early_stopping.class_best_score,    # DualMAP ë§žê²Œ ìˆ˜ì •\n",
    "        'early_stopping_best_epoch': early_stopping.best_epoch,\n",
    "        'early_stopping_domain_best_epoch': early_stopping.domain_best_epoch,  # DualMAP ë§žê²Œ ì¶”ê°€\n",
    "        'early_stopping_class_best_epoch': early_stopping.class_best_epoch,    # DualMAP ë§žê²Œ ì¶”ê°€\n",
    "        'early_stopping_early_stop': early_stopping.early_stop,\n",
    "        'early_stopping_domain_map_max': early_stopping.domain_map_max,  # DualMAP ë§žê²Œ ì¶”ê°€\n",
    "        'early_stopping_class_map_max': early_stopping.class_map_max,    # DualMAP ë§žê²Œ ì¶”ê°€\n",
    "        'config': config,  # ì„¤ì •ê°’ë„ ì €ìž¥\n",
    "    }\n",
    "    torch.save(checkpoint, filename)\n",
    "    print(f\"ì²´í¬í¬ì¸íŠ¸ê°€ {filename}ì— ì €ìž¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ í•¨ìˆ˜ ì¶”ê°€\n",
    "def load_checkpoint(model, optimizer, scheduler, early_stopping, filename=CHECKPOINT_PATH):\n",
    "    \"\"\"\n",
    "    í•™ìŠµ ìƒíƒœ ë¡œë“œ í•¨ìˆ˜\n",
    "    \"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        print(f\"ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ {filename}ì´ ì¡´ìž¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì²˜ìŒë¶€í„° í•™ìŠµì„ ì‹œìž‘í•©ë‹ˆë‹¤.\")\n",
    "        return model, optimizer, scheduler, early_stopping, 0, 0.0, 0.0\n",
    "    \n",
    "    print(f\"ì²´í¬í¬ì¸íŠ¸ {filename}ì„ ë¡œë“œí•©ë‹ˆë‹¤.\")\n",
    "    checkpoint = torch.load(filename)\n",
    "    \n",
    "    # ëª¨ë¸ì´ DataParallelë¡œ ê°ì‹¸ì ¸ ìžˆëŠ” ê²½ìš° ì²˜ë¦¬\n",
    "    if isinstance(model, nn.DataParallel):\n",
    "        model.module.load_state_dict(checkpoint['model_state_dict'])\n",
    "    else:\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    \n",
    "    # ìŠ¤ì¼€ì¤„ëŸ¬ ìƒíƒœ ë¡œë“œ ì¶”ê°€\n",
    "    if 'scheduler_state_dict' in checkpoint:\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    \n",
    "    # ì¡°ê¸° ì¤‘ë‹¨ ìƒíƒœ ë³µì› (DualMAPì— ë§žê²Œ ìˆ˜ì •)\n",
    "    early_stopping.counter = checkpoint['early_stopping_counter']\n",
    "    early_stopping.domain_best_score = checkpoint.get('early_stopping_domain_best_score')\n",
    "    early_stopping.class_best_score = checkpoint.get('early_stopping_class_best_score')\n",
    "    early_stopping.best_epoch = checkpoint['early_stopping_best_epoch']\n",
    "    early_stopping.domain_best_epoch = checkpoint.get('early_stopping_domain_best_epoch', early_stopping.best_epoch)\n",
    "    early_stopping.class_best_epoch = checkpoint.get('early_stopping_class_best_epoch', early_stopping.best_epoch)\n",
    "    early_stopping.early_stop = checkpoint['early_stopping_early_stop']\n",
    "    early_stopping.domain_map_max = checkpoint.get('early_stopping_domain_map_max', -np.Inf)\n",
    "    early_stopping.class_map_max = checkpoint.get('early_stopping_class_map_max', -np.Inf)\n",
    "    \n",
    "    # ê¸°íƒ€ í•™ìŠµ ìƒíƒœ\n",
    "    start_epoch = checkpoint['epoch'] + 1  # ë‹¤ìŒ ì—í­ë¶€í„° ì‹œìž‘\n",
    "    best_class_map = checkpoint['best_class_map']\n",
    "    best_domain_map = checkpoint['best_domain_map']\n",
    "    \n",
    "    print(f\"ì²´í¬í¬ì¸íŠ¸ì—ì„œ ë¡œë“œ ì™„ë£Œ: ì—í­ {start_epoch}ë¶€í„° ì‹œìž‘í•©ë‹ˆë‹¤.\")\n",
    "    print(f\"ì´ì „ ìµœê³  ì„±ëŠ¥: Class mAP: {best_class_map:.4f}, Domain mAP: {best_domain_map:.4f}\")\n",
    "    \n",
    "    return model, optimizer, scheduler, early_stopping, start_epoch, best_class_map, best_domain_map\n",
    "\n",
    "# ë©”ì¸ í•™ìŠµ ë£¨í”„\n",
    "def main_training_loop(model, trainloader, testloader, domain_criterion, class_criterion, optimizer, scheduler, device, num_epochs=None, patience=None, max_epochs_wait=None):\n",
    "    \"\"\"\n",
    "    ë©”ì¸ í•™ìŠµ ë£¨í”„ (mAP ê¸°ì¤€ early stopping)\n",
    "    \"\"\"\n",
    "    # configì—ì„œ ê°’ ê°€ì ¸ì˜¤ê¸°\n",
    "    if num_epochs is None:\n",
    "        num_epochs = config[\"num_epochs\"]\n",
    "    if patience is None:\n",
    "        patience = config[\"patience\"]\n",
    "    if max_epochs_wait is None:\n",
    "        max_epochs_wait = config[\"max_epochs_wait\"]\n",
    "        \n",
    "    # DualMAP ì–¼ë¦¬ ìŠ¤í† í•‘ ì´ˆê¸°í™” (AccuracyEarlyStoppingì—ì„œ ë³€ê²½)\n",
    "    early_stopping = DualMAPEarlyStopping(\n",
    "        patience=patience, \n",
    "        verbose=True, \n",
    "        path='checkpoint.pt', \n",
    "        max_epochs=max_epochs_wait\n",
    "    )\n",
    "    \n",
    "    # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì‹œë„\n",
    "    model, optimizer, scheduler, early_stopping, start_epoch, best_class_map, best_domain_map = load_checkpoint(\n",
    "        model, optimizer, scheduler, early_stopping\n",
    "    )\n",
    "\n",
    "    # ì´ë¯¸ ë¡œë“œëœ ê°’ì´ ì—†ìœ¼ë©´ ì´ˆê¸°í™”\n",
    "    if start_epoch == 0:\n",
    "        best_class_map = 0.0\n",
    "        best_domain_map = 0.0\n",
    "    \n",
    "    # tqdmì„ ì‚¬ìš©í•œ ì§„í–‰ ìƒí™© í‘œì‹œ\n",
    "    for epoch in tqdm(range(start_epoch, num_epochs)):\n",
    "        # í•™ìŠµ\n",
    "        train_loss, train_domain_loss, train_class_loss, train_domain_acc, train_class_acc, train_domain_map, train_class_map = train(\n",
    "            model, \n",
    "            trainloader, \n",
    "            domain_criterion, \n",
    "            class_criterion, \n",
    "            optimizer, \n",
    "            device, \n",
    "            epoch\n",
    "        )\n",
    "        \n",
    "        # í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ í‰ê°€\n",
    "        test_loss, test_domain_loss, test_class_loss, test_domain_acc, test_class_acc, test_domain_map, test_class_map = evaluate(\n",
    "            model, \n",
    "            testloader, \n",
    "            domain_criterion, \n",
    "            class_criterion, \n",
    "            device, \n",
    "            epoch\n",
    "        )\n",
    "        \n",
    "        # í•™ìŠµë¥  ì¡°ì • - ê²€ì¦ ì„±ëŠ¥ì— ë”°ë¼ ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸\n",
    "        avg_map = (test_domain_map + test_class_map) / 2\n",
    "        scheduler.step(avg_map)  # ìŠ¤ì¼€ì¤„ëŸ¬ í˜¸ì¶œ ì¶”ê°€\n",
    "        \n",
    "        # WandBì— ë¡œê¹… (ë„ë©”ì¸ ì†ì‹¤ê³¼ í´ëž˜ìŠ¤ ì†ì‹¤ ë³„ë„ ë¡œê¹…)\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"learning_rate\": optimizer.param_groups[0]['lr'],\n",
    "            \"train_loss\": train_loss,\n",
    "            \"train_domain_loss\": train_domain_loss,\n",
    "            \"train_class_loss\": train_class_loss,\n",
    "            \"train_domain_accuracy\": train_domain_acc,\n",
    "            \"train_class_accuracy\": train_class_acc,\n",
    "            \"train_domain_map\": train_domain_map,\n",
    "            \"train_class_map\": train_class_map,\n",
    "            \"test_loss\": test_loss,\n",
    "            \"test_domain_loss\": test_domain_loss,\n",
    "            \"test_class_loss\": test_class_loss,\n",
    "            \"test_domain_accuracy\": test_domain_acc,\n",
    "            \"test_class_accuracy\": test_class_acc,\n",
    "            \"test_domain_map\": test_domain_map,\n",
    "            \"test_class_map\": test_class_map\n",
    "        })\n",
    "            \n",
    "        # ìµœê³  í´ëž˜ìŠ¤ mAP ëª¨ë¸ ì €ìž¥\n",
    "        if test_class_map > best_class_map:\n",
    "            best_class_map = test_class_map\n",
    "            best_domain_map_at_best_class = test_domain_map\n",
    "            print(f'ìƒˆë¡œìš´ ìµœê³  Class mAP: {best_class_map:.4f}, Domain mAP: {best_domain_map_at_best_class:.4f}')\n",
    "            # ëª¨ë¸ ì €ìž¥\n",
    "            model_path = f'best_model_class_{wandb.run.name}.pth'\n",
    "            # ëª¨ë¸ì´ DataParallelë¡œ ê°ì‹¸ì ¸ ìžˆëŠ” ê²½ìš° ì²˜ë¦¬\n",
    "            model_state_dict = model.module.state_dict() if isinstance(model, nn.DataParallel) else model.state_dict()\n",
    "            torch.save(model_state_dict, model_path)\n",
    "            wandb.save(model_path)\n",
    "        \n",
    "        # ìµœê³  ë„ë©”ì¸ mAP ëª¨ë¸ ì €ìž¥\n",
    "        if test_domain_map > best_domain_map:\n",
    "            best_domain_map = test_domain_map\n",
    "            print(f'ìƒˆë¡œìš´ ìµœê³  Domain mAP: {best_domain_map:.4f}')\n",
    "            # ëª¨ë¸ ì €ìž¥\n",
    "            model_path = f'best_model_domain_{wandb.run.name}.pth'\n",
    "            # ëª¨ë¸ì´ DataParallelë¡œ ê°ì‹¸ì ¸ ìžˆëŠ” ê²½ìš° ì²˜ë¦¬\n",
    "            model_state_dict = model.module.state_dict() if isinstance(model, nn.DataParallel) else model.state_dict()\n",
    "            torch.save(model_state_dict, model_path)\n",
    "            wandb.save(model_path)\n",
    "\n",
    "        # ì£¼ê¸°ì ìœ¼ë¡œ ì²´í¬í¬ì¸íŠ¸ ì €ìž¥ (ì„¤ì •í•œ ê°„ê²©ë§ˆë‹¤)\n",
    "        if (epoch + 1) % config[\"save_every\"] == 0:\n",
    "            save_checkpoint(model, optimizer, scheduler, epoch, best_class_map, best_domain_map, early_stopping)\n",
    "\n",
    "        # DualMAP Early stopping ì²´í¬ (ë‘ mAP ëª¨ë‘ ì‚¬ìš©)\n",
    "        early_stopping(test_domain_map, test_class_map, model, epoch)\n",
    "        \n",
    "        # ë§¤ ì—í­ í›„ì— ì²´í¬í¬ì¸íŠ¸ ì €ìž¥ (ê°€ìž¥ ìµœì‹  ìƒíƒœ)\n",
    "        save_checkpoint(model, optimizer, scheduler, epoch, best_class_map, best_domain_map, early_stopping, \n",
    "                       filename=os.path.join('checkpoints', 'latest_checkpoint.pth'))\n",
    "        \n",
    "        if early_stopping.early_stop:\n",
    "            print(f\"ì—í­ {epoch+1}ì—ì„œ í•™ìŠµ ì¡°ê¸° ì¢…ë£Œ. ìµœê³  ì„±ëŠ¥ ì—í­: {early_stopping.best_epoch+1}\")\n",
    "            break\n",
    "    \n",
    "    # í›ˆë ¨ ì™„ë£Œ í›„ ìµœê³  ëª¨ë¸ ë¡œë“œ\n",
    "    print(\"ìµœê³  í´ëž˜ìŠ¤ mAP ëª¨ë¸ ë¡œë“œ ì¤‘...\")\n",
    "    model_path = f'best_model_class_{wandb.run.name}.pth'\n",
    "    if os.path.exists(model_path):\n",
    "        # ëª¨ë¸ì´ DataParallelë¡œ ê°ì‹¸ì ¸ ìžˆëŠ” ê²½ìš° ì²˜ë¦¬\n",
    "        if isinstance(model, nn.DataParallel):\n",
    "            model.module.load_state_dict(torch.load(model_path))\n",
    "        else:\n",
    "            model.load_state_dict(torch.load(model_path))\n",
    "    else:\n",
    "        print(f\"ê²½ê³ : {model_path} íŒŒì¼ì´ ì¡´ìž¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ìµœì¢… ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "    # ìµœì¢… í…ŒìŠ¤íŠ¸ í‰ê°€\n",
    "    final_test_loss, final_test_domain_loss, final_test_class_loss, final_test_domain_acc, final_test_class_acc, final_test_domain_map, final_test_class_map = evaluate(\n",
    "        model, testloader, domain_criterion, class_criterion, device, num_epochs-1\n",
    "    )\n",
    "    \n",
    "    print(f'ì™„ë£Œ! ìµœê³  Class mAP: {best_class_map:.4f}, ìµœê³  Domain mAP: {best_domain_map:.4f}')\n",
    "    \n",
    "    # WandBì— ìµœì¢… ê²°ê³¼ ê¸°ë¡\n",
    "    wandb.run.summary[\"best_class_map\"] = best_class_map\n",
    "    wandb.run.summary[\"best_domain_map\"] = best_domain_map\n",
    "    wandb.run.summary[\"final_test_class_map\"] = final_test_class_map\n",
    "    wandb.run.summary[\"final_test_domain_map\"] = final_test_domain_map\n",
    "\n",
    "    # Early stopping ì •ë³´ ì €ìž¥\n",
    "    if early_stopping.early_stop:\n",
    "        wandb.run.summary[\"early_stopped\"] = True\n",
    "        wandb.run.summary[\"early_stopped_epoch\"] = epoch+1\n",
    "        wandb.run.summary[\"best_epoch\"] = early_stopping.best_epoch+1\n",
    "        wandb.run.summary[\"domain_best_epoch\"] = early_stopping.domain_best_epoch+1\n",
    "        wandb.run.summary[\"class_best_epoch\"] = early_stopping.class_best_epoch+1\n",
    "    else:\n",
    "        wandb.run.summary[\"early_stopped\"] = False\n",
    "        \n",
    "    # ìµœì¢… ì²´í¬í¬ì¸íŠ¸ ì €ìž¥\n",
    "    save_checkpoint(model, optimizer, scheduler, epoch, best_class_map, best_domain_map, early_stopping, \n",
    "                   filename=os.path.join('checkpoints', 'final_checkpoint.pth'))\n",
    "\n",
    "# ë©”ì¸ ì‹¤í–‰ ì½”ë“œ\n",
    "if __name__ == \"__main__\":\n",
    "    # ì‹œë“œ ì„¤ì •\n",
    "    seed = config[\"seed\"]\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "    # ê²°ì •ì  ì•Œê³ ë¦¬ì¦˜ ì‚¬ìš© ì—¬ë¶€\n",
    "    if config[\"deterministic\"]:\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "    else:\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "    # ë””ë°”ì´ìŠ¤ ì„¤ì •\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # teacher ëª¨ë¸ ìƒì„±\n",
    "    teacher = EfficientNetTeacher(\n",
    "        num_domains=config[\"num_domains\"],\n",
    "        num_classes=config[\"num_classes\"]\n",
    "    ).to(device)\n",
    "\n",
    "    print(f\"Teacher íŒŒë¼ë¯¸í„°: {sum(p.numel() for p in teacher.parameters()):,}\")\n",
    "\n",
    "    # ëª¨ë¸ ì´ˆê¸°í™” - ImprovedMultiTaskModel ì‚¬ìš© (ìˆ˜ì •ë¨)\n",
    "    student = ImprovedMultiTaskModel(\n",
    "        num_domains=config[\"num_domains\"], \n",
    "        num_classes=config[\"num_classes\"]\n",
    "    ).to(device)\n",
    "\n",
    "    print(f\"Student íŒŒë¼ë¯¸í„°: {sum(p.numel() for p in student.parameters()):,}\")\n",
    "\n",
    "    teacher_checkpoint_path = \"efficientnet_teacher_best.pth\"\n",
    "    \n",
    "    if os.path.exists(teacher_checkpoint_path):\n",
    "        print(f\"âœ… ê¸°ì¡´ Teacher ëª¨ë¸ ë°œê²¬: {teacher_checkpoint_path}\")\n",
    "        print(\"ðŸ“ ì €ìž¥ëœ Teacher ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤...\")\n",
    "        teacher.load_state_dict(torch.load(teacher_checkpoint_path, map_location=device))\n",
    "        print(\"ðŸŽ“ Teacher ëª¨ë¸ ë¡œë“œ ì™„ë£Œ! KD í•™ìŠµì„ ì‹œìž‘í•©ë‹ˆë‹¤.\")\n",
    "    else:\n",
    "        print(\"âŒ Teacher ì²´í¬í¬ì¸íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤. Teacherë¥¼ í›ˆë ¨í•©ë‹ˆë‹¤...\")\n",
    "        teacher = pretrain_teacher(teacher, trainloader, testloader, device, epochs=config[\"teacher_epochs\"])\n",
    "    \n",
    "    #  KD Loss \n",
    "    kd_loss_fn = MultiTaskKDLoss(\n",
    "        domain_alpha=0.8,\n",
    "        class_alpha=0.6,\n",
    "        domain_temp=5.0,\n",
    "        class_temp=3.0\n",
    "    )\n",
    "    \n",
    "    # í•™ìŠµë¥  ì°¨ë³„í™” - domain_branch, class_branchë„ í¬í•¨\n",
    "    optimizer = optim.Adam([\n",
    "        {'params': student.domain_branch.parameters(), 'lr': config[\"learning_rate\"]},\n",
    "        {'params': student.class_branch.parameters(), 'lr': config[\"learning_rate\"]},\n",
    "        {'params': student.domain_classifier.parameters(), 'lr': config[\"learning_rate\"]},\n",
    "        {'params': student.class_classifier.parameters(), 'lr': config[\"learning_rate\"]}\n",
    "    ])\n",
    "\n",
    "    # GPU ë³‘ë ¬ ì²˜ë¦¬\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(f\"{torch.cuda.device_count()}ê°œì˜ GPUë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\")\n",
    "        student = nn.DataParallel(student)\n",
    "        teacher = nn.DataParallel(teacher)\n",
    "    \n",
    "    # 2. WandB watch ìˆ˜ì •\n",
    "    wandb.watch(student, log=\"all\")\n",
    "    \n",
    "    # ìŠ¤ì¼€ì¤„ëŸ¬ ì´ˆê¸°í™”\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, \n",
    "        mode=config[\"scheduler_mode\"], \n",
    "        factor=config[\"scheduler_factor\"], \n",
    "        patience=config[\"scheduler_patience\"], \n",
    "        verbose=config[\"scheduler_verbose\"]\n",
    "    )\n",
    "    \n",
    "     #  KD í›ˆë ¨ ë£¨í”„ (main_training_loop ëŒ€ì‹ !)\n",
    "    early_stopping = DualMAPEarlyStopping(\n",
    "        patience=config[\"patience\"], \n",
    "        verbose=True, \n",
    "        path='checkpoint_kd.pt', \n",
    "        max_epochs=config[\"max_epochs_wait\"]\n",
    "    )\n",
    "\n",
    "    best_class_map = 0.0\n",
    "    best_domain_map = 0.0\n",
    "    start_time = time.time()\n",
    "\n",
    "    print(\"Knowledge Distillation í›ˆë ¨ ì‹œìž‘!\")\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        for epoch in range(config[\"num_epochs\"]):\n",
    "            # ðŸ”¥ KD í›ˆë ¨ (train ëŒ€ì‹  train_with_kd ì‚¬ìš©!)\n",
    "            train_results = train_with_kd(\n",
    "                student, teacher, trainloader, kd_loss_fn, optimizer, device, epoch, config\n",
    "            )\n",
    "            \n",
    "            train_loss, train_domain_loss, train_class_loss, train_domain_acc, train_class_acc, train_domain_map, train_class_map, detailed_losses = train_results\n",
    "            \n",
    "            # ðŸ” í‰ê°€ (ê¸°ì¡´ evaluate í•¨ìˆ˜ ì‚¬ìš©)\n",
    "            test_loss, test_domain_loss, test_class_loss, test_domain_acc, test_class_acc, test_domain_map, test_class_map = evaluate(\n",
    "                student, testloader, nn.CrossEntropyLoss(), nn.CrossEntropyLoss(), device, epoch\n",
    "            )\n",
    "            \n",
    "            # ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸\n",
    "            avg_map = (test_domain_map + test_class_map) / 2\n",
    "            scheduler.step(avg_map)\n",
    "            \n",
    "            # WandB ë¡œê¹… (KD ì •ë³´ ì¶”ê°€)\n",
    "            wandb.log({\n",
    "                \"epoch\": epoch + 1,\n",
    "                \"learning_rate\": optimizer.param_groups[0]['lr'],\n",
    "                \"train_loss\": train_loss,\n",
    "                \"train_domain_accuracy\": train_domain_acc,\n",
    "                \"train_class_accuracy\": train_class_acc,\n",
    "                \"train_domain_map\": train_domain_map,\n",
    "                \"train_class_map\": train_class_map,\n",
    "                \"test_loss\": test_loss,\n",
    "                \"test_domain_accuracy\": test_domain_acc,\n",
    "                \"test_class_accuracy\": test_class_acc,\n",
    "                \"test_domain_map\": test_domain_map,\n",
    "                \"test_class_map\": test_class_map,\n",
    "                #  KD ì „ìš© ë©”íŠ¸ë¦­\n",
    "                \"kd_domain_kd_loss\": detailed_losses['domain_kd'],\n",
    "                \"kd_domain_hard_loss\": detailed_losses['domain_hard'],\n",
    "                \"kd_class_kd_loss\": detailed_losses['class_kd'],\n",
    "                \"kd_class_hard_loss\": detailed_losses['class_hard'],\n",
    "            })\n",
    "            \n",
    "            # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ìž¥\n",
    "            if test_class_map > best_class_map:\n",
    "                best_class_map = test_class_map\n",
    "                model_to_save = student.module if isinstance(student, nn.DataParallel) else student\n",
    "                torch.save(model_to_save.state_dict(), f'best_student_class_kd_{wandb.run.name}.pth')\n",
    "                print(f'ðŸ† ìƒˆë¡œìš´ ìµœê³  Class mAP: {best_class_map:.4f}')\n",
    "            \n",
    "            if test_domain_map > best_domain_map:\n",
    "                best_domain_map = test_domain_map\n",
    "                model_to_save = student.module if isinstance(student, nn.DataParallel) else student\n",
    "                torch.save(model_to_save.state_dict(), f'best_student_domain_kd_{wandb.run.name}.pth')\n",
    "                print(f'ðŸ† ìƒˆë¡œìš´ ìµœê³  Domain mAP: {best_domain_map:.4f}')\n",
    "            \n",
    "            # Early stopping\n",
    "            early_stopping(test_domain_map, test_class_map, student, epoch)\n",
    "            if early_stopping.early_stop:\n",
    "                print(f\"ðŸ›‘ KD í›ˆë ¨ ì¡°ê¸° ì¢…ë£Œ - ì—í­ {epoch+1}\")\n",
    "                break\n",
    "        \n",
    "        print(f'ðŸŽ“ KD í›ˆë ¨ ì™„ë£Œ!')\n",
    "        print(f'ìµœê³  Class mAP: {best_class_map:.4f}')\n",
    "        print(f'ìµœê³  Domain mAP: {best_domain_map:.4f}')\n",
    "        \n",
    "        # WandB ìµœì¢… ê²°ê³¼ ê¸°ë¡\n",
    "        wandb.run.summary[\"best_class_map_kd\"] = best_class_map\n",
    "        wandb.run.summary[\"best_domain_map_kd\"] = best_domain_map\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"KD í›ˆë ¨ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "    \n",
    "    # í›ˆë ¨ ì¢…ë£Œ ì‹œê°„\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    wandb.log({\"total_training_time\": total_time})\n",
    "    print(f\"ì „ì²´ í•™ìŠµ ì‹œê°„: {total_time:.2f} ì´ˆ\")\n",
    "    \n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7d0f34-01c7-4f72-9853-140d1528e1e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
