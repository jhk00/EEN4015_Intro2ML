{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c261d0-8022-4151-9f41-246cbca67d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/guswls/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msokjh1310\u001b[0m (\u001b[33msokjh1310-hanyang-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/guswls/EEN4015_Intro2ML/pbl-3/wandb/run-20250519_135059-oj1md4o6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hh0804352-hanyang-university/office-home-classification/runs/oj1md4o6' target=\"_blank\">Dilated_ResNet50_MultiTask_layer3 - Style, all_Category</a></strong> to <a href='https://wandb.ai/hh0804352-hanyang-university/office-home-classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hh0804352-hanyang-university/office-home-classification' target=\"_blank\">https://wandb.ai/hh0804352-hanyang-university/office-home-classification</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hh0804352-hanyang-university/office-home-classification/runs/oj1md4o6' target=\"_blank\">https://wandb.ai/hh0804352-hanyang-university/office-home-classification/runs/oj1md4o6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 11113 이미지, 65 클래스, 4 도메인을 로드했습니다.\n",
      "총 3213 이미지, 65 클래스, 4 도메인을 로드했습니다.\n",
      "Train set size: 11113\n",
      "Test set size: 3213\n",
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 가능한 파라미터: 1,856,325 / 25,364,357 (7.32%)\n",
      "2개의 GPU를 사용합니다.\n",
      "체크포인트 파일 checkpoints/Dilated_ResNet50_MultiTask_layer3 - Style, all_Category_checkpoint.pth이 존재하지 않습니다. 처음부터 학습을 시작합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                       | 0/300 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1], Batch [20/44], Loss: 1.6300, Domain Loss: 0.6882, Class Loss: 2.5718, LR: 0.001000\n",
      "Epoch [1], Batch [40/44], Loss: 1.1044, Domain Loss: 0.5634, Class Loss: 1.6454, LR: 0.001000\n",
      "Train set: Epoch: 1, Avg loss: 1.6961, Domain Loss: 0.7673, Class Loss: 2.6250, Domain Acc: 68.68%, Class Acc: 41.71%, Domain mAP: 0.6984, Class mAP: 0.3613, Time: 148.49s\n",
      "Test set: Epoch: 1, Avg loss: 1.0920, Domain Loss: 0.7076, Class Loss: 1.4763, Domain Acc: 73.20%, Class Acc: 63.96%, Domain mAP: 0.8015, Class mAP: 0.7280, Time: 42.72s\n",
      "\n",
      "새로운 최고 Class mAP: 0.7280, Domain mAP: 0.8015\n",
      "새로운 최고 Domain mAP: 0.8015\n",
      "Performance improved. Saving model ...\n",
      "Domain mAP: -inf --> 0.8015, Class mAP: -inf --> 0.7280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▎                                                                                          | 1/300 [03:12<15:57:50, 192.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "체크포인트가 checkpoints/latest_checkpoint.pth에 저장되었습니다.\n",
      "Epoch [2], Batch [20/44], Loss: 0.9723, Domain Loss: 0.5098, Class Loss: 1.4347, LR: 0.001000\n",
      "Epoch [2], Batch [40/44], Loss: 0.7825, Domain Loss: 0.4669, Class Loss: 1.0982, LR: 0.001000\n",
      "Train set: Epoch: 2, Avg loss: 0.9198, Domain Loss: 0.5195, Class Loss: 1.3201, Domain Acc: 80.01%, Class Acc: 65.85%, Domain mAP: 0.8213, Class mAP: 0.6623, Time: 141.35s\n",
      "Test set: Epoch: 2, Avg loss: 0.8471, Domain Loss: 0.6234, Class Loss: 1.0707, Domain Acc: 75.47%, Class Acc: 72.46%, Domain mAP: 0.8350, Class mAP: 0.7974, Time: 43.95s\n",
      "\n",
      "새로운 최고 Class mAP: 0.7974, Domain mAP: 0.8350\n",
      "새로운 최고 Domain mAP: 0.8350\n",
      "Domain mAP improved (0.8015 --> 0.8350).\n",
      "Class mAP improved (0.7280 --> 0.7974).\n",
      "Performance improved. Saving model ...\n",
      "Domain mAP: 0.8015 --> 0.8350, Class mAP: 0.7280 --> 0.7974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▌                                                                                          | 2/300 [06:18<15:38:06, 188.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "체크포인트가 checkpoints/latest_checkpoint.pth에 저장되었습니다.\n",
      "Epoch [3], Batch [20/44], Loss: 0.7282, Domain Loss: 0.4228, Class Loss: 1.0336, LR: 0.001000\n",
      "Epoch [3], Batch [40/44], Loss: 0.6515, Domain Loss: 0.4987, Class Loss: 0.8043, LR: 0.001000\n",
      "Train set: Epoch: 3, Avg loss: 0.7328, Domain Loss: 0.4465, Class Loss: 1.0192, Domain Acc: 83.23%, Class Acc: 72.98%, Domain mAP: 0.8612, Class mAP: 0.7586, Time: 140.63s\n",
      "Test set: Epoch: 3, Avg loss: 0.7685, Domain Loss: 0.5254, Class Loss: 1.0116, Domain Acc: 79.83%, Class Acc: 74.07%, Domain mAP: 0.8472, Class mAP: 0.8133, Time: 44.18s\n",
      "\n",
      "새로운 최고 Class mAP: 0.8133, Domain mAP: 0.8472\n",
      "새로운 최고 Domain mAP: 0.8472\n",
      "Domain mAP improved (0.8350 --> 0.8472).\n",
      "Class mAP improved (0.7974 --> 0.8133).\n",
      "Performance improved. Saving model ...\n",
      "Domain mAP: 0.8350 --> 0.8472, Class mAP: 0.7974 --> 0.8133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▉                                                                                          | 3/300 [09:24<15:28:22, 187.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "체크포인트가 checkpoints/latest_checkpoint.pth에 저장되었습니다.\n",
      "Epoch [4], Batch [20/44], Loss: 0.6863, Domain Loss: 0.4717, Class Loss: 0.9010, LR: 0.001000\n",
      "Epoch [4], Batch [40/44], Loss: 0.6347, Domain Loss: 0.4617, Class Loss: 0.8077, LR: 0.001000\n",
      "Train set: Epoch: 4, Avg loss: 0.6254, Domain Loss: 0.4085, Class Loss: 0.8424, Domain Acc: 84.53%, Class Acc: 77.05%, Domain mAP: 0.8764, Class mAP: 0.8130, Time: 142.60s\n",
      "Test set: Epoch: 4, Avg loss: 0.7133, Domain Loss: 0.5155, Class Loss: 0.9110, Domain Acc: 81.11%, Class Acc: 75.75%, Domain mAP: 0.8553, Class mAP: 0.8318, Time: 43.63s\n",
      "\n",
      "새로운 최고 Class mAP: 0.8318, Domain mAP: 0.8553\n",
      "새로운 최고 Domain mAP: 0.8553\n",
      "Domain mAP improved (0.8472 --> 0.8553).\n",
      "Class mAP improved (0.8133 --> 0.8318).\n",
      "Performance improved. Saving model ...\n",
      "Domain mAP: 0.8472 --> 0.8553, Class mAP: 0.8133 --> 0.8318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|█▏                                                                                         | 4/300 [12:32<15:25:05, 187.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "체크포인트가 checkpoints/latest_checkpoint.pth에 저장되었습니다.\n",
      "Epoch [5], Batch [20/44], Loss: 0.5856, Domain Loss: 0.4569, Class Loss: 0.7144, LR: 0.001000\n",
      "Epoch [5], Batch [40/44], Loss: 0.5978, Domain Loss: 0.3796, Class Loss: 0.8159, LR: 0.001000\n",
      "Train set: Epoch: 5, Avg loss: 0.5510, Domain Loss: 0.3703, Class Loss: 0.7317, Domain Acc: 86.20%, Class Acc: 79.99%, Domain mAP: 0.8938, Class mAP: 0.8472, Time: 143.06s\n",
      "Test set: Epoch: 5, Avg loss: 0.7020, Domain Loss: 0.5267, Class Loss: 0.8772, Domain Acc: 80.11%, Class Acc: 76.91%, Domain mAP: 0.8517, Class mAP: 0.8352, Time: 43.68s\n",
      "\n",
      "새로운 최고 Class mAP: 0.8352, Domain mAP: 0.8517\n",
      "체크포인트가 checkpoints/Dilated_ResNet50_MultiTask_layer3 - Style, all_Category_checkpoint.pth에 저장되었습니다.\n",
      "Class mAP improved (0.8318 --> 0.8352).\n",
      "Performance improved. Saving model ...\n",
      "Domain mAP: 0.8553 --> 0.8517, Class mAP: 0.8318 --> 0.8352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|█▌                                                                                         | 5/300 [15:40<15:22:37, 187.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "체크포인트가 checkpoints/latest_checkpoint.pth에 저장되었습니다.\n",
      "Epoch [6], Batch [20/44], Loss: 0.5414, Domain Loss: 0.4024, Class Loss: 0.6803, LR: 0.001000\n",
      "Epoch [6], Batch [40/44], Loss: 0.4868, Domain Loss: 0.3426, Class Loss: 0.6310, LR: 0.001000\n",
      "Train set: Epoch: 6, Avg loss: 0.4899, Domain Loss: 0.3391, Class Loss: 0.6407, Domain Acc: 87.36%, Class Acc: 81.91%, Domain mAP: 0.9087, Class mAP: 0.8730, Time: 139.15s\n",
      "Test set: Epoch: 6, Avg loss: 0.7060, Domain Loss: 0.5169, Class Loss: 0.8951, Domain Acc: 81.36%, Class Acc: 75.85%, Domain mAP: 0.8572, Class mAP: 0.8361, Time: 43.99s\n",
      "\n",
      "새로운 최고 Class mAP: 0.8361, Domain mAP: 0.8572\n",
      "새로운 최고 Domain mAP: 0.8572\n",
      "Domain mAP improved (0.8553 --> 0.8572).\n",
      "Class mAP improved (0.8352 --> 0.8361).\n",
      "Performance improved. Saving model ...\n",
      "Domain mAP: 0.8553 --> 0.8572, Class mAP: 0.8352 --> 0.8361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|█▊                                                                                         | 6/300 [18:44<15:13:44, 186.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "체크포인트가 checkpoints/latest_checkpoint.pth에 저장되었습니다.\n",
      "Epoch [7], Batch [20/44], Loss: 0.4189, Domain Loss: 0.2968, Class Loss: 0.5410, LR: 0.001000\n",
      "Epoch [7], Batch [40/44], Loss: 0.4381, Domain Loss: 0.3427, Class Loss: 0.5335, LR: 0.001000\n",
      "Train set: Epoch: 7, Avg loss: 0.4444, Domain Loss: 0.3207, Class Loss: 0.5681, Domain Acc: 87.91%, Class Acc: 83.79%, Domain mAP: 0.9162, Class mAP: 0.8935, Time: 141.40s\n",
      "Test set: Epoch: 7, Avg loss: 0.7676, Domain Loss: 0.6415, Class Loss: 0.8937, Domain Acc: 75.79%, Class Acc: 76.35%, Domain mAP: 0.8313, Class mAP: 0.8406, Time: 45.37s\n",
      "\n",
      "새로운 최고 Class mAP: 0.8406, Domain mAP: 0.8313\n",
      "Class mAP improved (0.8361 --> 0.8406).\n",
      "Performance improved. Saving model ...\n",
      "Domain mAP: 0.8572 --> 0.8313, Class mAP: 0.8361 --> 0.8406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|██                                                                                         | 7/300 [21:51<15:12:35, 186.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "체크포인트가 checkpoints/latest_checkpoint.pth에 저장되었습니다.\n",
      "Epoch [8], Batch [20/44], Loss: 0.4417, Domain Loss: 0.3780, Class Loss: 0.5055, LR: 0.001000\n",
      "Epoch [8], Batch [40/44], Loss: 0.4179, Domain Loss: 0.3079, Class Loss: 0.5279, LR: 0.001000\n",
      "Train set: Epoch: 8, Avg loss: 0.4020, Domain Loss: 0.3001, Class Loss: 0.5039, Domain Acc: 89.14%, Class Acc: 85.74%, Domain mAP: 0.9264, Class mAP: 0.9118, Time: 142.88s\n",
      "Test set: Epoch: 8, Avg loss: 0.7054, Domain Loss: 0.5452, Class Loss: 0.8656, Domain Acc: 80.42%, Class Acc: 77.56%, Domain mAP: 0.8593, Class mAP: 0.8443, Time: 43.68s\n",
      "\n",
      "새로운 최고 Class mAP: 0.8443, Domain mAP: 0.8593\n",
      "새로운 최고 Domain mAP: 0.8593\n",
      "Domain mAP improved (0.8572 --> 0.8593).\n",
      "Class mAP improved (0.8406 --> 0.8443).\n",
      "Performance improved. Saving model ...\n",
      "Domain mAP: 0.8572 --> 0.8593, Class mAP: 0.8406 --> 0.8443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|██▍                                                                                        | 8/300 [24:59<15:10:55, 187.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "체크포인트가 checkpoints/latest_checkpoint.pth에 저장되었습니다.\n",
      "Epoch [9], Batch [20/44], Loss: 0.3723, Domain Loss: 0.2567, Class Loss: 0.4879, LR: 0.001000\n",
      "Epoch [9], Batch [40/44], Loss: 0.3733, Domain Loss: 0.2415, Class Loss: 0.5051, LR: 0.001000\n",
      "Train set: Epoch: 9, Avg loss: 0.3631, Domain Loss: 0.2779, Class Loss: 0.4484, Domain Acc: 89.88%, Class Acc: 86.90%, Domain mAP: 0.9363, Class mAP: 0.9257, Time: 144.43s\n",
      "Test set: Epoch: 9, Avg loss: 0.7941, Domain Loss: 0.7422, Class Loss: 0.8460, Domain Acc: 75.51%, Class Acc: 77.00%, Domain mAP: 0.8498, Class mAP: 0.8472, Time: 44.25s\n",
      "\n",
      "새로운 최고 Class mAP: 0.8472, Domain mAP: 0.8498\n",
      "Class mAP improved (0.8443 --> 0.8472).\n",
      "Performance improved. Saving model ...\n",
      "Domain mAP: 0.8593 --> 0.8498, Class mAP: 0.8443 --> 0.8472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|██▋                                                                                        | 9/300 [28:09<15:11:31, 187.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "체크포인트가 checkpoints/latest_checkpoint.pth에 저장되었습니다.\n",
      "Epoch [10], Batch [20/44], Loss: 0.2784, Domain Loss: 0.2120, Class Loss: 0.3449, LR: 0.001000\n",
      "Epoch [10], Batch [40/44], Loss: 0.3834, Domain Loss: 0.2646, Class Loss: 0.5022, LR: 0.001000\n",
      "Train set: Epoch: 10, Avg loss: 0.3397, Domain Loss: 0.2743, Class Loss: 0.4050, Domain Acc: 90.07%, Class Acc: 88.36%, Domain mAP: 0.9369, Class mAP: 0.9382, Time: 144.91s\n",
      "Test set: Epoch: 10, Avg loss: 0.6924, Domain Loss: 0.5639, Class Loss: 0.8210, Domain Acc: 80.73%, Class Acc: 77.56%, Domain mAP: 0.8604, Class mAP: 0.8559, Time: 44.34s\n",
      "\n",
      "새로운 최고 Class mAP: 0.8559, Domain mAP: 0.8604\n",
      "새로운 최고 Domain mAP: 0.8604\n",
      "체크포인트가 checkpoints/Dilated_ResNet50_MultiTask_layer3 - Style, all_Category_checkpoint.pth에 저장되었습니다.\n",
      "Domain mAP improved (0.8593 --> 0.8604).\n",
      "Class mAP improved (0.8472 --> 0.8559).\n",
      "Performance improved. Saving model ...\n",
      "Domain mAP: 0.8593 --> 0.8604, Class mAP: 0.8472 --> 0.8559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|███                                                                                       | 10/300 [31:20<15:12:23, 188.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "체크포인트가 checkpoints/latest_checkpoint.pth에 저장되었습니다.\n",
      "Epoch [11], Batch [20/44], Loss: 0.2741, Domain Loss: 0.1963, Class Loss: 0.3518, LR: 0.001000\n",
      "Epoch [11], Batch [40/44], Loss: 0.3364, Domain Loss: 0.2600, Class Loss: 0.4127, LR: 0.001000\n",
      "Train set: Epoch: 11, Avg loss: 0.3061, Domain Loss: 0.2423, Class Loss: 0.3699, Domain Acc: 91.06%, Class Acc: 89.43%, Domain mAP: 0.9485, Class mAP: 0.9452, Time: 143.62s\n",
      "Test set: Epoch: 11, Avg loss: 0.7289, Domain Loss: 0.5767, Class Loss: 0.8811, Domain Acc: 80.36%, Class Acc: 77.40%, Domain mAP: 0.8591, Class mAP: 0.8491, Time: 44.08s\n",
      "\n",
      "EarlyStopping 카운터: 1 / 30 (Domain 최고: 0.8604, Class 최고: 0.8559)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|███▎                                                                                      | 11/300 [34:28<15:08:12, 188.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "체크포인트가 checkpoints/latest_checkpoint.pth에 저장되었습니다.\n",
      "Epoch [12], Batch [20/44], Loss: 0.2514, Domain Loss: 0.1944, Class Loss: 0.3085, LR: 0.001000\n",
      "Epoch [12], Batch [40/44], Loss: 0.2762, Domain Loss: 0.2008, Class Loss: 0.3517, LR: 0.001000\n",
      "Train set: Epoch: 12, Avg loss: 0.2806, Domain Loss: 0.2260, Class Loss: 0.3352, Domain Acc: 91.70%, Class Acc: 90.47%, Domain mAP: 0.9556, Class mAP: 0.9519, Time: 143.48s\n",
      "Test set: Epoch: 12, Avg loss: 0.7908, Domain Loss: 0.7489, Class Loss: 0.8326, Domain Acc: 77.56%, Class Acc: 78.18%, Domain mAP: 0.8507, Class mAP: 0.8491, Time: 43.60s\n",
      "\n",
      "EarlyStopping 카운터: 2 / 30 (Domain 최고: 0.8604, Class 최고: 0.8559)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|███▌                                                                                      | 12/300 [37:35<15:03:24, 188.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "체크포인트가 checkpoints/latest_checkpoint.pth에 저장되었습니다.\n",
      "Epoch [13], Batch [20/44], Loss: 0.2283, Domain Loss: 0.1683, Class Loss: 0.2883, LR: 0.001000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchmetrics\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "from utility.utils import DualMAPEarlyStopping  # DualMAPEarlyStopping으로 변경\n",
    "from torchmetrics.classification import MulticlassAveragePrecision\n",
    "from Dataset.data import OfficeHomeDataset  # 기존 데이터셋 클래스 임포트\n",
    "from models.resnet_dilated import ResnetDilated  # ResnetDilated 임포트\n",
    "\n",
    "# 체크포인트 디렉토리 생성\n",
    "os.makedirs('checkpoints', exist_ok=True)\n",
    "\n",
    "# WandB 설정 \n",
    "wandb.login(key=\"ef091b9abcea3186341ddf8995d62bde62d7469e\")\n",
    "wandb.init(\n",
    "    project=\"office-home-classification\", \n",
    "    name=\"Dilated_ResNet50_MultiTask_layer3 - Style, all_Category\",\n",
    "    entity=\"hh0804352-hanyang-university\"\n",
    ")\n",
    "\n",
    "# wandb run name을 체크포인트 경로에 사용\n",
    "run_name = wandb.run.name\n",
    "CHECKPOINT_PATH = os.path.join('checkpoints', f'{run_name}_checkpoint.pth')\n",
    "\n",
    "\n",
    "# 설정\n",
    "config = {\n",
    "    # 모델 설정\n",
    "    \"model\": \"resnet50\",\n",
    "    \"batch_size\": 256,\n",
    "    \"num_epochs\": 300,\n",
    "    \n",
    "    \"learning_rate\": 0.001,  # Adam 기본 학습률\n",
    "    \"optimizer\": \"Adam\",\n",
    "    \n",
    "    # 학습 과정 설정\n",
    "    \"seed\": 2025,\n",
    "    \"deterministic\": False,\n",
    "    \"patience\": 30,  # early stopping patience\n",
    "    \"max_epochs_wait\": float('inf'),\n",
    "    \n",
    "    # 멀티태스크 설정\n",
    "    \"num_domains\": 4,\n",
    "    \"num_classes\": 65,\n",
    "    \"domain_weight\": 0.5,  # 도메인 분류 손실 가중치\n",
    "    \"class_weight\": 0.5,   # 클래스 분류 손실 가중치\n",
    "    \n",
    "    # 시스템 설정\n",
    "    \"num_workers\": 24,\n",
    "    \"pin_memory\": True,\n",
    "    \n",
    "    # 체크포인트 설정\n",
    "    \"save_every\": 5,  # 몇 epoch마다 저장할지\n",
    "    \n",
    "    # 스케줄러 설정\n",
    "    \"scheduler\": \"ReduceLROnPlateau\",\n",
    "    \"scheduler_mode\": \"max\",\n",
    "    \"scheduler_factor\": 0.1,\n",
    "    \"scheduler_patience\": 5,\n",
    "    \"scheduler_verbose\": True,\n",
    "}\n",
    "\n",
    "wandb.config.update(config)\n",
    "\n",
    "class ImprovedMultiTaskModel(nn.Module):\n",
    "    def __init__(self, num_domains=4, num_classes=65):\n",
    "        super(ImprovedMultiTaskModel, self).__init__()\n",
    "        \n",
    "        # 공유 백본 - ResnetDilated 사용\n",
    "        pretrained_resnet = models.resnet50(pretrained=True)\n",
    "        self.backbone = ResnetDilated(pretrained_resnet, dilate_scale=8)\n",
    "        \n",
    "        # 백본 동결 (선택적)\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        # 도메인 분류를 위한 분기점 (layer3 출력에서)\n",
    "        self.domain_branch = nn.Sequential(\n",
    "            nn.Conv2d(1024, 512, kernel_size=1),  # 채널 수 감소\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # 클래스 분류를 위한 분기점 (layer4 출력에서)\n",
    "        self.class_branch = nn.Sequential(\n",
    "            nn.Conv2d(2048, 512, kernel_size=1),  # 채널 수 감소\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # 풀링 레이어\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        # 분류 헤드\n",
    "        self.domain_classifier = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, num_domains)\n",
    "        )\n",
    "        \n",
    "        self.class_classifier = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 백본의 각 스테이지 출력 추출\n",
    "        x = self.backbone.conv1(x)\n",
    "        x = self.backbone.bn1(x)\n",
    "        x = self.backbone.relu1(x)  # relu1으로 수정 (ResnetDilated에서 정의된 이름과 일치)\n",
    "        x = self.backbone.maxpool(x)\n",
    "        \n",
    "        x = self.backbone.layer1(x)\n",
    "        x = self.backbone.layer2(x)\n",
    "        \n",
    "        # Layer3 출력 저장 (도메인 분류용)\n",
    "        layer3_output = self.backbone.layer3(x)\n",
    "        \n",
    "        # Layer4 출력 (클래스 분류용)\n",
    "        layer4_output = self.backbone.layer4(layer3_output)\n",
    "        \n",
    "        # 도메인 분류 경로\n",
    "        domain_features = self.domain_branch(layer3_output)\n",
    "        domain_features = self.avgpool(domain_features)\n",
    "        domain_features = torch.flatten(domain_features, 1)\n",
    "        domain_out = self.domain_classifier(domain_features)\n",
    "        \n",
    "        # 클래스 분류 경로\n",
    "        class_features = self.class_branch(layer4_output)\n",
    "        class_features = self.avgpool(class_features)\n",
    "        class_features = torch.flatten(class_features, 1)\n",
    "        class_out = self.class_classifier(class_features)\n",
    "        \n",
    "        return domain_out, class_out\n",
    "\n",
    "\n",
    "# 데이터 변환 (RandomResizedCrop 제거)\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((224,224)),  # \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# 데이터셋 로드\n",
    "trainset = OfficeHomeDataset(root_dir='./Dataset/train', transform=transform_train)\n",
    "testset = OfficeHomeDataset(root_dir='./Dataset/test', transform=transform_test)\n",
    "\n",
    "# DataLoader 생성\n",
    "trainloader = DataLoader(\n",
    "    trainset, \n",
    "    batch_size=config[\"batch_size\"], \n",
    "    shuffle=True, \n",
    "    pin_memory=config[\"pin_memory\"], \n",
    "    num_workers=config[\"num_workers\"]\n",
    ")\n",
    "\n",
    "testloader = DataLoader(\n",
    "    testset, \n",
    "    batch_size=config[\"batch_size\"], \n",
    "    shuffle=False, \n",
    "    pin_memory=config[\"pin_memory\"], \n",
    "    num_workers=config[\"num_workers\"]\n",
    ")\n",
    "\n",
    "print(f\"Train set size: {len(trainset)}\")\n",
    "print(f\"Test set size: {len(testset)}\")\n",
    "\n",
    "def train(model, trainloader, domain_criterion, class_criterion, optimizer, device, epoch):\n",
    "    \"\"\"\n",
    "    멀티태스크 학습 함수 (도메인 + 클래스 분류)\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    running_loss = 0.0\n",
    "    running_domain_loss = 0.0  # 도메인 손실 별도 추적\n",
    "    running_class_loss = 0.0   # 클래스 손실 별도 추적\n",
    "    domain_correct = 0\n",
    "    class_correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # mAP 계산기 초기화\n",
    "    domain_map = MulticlassAveragePrecision(num_classes=config[\"num_domains\"], average='macro')\n",
    "    class_map = MulticlassAveragePrecision(num_classes=config[\"num_classes\"], average='macro')\n",
    "    \n",
    "    domain_map = domain_map.to(device)\n",
    "    class_map = class_map.to(device)\n",
    "    \n",
    "    for i, (inputs, domain_labels, class_labels) in enumerate(trainloader):\n",
    "        inputs = inputs.to(device)\n",
    "        domain_labels = domain_labels.to(device)\n",
    "        class_labels = class_labels.to(device)\n",
    "        \n",
    "        # 그래디언트 초기화\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 모델 전방 전파\n",
    "        domain_outputs, class_outputs = model(inputs)\n",
    "        \n",
    "        # 손실 계산\n",
    "        domain_loss = domain_criterion(domain_outputs, domain_labels)\n",
    "        class_loss = class_criterion(class_outputs, class_labels)\n",
    "        loss = config[\"domain_weight\"] * domain_loss + config[\"class_weight\"] * class_loss\n",
    "        \n",
    "        # 역전파 및 최적화\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        running_domain_loss += domain_loss.item()  # 도메인 손실 누적\n",
    "        running_class_loss += class_loss.item()    # 클래스 손실 누적\n",
    "        \n",
    "        # 정확도 계산\n",
    "        _, domain_preds = domain_outputs.max(1)\n",
    "        domain_correct += domain_preds.eq(domain_labels).sum().item()\n",
    "        \n",
    "        _, class_preds = class_outputs.max(1)\n",
    "        class_correct += class_preds.eq(class_labels).sum().item()\n",
    "        \n",
    "        total += inputs.size(0)\n",
    "        \n",
    "        # mAP 업데이트\n",
    "        domain_map.update(domain_outputs, domain_labels)\n",
    "        class_map.update(class_outputs, class_labels)\n",
    "        \n",
    "        if (i + 1) % 20 == 0:\n",
    "            print(f'Epoch [{epoch+1}], Batch [{i+1}/{len(trainloader)}], Loss: {loss.item():.4f}, '\n",
    "                  f'Domain Loss: {domain_loss.item():.4f}, Class Loss: {class_loss.item():.4f}, '\n",
    "                  f'LR: {optimizer.param_groups[0][\"lr\"]:.6f}')\n",
    "    \n",
    "    # 에폭 통계\n",
    "    epoch_loss = running_loss / len(trainloader)\n",
    "    epoch_domain_loss = running_domain_loss / len(trainloader)  # 평균 도메인 손실\n",
    "    epoch_class_loss = running_class_loss / len(trainloader)    # 평균 클래스 손실\n",
    "    domain_accuracy = 100.0 * domain_correct / total\n",
    "    class_accuracy = 100.0 * class_correct / total\n",
    "    \n",
    "    # mAP 계산\n",
    "    domain_map_value = domain_map.compute().item()\n",
    "    class_map_value = class_map.compute().item()\n",
    "    \n",
    "    train_time = time.time() - start_time\n",
    "    \n",
    "    # 학습 세트에 대한 성능 출력\n",
    "    print(f'Train set: Epoch: {epoch+1}, Avg loss: {epoch_loss:.4f}, '\n",
    "          f'Domain Loss: {epoch_domain_loss:.4f}, Class Loss: {epoch_class_loss:.4f}, '\n",
    "          f'Domain Acc: {domain_accuracy:.2f}%, Class Acc: {class_accuracy:.2f}%, '\n",
    "          f'Domain mAP: {domain_map_value:.4f}, Class mAP: {class_map_value:.4f}, '\n",
    "          f'Time: {train_time:.2f}s')\n",
    "    \n",
    "    return epoch_loss, epoch_domain_loss, epoch_class_loss, domain_accuracy, class_accuracy, domain_map_value, class_map_value\n",
    "\n",
    "def evaluate(model, dataloader, domain_criterion, class_criterion, device, epoch):\n",
    "    \"\"\"\n",
    "    멀티태스크 평가 함수\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    start_time = time.time()\n",
    "    running_loss = 0.0\n",
    "    running_domain_loss = 0.0  # 도메인 손실 별도 추적\n",
    "    running_class_loss = 0.0   # 클래스 손실 별도 추적\n",
    "    domain_correct = 0\n",
    "    class_correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # mAP 계산기 초기화\n",
    "    domain_map = MulticlassAveragePrecision(num_classes=config[\"num_domains\"], average='macro')\n",
    "    class_map = MulticlassAveragePrecision(num_classes=config[\"num_classes\"], average='macro')\n",
    "    \n",
    "    domain_map = domain_map.to(device)\n",
    "    class_map = class_map.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, domain_labels, class_labels in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            domain_labels = domain_labels.to(device)\n",
    "            class_labels = class_labels.to(device)\n",
    "            \n",
    "            # 순전파\n",
    "            domain_outputs, class_outputs = model(inputs)\n",
    "            \n",
    "            # 손실 계산\n",
    "            domain_loss = domain_criterion(domain_outputs, domain_labels)\n",
    "            class_loss = class_criterion(class_outputs, class_labels)\n",
    "            loss = config[\"domain_weight\"] * domain_loss + config[\"class_weight\"] * class_loss\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            running_domain_loss += domain_loss.item()  # 도메인 손실 누적\n",
    "            running_class_loss += class_loss.item()    # 클래스 손실 누적\n",
    "            \n",
    "            # 정확도 계산\n",
    "            _, domain_preds = domain_outputs.max(1)\n",
    "            domain_correct += domain_preds.eq(domain_labels).sum().item()\n",
    "            \n",
    "            _, class_preds = class_outputs.max(1)\n",
    "            class_correct += class_preds.eq(class_labels).sum().item()\n",
    "            \n",
    "            total += inputs.size(0)\n",
    "            \n",
    "            # mAP 업데이트\n",
    "            domain_map.update(domain_outputs, domain_labels)\n",
    "            class_map.update(class_outputs, class_labels)\n",
    "    \n",
    "    # 평균 손실 및 정확도 계산\n",
    "    eval_loss = running_loss / len(dataloader)\n",
    "    eval_domain_loss = running_domain_loss / len(dataloader)  # 평균 도메인 손실\n",
    "    eval_class_loss = running_class_loss / len(dataloader)    # 평균 클래스 손실\n",
    "    domain_accuracy = 100.0 * domain_correct / total\n",
    "    class_accuracy = 100.0 * class_correct / total\n",
    "    \n",
    "    # mAP 계산\n",
    "    domain_map_value = domain_map.compute().item()\n",
    "    class_map_value = class_map.compute().item()\n",
    "    \n",
    "    # 평가 시간 계산\n",
    "    eval_time = time.time() - start_time\n",
    "    \n",
    "    # 테스트 세트에 대한 성능 출력\n",
    "    print(f'Test set: Epoch: {epoch+1}, Avg loss: {eval_loss:.4f}, '\n",
    "          f'Domain Loss: {eval_domain_loss:.4f}, Class Loss: {eval_class_loss:.4f}, '\n",
    "          f'Domain Acc: {domain_accuracy:.2f}%, Class Acc: {class_accuracy:.2f}%, '\n",
    "          f'Domain mAP: {domain_map_value:.4f}, Class mAP: {class_map_value:.4f}, '\n",
    "          f'Time: {eval_time:.2f}s')\n",
    "    print()\n",
    "    \n",
    "    return eval_loss, eval_domain_loss, eval_class_loss, domain_accuracy, class_accuracy, domain_map_value, class_map_value\n",
    "\n",
    "# 체크포인트 저장 함수 추가\n",
    "def save_checkpoint(model, optimizer, scheduler, epoch, best_class_map, best_domain_map, early_stopping, filename=CHECKPOINT_PATH):\n",
    "    \"\"\"\n",
    "    학습 상태 저장 함수\n",
    "    \"\"\"\n",
    "    # 모델이 DataParallel로 감싸져 있는 경우 처리\n",
    "    model_state_dict = model.module.state_dict() if isinstance(model, nn.DataParallel) else model.state_dict()\n",
    "    \n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model_state_dict,\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),  # 스케줄러 상태 저장 추가\n",
    "        'best_class_map': best_class_map,\n",
    "        'best_domain_map': best_domain_map,\n",
    "        'early_stopping_counter': early_stopping.counter,\n",
    "        'early_stopping_domain_best_score': early_stopping.domain_best_score,  # DualMAP 맞게 수정\n",
    "        'early_stopping_class_best_score': early_stopping.class_best_score,    # DualMAP 맞게 수정\n",
    "        'early_stopping_best_epoch': early_stopping.best_epoch,\n",
    "        'early_stopping_domain_best_epoch': early_stopping.domain_best_epoch,  # DualMAP 맞게 추가\n",
    "        'early_stopping_class_best_epoch': early_stopping.class_best_epoch,    # DualMAP 맞게 추가\n",
    "        'early_stopping_early_stop': early_stopping.early_stop,\n",
    "        'early_stopping_domain_map_max': early_stopping.domain_map_max,  # DualMAP 맞게 추가\n",
    "        'early_stopping_class_map_max': early_stopping.class_map_max,    # DualMAP 맞게 추가\n",
    "        'config': config,  # 설정값도 저장\n",
    "    }\n",
    "    torch.save(checkpoint, filename)\n",
    "    print(f\"체크포인트가 {filename}에 저장되었습니다.\")\n",
    "\n",
    "# 체크포인트 로드 함수 추가\n",
    "def load_checkpoint(model, optimizer, scheduler, early_stopping, filename=CHECKPOINT_PATH):\n",
    "    \"\"\"\n",
    "    학습 상태 로드 함수\n",
    "    \"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        print(f\"체크포인트 파일 {filename}이 존재하지 않습니다. 처음부터 학습을 시작합니다.\")\n",
    "        return model, optimizer, scheduler, early_stopping, 0, 0.0, 0.0\n",
    "    \n",
    "    print(f\"체크포인트 {filename}을 로드합니다.\")\n",
    "    checkpoint = torch.load(filename)\n",
    "    \n",
    "    # 모델이 DataParallel로 감싸져 있는 경우 처리\n",
    "    if isinstance(model, nn.DataParallel):\n",
    "        model.module.load_state_dict(checkpoint['model_state_dict'])\n",
    "    else:\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    \n",
    "    # 스케줄러 상태 로드 추가\n",
    "    if 'scheduler_state_dict' in checkpoint:\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    \n",
    "    # 조기 중단 상태 복원 (DualMAP에 맞게 수정)\n",
    "    early_stopping.counter = checkpoint['early_stopping_counter']\n",
    "    early_stopping.domain_best_score = checkpoint.get('early_stopping_domain_best_score')\n",
    "    early_stopping.class_best_score = checkpoint.get('early_stopping_class_best_score')\n",
    "    early_stopping.best_epoch = checkpoint['early_stopping_best_epoch']\n",
    "    early_stopping.domain_best_epoch = checkpoint.get('early_stopping_domain_best_epoch', early_stopping.best_epoch)\n",
    "    early_stopping.class_best_epoch = checkpoint.get('early_stopping_class_best_epoch', early_stopping.best_epoch)\n",
    "    early_stopping.early_stop = checkpoint['early_stopping_early_stop']\n",
    "    early_stopping.domain_map_max = checkpoint.get('early_stopping_domain_map_max', -np.Inf)\n",
    "    early_stopping.class_map_max = checkpoint.get('early_stopping_class_map_max', -np.Inf)\n",
    "    \n",
    "    # 기타 학습 상태\n",
    "    start_epoch = checkpoint['epoch'] + 1  # 다음 에폭부터 시작\n",
    "    best_class_map = checkpoint['best_class_map']\n",
    "    best_domain_map = checkpoint['best_domain_map']\n",
    "    \n",
    "    print(f\"체크포인트에서 로드 완료: 에폭 {start_epoch}부터 시작합니다.\")\n",
    "    print(f\"이전 최고 성능: Class mAP: {best_class_map:.4f}, Domain mAP: {best_domain_map:.4f}\")\n",
    "    \n",
    "    return model, optimizer, scheduler, early_stopping, start_epoch, best_class_map, best_domain_map\n",
    "\n",
    "# 메인 학습 루프\n",
    "def main_training_loop(model, trainloader, testloader, domain_criterion, class_criterion, optimizer, scheduler, device, num_epochs=None, patience=None, max_epochs_wait=None):\n",
    "    \"\"\"\n",
    "    메인 학습 루프 (mAP 기준 early stopping)\n",
    "    \"\"\"\n",
    "    # config에서 값 가져오기\n",
    "    if num_epochs is None:\n",
    "        num_epochs = config[\"num_epochs\"]\n",
    "    if patience is None:\n",
    "        patience = config[\"patience\"]\n",
    "    if max_epochs_wait is None:\n",
    "        max_epochs_wait = config[\"max_epochs_wait\"]\n",
    "        \n",
    "    # DualMAP 얼리 스토핑 초기화 (AccuracyEarlyStopping에서 변경)\n",
    "    early_stopping = DualMAPEarlyStopping(\n",
    "        patience=patience, \n",
    "        verbose=True, \n",
    "        path='checkpoint.pt', \n",
    "        max_epochs=max_epochs_wait\n",
    "    )\n",
    "    \n",
    "    # 체크포인트 로드 시도\n",
    "    model, optimizer, scheduler, early_stopping, start_epoch, best_class_map, best_domain_map = load_checkpoint(\n",
    "        model, optimizer, scheduler, early_stopping\n",
    "    )\n",
    "\n",
    "    # 이미 로드된 값이 없으면 초기화\n",
    "    if start_epoch == 0:\n",
    "        best_class_map = 0.0\n",
    "        best_domain_map = 0.0\n",
    "    \n",
    "    # tqdm을 사용한 진행 상황 표시\n",
    "    for epoch in tqdm(range(start_epoch, num_epochs)):\n",
    "        # 학습\n",
    "        train_loss, train_domain_loss, train_class_loss, train_domain_acc, train_class_acc, train_domain_map, train_class_map = train(\n",
    "            model, \n",
    "            trainloader, \n",
    "            domain_criterion, \n",
    "            class_criterion, \n",
    "            optimizer, \n",
    "            device, \n",
    "            epoch\n",
    "        )\n",
    "        \n",
    "        # 테스트 데이터로 평가\n",
    "        test_loss, test_domain_loss, test_class_loss, test_domain_acc, test_class_acc, test_domain_map, test_class_map = evaluate(\n",
    "            model, \n",
    "            testloader, \n",
    "            domain_criterion, \n",
    "            class_criterion, \n",
    "            device, \n",
    "            epoch\n",
    "        )\n",
    "        \n",
    "        # 학습률 조정 - 검증 성능에 따라 스케줄러 업데이트\n",
    "        avg_map = (test_domain_map + test_class_map) / 2\n",
    "        scheduler.step(avg_map)  # 스케줄러 호출 추가\n",
    "        \n",
    "        # WandB에 로깅 (도메인 손실과 클래스 손실 별도 로깅)\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"learning_rate\": optimizer.param_groups[0]['lr'],\n",
    "            \"train_loss\": train_loss,\n",
    "            \"train_domain_loss\": train_domain_loss,\n",
    "            \"train_class_loss\": train_class_loss,\n",
    "            \"train_domain_accuracy\": train_domain_acc,\n",
    "            \"train_class_accuracy\": train_class_acc,\n",
    "            \"train_domain_map\": train_domain_map,\n",
    "            \"train_class_map\": train_class_map,\n",
    "            \"test_loss\": test_loss,\n",
    "            \"test_domain_loss\": test_domain_loss,\n",
    "            \"test_class_loss\": test_class_loss,\n",
    "            \"test_domain_accuracy\": test_domain_acc,\n",
    "            \"test_class_accuracy\": test_class_acc,\n",
    "            \"test_domain_map\": test_domain_map,\n",
    "            \"test_class_map\": test_class_map\n",
    "        })\n",
    "            \n",
    "        # 최고 클래스 mAP 모델 저장\n",
    "        if test_class_map > best_class_map:\n",
    "            best_class_map = test_class_map\n",
    "            best_domain_map_at_best_class = test_domain_map\n",
    "            print(f'새로운 최고 Class mAP: {best_class_map:.4f}, Domain mAP: {best_domain_map_at_best_class:.4f}')\n",
    "            # 모델 저장\n",
    "            model_path = f'best_model_class_{wandb.run.name}.pth'\n",
    "            # 모델이 DataParallel로 감싸져 있는 경우 처리\n",
    "            model_state_dict = model.module.state_dict() if isinstance(model, nn.DataParallel) else model.state_dict()\n",
    "            torch.save(model_state_dict, model_path)\n",
    "            wandb.save(model_path)\n",
    "        \n",
    "        # 최고 도메인 mAP 모델 저장\n",
    "        if test_domain_map > best_domain_map:\n",
    "            best_domain_map = test_domain_map\n",
    "            print(f'새로운 최고 Domain mAP: {best_domain_map:.4f}')\n",
    "            # 모델 저장\n",
    "            model_path = f'best_model_domain_{wandb.run.name}.pth'\n",
    "            # 모델이 DataParallel로 감싸져 있는 경우 처리\n",
    "            model_state_dict = model.module.state_dict() if isinstance(model, nn.DataParallel) else model.state_dict()\n",
    "            torch.save(model_state_dict, model_path)\n",
    "            wandb.save(model_path)\n",
    "\n",
    "        # 주기적으로 체크포인트 저장 (설정한 간격마다)\n",
    "        if (epoch + 1) % config[\"save_every\"] == 0:\n",
    "            save_checkpoint(model, optimizer, scheduler, epoch, best_class_map, best_domain_map, early_stopping)\n",
    "\n",
    "        # DualMAP Early stopping 체크 (두 mAP 모두 사용)\n",
    "        early_stopping(test_domain_map, test_class_map, model, epoch)\n",
    "        \n",
    "        # 매 에폭 후에 체크포인트 저장 (가장 최신 상태)\n",
    "        save_checkpoint(model, optimizer, scheduler, epoch, best_class_map, best_domain_map, early_stopping, \n",
    "                       filename=os.path.join('checkpoints', 'latest_checkpoint.pth'))\n",
    "        \n",
    "        if early_stopping.early_stop:\n",
    "            print(f\"에폭 {epoch+1}에서 학습 조기 종료. 최고 성능 에폭: {early_stopping.best_epoch+1}\")\n",
    "            break\n",
    "    \n",
    "    # 훈련 완료 후 최고 모델 로드\n",
    "    print(\"최고 클래스 mAP 모델 로드 중...\")\n",
    "    model_path = f'best_model_class_{wandb.run.name}.pth'\n",
    "    if os.path.exists(model_path):\n",
    "        # 모델이 DataParallel로 감싸져 있는 경우 처리\n",
    "        if isinstance(model, nn.DataParallel):\n",
    "            model.module.load_state_dict(torch.load(model_path))\n",
    "        else:\n",
    "            model.load_state_dict(torch.load(model_path))\n",
    "    else:\n",
    "        print(f\"경고: {model_path} 파일이 존재하지 않습니다. 최종 모델을 사용합니다.\")\n",
    "\n",
    "    # 최종 테스트 평가\n",
    "    final_test_loss, final_test_domain_loss, final_test_class_loss, final_test_domain_acc, final_test_class_acc, final_test_domain_map, final_test_class_map = evaluate(\n",
    "        model, testloader, domain_criterion, class_criterion, device, num_epochs-1\n",
    "    )\n",
    "    \n",
    "    print(f'완료! 최고 Class mAP: {best_class_map:.4f}, 최고 Domain mAP: {best_domain_map:.4f}')\n",
    "    \n",
    "    # WandB에 최종 결과 기록\n",
    "    wandb.run.summary[\"best_class_map\"] = best_class_map\n",
    "    wandb.run.summary[\"best_domain_map\"] = best_domain_map\n",
    "    wandb.run.summary[\"final_test_class_map\"] = final_test_class_map\n",
    "    wandb.run.summary[\"final_test_domain_map\"] = final_test_domain_map\n",
    "\n",
    "    # Early stopping 정보 저장\n",
    "    if early_stopping.early_stop:\n",
    "        wandb.run.summary[\"early_stopped\"] = True\n",
    "        wandb.run.summary[\"early_stopped_epoch\"] = epoch+1\n",
    "        wandb.run.summary[\"best_epoch\"] = early_stopping.best_epoch+1\n",
    "        wandb.run.summary[\"domain_best_epoch\"] = early_stopping.domain_best_epoch+1\n",
    "        wandb.run.summary[\"class_best_epoch\"] = early_stopping.class_best_epoch+1\n",
    "    else:\n",
    "        wandb.run.summary[\"early_stopped\"] = False\n",
    "        \n",
    "    # 최종 체크포인트 저장\n",
    "    save_checkpoint(model, optimizer, scheduler, epoch, best_class_map, best_domain_map, early_stopping, \n",
    "                   filename=os.path.join('checkpoints', 'final_checkpoint.pth'))\n",
    "\n",
    "# 메인 실행 코드\n",
    "if __name__ == \"__main__\":\n",
    "    # 시드 설정\n",
    "    seed = config[\"seed\"]\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "    # 결정적 알고리즘 사용 여부\n",
    "    if config[\"deterministic\"]:\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "    else:\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "    # 디바이스 설정\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # 모델 초기화 - ImprovedMultiTaskModel 사용 (수정됨)\n",
    "    model = ImprovedMultiTaskModel(\n",
    "        num_domains=config[\"num_domains\"], \n",
    "        num_classes=config[\"num_classes\"]\n",
    "    ).to(device)\n",
    "    \n",
    "    # 학습 가능한 파라미터 확인\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"학습 가능한 파라미터: {trainable_params:,} / {total_params:,} ({trainable_params / total_params:.2%})\")\n",
    "    \n",
    "    # 손실 함수\n",
    "    domain_criterion = nn.CrossEntropyLoss()\n",
    "    class_criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # 학습률 차별화 - domain_branch, class_branch도 포함\n",
    "    optimizer = optim.Adam([\n",
    "        {'params': model.domain_branch.parameters(), 'lr': config[\"learning_rate\"]},  # 도메인 분기점\n",
    "        {'params': model.class_branch.parameters(), 'lr': config[\"learning_rate\"]},   # 클래스 분기점\n",
    "        {'params': model.domain_classifier.parameters(), 'lr': config[\"learning_rate\"]},  # 도메인 분류기\n",
    "        {'params': model.class_classifier.parameters(), 'lr': config[\"learning_rate\"]}    # 클래스 분류기\n",
    "    ])\n",
    "    \n",
    "    # 스케줄러 초기화\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, \n",
    "        mode=config[\"scheduler_mode\"], \n",
    "        factor=config[\"scheduler_factor\"], \n",
    "        patience=config[\"scheduler_patience\"], \n",
    "        verbose=config[\"scheduler_verbose\"]\n",
    "    )\n",
    "    \n",
    "    # WandB에 모델 구조 기록\n",
    "    wandb.watch(model, log=\"all\")\n",
    "    \n",
    "    # GPU 가속 \n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(f\"{torch.cuda.device_count()}개의 GPU를 사용합니다.\")\n",
    "        model = nn.DataParallel(model)\n",
    "    \n",
    "    # 훈련 시작 시간 기록\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # 메인 학습 루프 호출 \n",
    "    try:\n",
    "        main_training_loop(\n",
    "            model=model,\n",
    "            trainloader=trainloader,\n",
    "            testloader=testloader,\n",
    "            domain_criterion=domain_criterion,\n",
    "            class_criterion=class_criterion,\n",
    "            optimizer=optimizer,\n",
    "            scheduler=scheduler,  # 스케줄러 전달 추가\n",
    "            device=device\n",
    "        )\n",
    "    except KeyboardInterrupt:\n",
    "        # 사용자가 Ctrl+C로 중단한 경우, 현재 상태 저장\n",
    "        print(\"학습이 사용자에 의해 중단되었습니다. 현재 상태를 저장합니다.\")\n",
    "        early_stopping = DualMAPEarlyStopping(patience=config[\"patience\"], verbose=True)\n",
    "        save_checkpoint(model, optimizer, scheduler, 0, 0.0, 0.0, early_stopping, \n",
    "                       filename=os.path.join('checkpoints', 'interrupted_checkpoint.pth'))\n",
    "    \n",
    "    # 훈련 종료 시간 및 출력\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    wandb.log({\"total_training_time\": total_time})\n",
    "    \n",
    "    print(f\"전체 학습 시간: {total_time:.2f} 초\")\n",
    "    \n",
    "    # WandB 실행 종료\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7d0f34-01c7-4f72-9853-140d1528e1e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
