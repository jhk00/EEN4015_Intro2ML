{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "516f9d5f-6361-4b03-98ee-e1ba4de86b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/guswls/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msokjh1310\u001b[0m (\u001b[33msokjh1310-hanyang-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/guswls/EEN4015_Intro2ML/pbl-2/wandb/run-20250413_054207-lcdowxro</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sokjh1310-hanyang-university/PBL-2/runs/lcdowxro' target=\"_blank\">resnet18_cutmix,flip)</a></strong> to <a href='https://wandb.ai/sokjh1310-hanyang-university/PBL-2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sokjh1310-hanyang-university/PBL-2' target=\"_blank\">https://wandb.ai/sokjh1310-hanyang-university/PBL-2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sokjh1310-hanyang-university/PBL-2/runs/lcdowxro' target=\"_blank\">https://wandb.ai/sokjh1310-hanyang-university/PBL-2/runs/lcdowxro</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Train set size: 40000\n",
      "Validation set size: 5000\n",
      "Test set size: 5000\n",
      "Using device: cuda\n",
      "2개의 GPU를 사용합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                       | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1], Batch [50/313], Loss: 4.1063\n",
      "Epoch [1], Batch [100/313], Loss: 4.0174\n",
      "Epoch [1], Batch [150/313], Loss: 3.7361\n",
      "Epoch [1], Batch [200/313], Loss: 3.9623\n",
      "Epoch [1], Batch [250/313], Loss: 3.4484\n",
      "Epoch [1], Batch [300/313], Loss: 4.2066\n",
      "Train set: Epoch: 1, Average loss:4.0147, LR: 0.001000 Top-1 Accuracy: 9.4900%, Top-5 Accuracy: 28.9800%, Time consumed:38.73s\n",
      "Val set: Epoch: 1, Average loss:3.5829, Top-1 Accuracy: 13.7800%, Top-5 Accuracy: 39.5800%, Time consumed:4.42s\n",
      "\n",
      "New best top-1 accuracy: 13.78%, top-5 accuracy: 39.58%\n",
      "New best top-5 accuracy: 39.58%\n",
      "Validation loss decreased (inf --> 3.582931). Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▉                                                                                            | 1/100 [00:43<1:11:32, 43.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2], Batch [50/313], Loss: 4.3247\n",
      "Epoch [2], Batch [100/313], Loss: 3.0742\n",
      "Epoch [2], Batch [150/313], Loss: 4.0784\n",
      "Epoch [2], Batch [200/313], Loss: 4.0387\n",
      "Epoch [2], Batch [250/313], Loss: 3.9220\n",
      "Epoch [2], Batch [300/313], Loss: 4.1254\n",
      "Train set: Epoch: 2, Average loss:3.4348, LR: 0.001000 Top-1 Accuracy: 19.9675%, Top-5 Accuracy: 47.8400%, Time consumed:37.12s\n",
      "Val set: Epoch: 2, Average loss:2.7454, Top-1 Accuracy: 28.4800%, Top-5 Accuracy: 61.0000%, Time consumed:4.49s\n",
      "\n",
      "New best top-1 accuracy: 28.48%, top-5 accuracy: 61.00%\n",
      "New best top-5 accuracy: 61.00%\n",
      "Validation loss decreased (3.582931 --> 2.745432). Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|█▊                                                                                           | 2/100 [01:25<1:09:22, 42.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3], Batch [50/313], Loss: 2.7739\n",
      "Epoch [3], Batch [100/313], Loss: 4.0987\n",
      "Epoch [3], Batch [150/313], Loss: 3.8484\n",
      "Epoch [3], Batch [200/313], Loss: 2.4346\n",
      "Epoch [3], Batch [250/313], Loss: 3.4548\n",
      "Epoch [3], Batch [300/313], Loss: 2.3711\n",
      "Train set: Epoch: 3, Average loss:3.0284, LR: 0.001000 Top-1 Accuracy: 28.9675%, Top-5 Accuracy: 59.7625%, Time consumed:39.24s\n",
      "Val set: Epoch: 3, Average loss:2.6246, Top-1 Accuracy: 31.8200%, Top-5 Accuracy: 65.0400%, Time consumed:4.91s\n",
      "\n",
      "New best top-1 accuracy: 31.82%, top-5 accuracy: 65.04%\n",
      "New best top-5 accuracy: 65.04%\n",
      "Validation loss decreased (2.745432 --> 2.624639). Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|██▊                                                                                          | 3/100 [02:09<1:10:06, 43.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4], Batch [50/313], Loss: 3.6297\n",
      "Epoch [4], Batch [100/313], Loss: 2.1832\n",
      "Epoch [4], Batch [150/313], Loss: 2.2232\n",
      "Epoch [4], Batch [200/313], Loss: 4.0074\n",
      "Epoch [4], Batch [250/313], Loss: 2.5438\n",
      "Epoch [4], Batch [300/313], Loss: 3.5922\n",
      "Train set: Epoch: 4, Average loss:2.7848, LR: 0.001000 Top-1 Accuracy: 35.0975%, Top-5 Accuracy: 65.8300%, Time consumed:40.07s\n",
      "Val set: Epoch: 4, Average loss:2.2976, Top-1 Accuracy: 39.7000%, Top-5 Accuracy: 71.7000%, Time consumed:4.95s\n",
      "\n",
      "New best top-1 accuracy: 39.70%, top-5 accuracy: 71.70%\n",
      "New best top-5 accuracy: 71.70%\n",
      "Validation loss decreased (2.624639 --> 2.297627). Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|███▋                                                                                         | 4/100 [02:54<1:10:35, 44.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5], Batch [50/313], Loss: 3.6288\n",
      "Epoch [5], Batch [100/313], Loss: 2.9908\n",
      "Epoch [5], Batch [150/313], Loss: 3.4048\n",
      "Epoch [5], Batch [200/313], Loss: 2.0017\n",
      "Epoch [5], Batch [250/313], Loss: 1.6664\n",
      "Epoch [5], Batch [300/313], Loss: 3.0194\n",
      "Train set: Epoch: 5, Average loss:2.5664, LR: 0.001000 Top-1 Accuracy: 40.5075%, Top-5 Accuracy: 71.1825%, Time consumed:38.96s\n",
      "Val set: Epoch: 5, Average loss:2.0400, Top-1 Accuracy: 45.5400%, Top-5 Accuracy: 76.8800%, Time consumed:4.91s\n",
      "\n",
      "New best top-1 accuracy: 45.54%, top-5 accuracy: 76.88%\n",
      "New best top-5 accuracy: 76.88%\n",
      "Validation loss decreased (2.297627 --> 2.040017). Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|████▋                                                                                        | 5/100 [03:39<1:09:51, 44.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6], Batch [50/313], Loss: 1.7060\n",
      "Epoch [6], Batch [100/313], Loss: 1.8482\n",
      "Epoch [6], Batch [150/313], Loss: 1.9012\n",
      "Epoch [6], Batch [200/313], Loss: 2.0130\n",
      "Epoch [6], Batch [250/313], Loss: 3.6180\n",
      "Epoch [6], Batch [300/313], Loss: 1.7218\n",
      "Train set: Epoch: 6, Average loss:2.4082, LR: 0.001000 Top-1 Accuracy: 43.5250%, Top-5 Accuracy: 73.5425%, Time consumed:41.44s\n",
      "Val set: Epoch: 6, Average loss:1.8709, Top-1 Accuracy: 49.6200%, Top-5 Accuracy: 80.1600%, Time consumed:4.87s\n",
      "\n",
      "New best top-1 accuracy: 49.62%, top-5 accuracy: 80.16%\n",
      "New best top-5 accuracy: 80.16%\n",
      "Validation loss decreased (2.040017 --> 1.870878). Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|█████▌                                                                                       | 6/100 [04:25<1:10:25, 44.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7], Batch [50/313], Loss: 3.0954\n",
      "Epoch [7], Batch [100/313], Loss: 1.6126\n",
      "Epoch [7], Batch [150/313], Loss: 3.2703\n",
      "Epoch [7], Batch [200/313], Loss: 3.6132\n",
      "Epoch [7], Batch [250/313], Loss: 1.3973\n",
      "Epoch [7], Batch [300/313], Loss: 2.7603\n",
      "Train set: Epoch: 7, Average loss:2.3405, LR: 0.001000 Top-1 Accuracy: 47.1825%, Top-5 Accuracy: 76.4700%, Time consumed:39.39s\n",
      "Val set: Epoch: 7, Average loss:1.8449, Top-1 Accuracy: 50.9400%, Top-5 Accuracy: 80.8400%, Time consumed:5.24s\n",
      "\n",
      "New best top-1 accuracy: 50.94%, top-5 accuracy: 80.84%\n",
      "New best top-5 accuracy: 80.84%\n",
      "Validation loss decreased (1.870878 --> 1.844943). Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|██████▌                                                                                      | 7/100 [05:10<1:09:39, 44.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8], Batch [50/313], Loss: 1.8348\n",
      "Epoch [8], Batch [100/313], Loss: 3.2642\n",
      "Epoch [8], Batch [150/313], Loss: 2.2593\n",
      "Epoch [8], Batch [200/313], Loss: 1.3345\n",
      "Epoch [8], Batch [250/313], Loss: 2.4002\n",
      "Epoch [8], Batch [300/313], Loss: 1.3293\n",
      "Train set: Epoch: 8, Average loss:2.1341, LR: 0.001000 Top-1 Accuracy: 51.8725%, Top-5 Accuracy: 79.9350%, Time consumed:38.89s\n",
      "Val set: Epoch: 8, Average loss:1.6160, Top-1 Accuracy: 56.1800%, Top-5 Accuracy: 84.5200%, Time consumed:4.82s\n",
      "\n",
      "New best top-1 accuracy: 56.18%, top-5 accuracy: 84.52%\n",
      "New best top-5 accuracy: 84.52%\n",
      "Validation loss decreased (1.844943 --> 1.616027). Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|███████▍                                                                                     | 8/100 [05:54<1:08:25, 44.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9], Batch [50/313], Loss: 1.3715\n",
      "Epoch [9], Batch [100/313], Loss: 0.9351\n",
      "Epoch [9], Batch [150/313], Loss: 1.2889\n",
      "Epoch [9], Batch [200/313], Loss: 1.2279\n",
      "Epoch [9], Batch [250/313], Loss: 3.5772\n",
      "Epoch [9], Batch [300/313], Loss: 2.9425\n",
      "Train set: Epoch: 9, Average loss:2.0252, LR: 0.001000 Top-1 Accuracy: 54.1175%, Top-5 Accuracy: 80.8775%, Time consumed:38.87s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|████████▎                                                                                    | 9/100 [06:38<1:07:18, 44.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val set: Epoch: 9, Average loss:1.6059, Top-1 Accuracy: 55.5200%, Top-5 Accuracy: 84.5200%, Time consumed:4.83s\n",
      "\n",
      "Validation loss decreased (1.616027 --> 1.605919). Saving model ...\n",
      "Epoch [10], Batch [50/313], Loss: 1.1718\n",
      "Epoch [10], Batch [100/313], Loss: 2.6991\n",
      "Epoch [10], Batch [150/313], Loss: 1.1650\n",
      "Epoch [10], Batch [200/313], Loss: 0.9993\n",
      "Epoch [10], Batch [250/313], Loss: 0.9819\n",
      "Epoch [10], Batch [300/313], Loss: 1.2751\n",
      "Train set: Epoch: 10, Average loss:1.8359, LR: 0.001000 Top-1 Accuracy: 59.5775%, Top-5 Accuracy: 84.8975%, Time consumed:38.76s\n",
      "Val set: Epoch: 10, Average loss:1.5582, Top-1 Accuracy: 57.2600%, Top-5 Accuracy: 85.5200%, Time consumed:4.86s\n",
      "\n",
      "New best top-1 accuracy: 57.26%, top-5 accuracy: 85.52%\n",
      "New best top-5 accuracy: 85.52%\n",
      "Validation loss decreased (1.605919 --> 1.558218). Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█████████▏                                                                                  | 10/100 [07:22<1:06:20, 44.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11], Batch [50/313], Loss: 1.0114\n",
      "Epoch [11], Batch [100/313], Loss: 3.0600\n",
      "Epoch [11], Batch [150/313], Loss: 1.1113\n",
      "Epoch [11], Batch [200/313], Loss: 1.0413\n",
      "Epoch [11], Batch [250/313], Loss: 1.0316\n",
      "Epoch [11], Batch [300/313], Loss: 2.4228\n",
      "Train set: Epoch: 11, Average loss:1.7513, LR: 0.001000 Top-1 Accuracy: 62.1700%, Top-5 Accuracy: 86.5975%, Time consumed:39.53s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|██████████                                                                                  | 11/100 [08:06<1:05:38, 44.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val set: Epoch: 11, Average loss:1.6052, Top-1 Accuracy: 56.0600%, Top-5 Accuracy: 84.1400%, Time consumed:4.77s\n",
      "\n",
      "EarlyStopping 카운터: 1 / 10\n",
      "Epoch [12], Batch [50/313], Loss: 2.0131\n",
      "Epoch [12], Batch [100/313], Loss: 1.9560\n",
      "Epoch [12], Batch [150/313], Loss: 3.2636\n",
      "Epoch [12], Batch [200/313], Loss: 1.0097\n",
      "Epoch [12], Batch [250/313], Loss: 0.6208\n",
      "Epoch [12], Batch [300/313], Loss: 0.8656\n",
      "Train set: Epoch: 12, Average loss:1.6631, LR: 0.001000 Top-1 Accuracy: 65.0600%, Top-5 Accuracy: 87.8375%, Time consumed:39.95s\n",
      "Val set: Epoch: 12, Average loss:1.5331, Top-1 Accuracy: 58.1000%, Top-5 Accuracy: 86.1800%, Time consumed:4.88s\n",
      "\n",
      "New best top-1 accuracy: 58.10%, top-5 accuracy: 86.18%\n",
      "New best top-5 accuracy: 86.18%\n",
      "Validation loss decreased (1.558218 --> 1.533124). Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|███████████                                                                                 | 12/100 [08:51<1:05:16, 44.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13], Batch [50/313], Loss: 1.6059\n",
      "Epoch [13], Batch [100/313], Loss: 3.1278\n",
      "Epoch [13], Batch [150/313], Loss: 1.6520\n",
      "Epoch [13], Batch [200/313], Loss: 1.7702\n",
      "Epoch [13], Batch [250/313], Loss: 1.5923\n",
      "Epoch [13], Batch [300/313], Loss: 0.6529\n",
      "Train set: Epoch: 13, Average loss:1.7279, LR: 0.001000 Top-1 Accuracy: 64.3625%, Top-5 Accuracy: 87.2850%, Time consumed:38.17s\n",
      "Val set: Epoch: 13, Average loss:1.4508, Top-1 Accuracy: 60.6600%, Top-5 Accuracy: 86.7600%, Time consumed:5.14s\n",
      "\n",
      "New best top-1 accuracy: 60.66%, top-5 accuracy: 86.76%\n",
      "New best top-5 accuracy: 86.76%\n",
      "Validation loss decreased (1.533124 --> 1.450814). Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|███████████▉                                                                                | 13/100 [09:35<1:04:07, 44.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14], Batch [50/313], Loss: 3.1650\n",
      "Epoch [14], Batch [100/313], Loss: 0.9460\n",
      "Epoch [14], Batch [150/313], Loss: 0.6072\n",
      "Epoch [14], Batch [200/313], Loss: 0.5138\n",
      "Epoch [14], Batch [250/313], Loss: 2.5388\n",
      "Epoch [14], Batch [300/313], Loss: 0.7680\n",
      "Train set: Epoch: 14, Average loss:1.5355, LR: 0.001000 Top-1 Accuracy: 68.2300%, Top-5 Accuracy: 88.6500%, Time consumed:38.77s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|████████████▉                                                                               | 14/100 [10:18<1:03:01, 43.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val set: Epoch: 14, Average loss:1.5129, Top-1 Accuracy: 60.3600%, Top-5 Accuracy: 85.8000%, Time consumed:4.63s\n",
      "\n",
      "EarlyStopping 카운터: 1 / 10\n",
      "Epoch [15], Batch [50/313], Loss: 2.8411\n",
      "Epoch [15], Batch [100/313], Loss: 1.1353\n",
      "Epoch [15], Batch [150/313], Loss: 0.4348\n",
      "Epoch [15], Batch [200/313], Loss: 2.5301\n",
      "Epoch [15], Batch [250/313], Loss: 0.5535\n",
      "Epoch [15], Batch [300/313], Loss: 2.9318\n",
      "Train set: Epoch: 15, Average loss:1.4327, LR: 0.001000 Top-1 Accuracy: 72.2275%, Top-5 Accuracy: 91.0150%, Time consumed:39.59s\n",
      "Val set: Epoch: 15, Average loss:1.4053, Top-1 Accuracy: 61.1200%, Top-5 Accuracy: 87.8000%, Time consumed:4.97s\n",
      "\n",
      "New best top-1 accuracy: 61.12%, top-5 accuracy: 87.80%\n",
      "New best top-5 accuracy: 87.80%\n",
      "Validation loss decreased (1.450814 --> 1.405282). Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█████████████▊                                                                              | 15/100 [11:03<1:02:39, 44.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16], Batch [50/313], Loss: 0.3682\n",
      "Epoch [16], Batch [100/313], Loss: 2.6680\n",
      "Epoch [16], Batch [150/313], Loss: 2.3585\n",
      "Epoch [16], Batch [200/313], Loss: 2.7712\n",
      "Epoch [16], Batch [250/313], Loss: 0.4261\n",
      "Epoch [16], Batch [300/313], Loss: 2.8475\n",
      "Train set: Epoch: 16, Average loss:1.4375, LR: 0.001000 Top-1 Accuracy: 73.1625%, Top-5 Accuracy: 91.0125%, Time consumed:41.35s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|██████████████▋                                                                             | 16/100 [11:49<1:02:46, 44.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val set: Epoch: 16, Average loss:1.4126, Top-1 Accuracy: 62.0000%, Top-5 Accuracy: 87.2400%, Time consumed:4.76s\n",
      "\n",
      "New best top-1 accuracy: 62.00%, top-5 accuracy: 87.24%\n",
      "EarlyStopping 카운터: 1 / 10\n",
      "Epoch [17], Batch [50/313], Loss: 0.3765\n",
      "Epoch [17], Batch [100/313], Loss: 0.4438\n",
      "Epoch [17], Batch [150/313], Loss: 0.4310\n",
      "Epoch [17], Batch [200/313], Loss: 0.3496\n",
      "Epoch [17], Batch [250/313], Loss: 0.4913\n",
      "Epoch [17], Batch [300/313], Loss: 0.4471\n",
      "Train set: Epoch: 17, Average loss:1.3209, LR: 0.001000 Top-1 Accuracy: 75.3475%, Top-5 Accuracy: 91.0200%, Time consumed:38.62s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|███████████████▋                                                                            | 17/100 [12:33<1:01:33, 44.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val set: Epoch: 17, Average loss:1.4467, Top-1 Accuracy: 61.9400%, Top-5 Accuracy: 86.4000%, Time consumed:5.11s\n",
      "\n",
      "EarlyStopping 카운터: 2 / 10\n",
      "Epoch [18], Batch [50/313], Loss: 2.9908\n",
      "Epoch [18], Batch [100/313], Loss: 2.5674\n",
      "Epoch [18], Batch [150/313], Loss: 2.9298\n",
      "Epoch [18], Batch [200/313], Loss: 0.3321\n",
      "Epoch [18], Batch [250/313], Loss: 3.1753\n",
      "Epoch [18], Batch [300/313], Loss: 0.4172\n",
      "Train set: Epoch: 18, Average loss:1.3899, LR: 0.001000 Top-1 Accuracy: 74.2450%, Top-5 Accuracy: 91.0125%, Time consumed:38.40s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|████████████████▌                                                                           | 18/100 [13:16<1:00:18, 44.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val set: Epoch: 18, Average loss:1.4178, Top-1 Accuracy: 62.7200%, Top-5 Accuracy: 87.5800%, Time consumed:4.73s\n",
      "\n",
      "New best top-1 accuracy: 62.72%, top-5 accuracy: 87.58%\n",
      "EarlyStopping 카운터: 3 / 10\n",
      "Epoch [19], Batch [50/313], Loss: 0.3420\n",
      "Epoch [19], Batch [100/313], Loss: 0.2586\n",
      "Epoch [19], Batch [150/313], Loss: 1.3814\n",
      "Epoch [19], Batch [200/313], Loss: 0.2950\n",
      "Epoch [19], Batch [250/313], Loss: 0.3465\n",
      "Epoch [19], Batch [300/313], Loss: 2.8744\n",
      "Train set: Epoch: 19, Average loss:1.3302, LR: 0.001000 Top-1 Accuracy: 77.1225%, Top-5 Accuracy: 92.2000%, Time consumed:39.59s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█████████████████▊                                                                            | 19/100 [14:00<59:37, 44.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val set: Epoch: 19, Average loss:1.5090, Top-1 Accuracy: 60.8600%, Top-5 Accuracy: 86.3600%, Time consumed:4.67s\n",
      "\n",
      "EarlyStopping 카운터: 4 / 10\n",
      "Epoch [20], Batch [50/313], Loss: 2.4369\n",
      "Epoch [20], Batch [100/313], Loss: 1.4266\n",
      "Epoch [20], Batch [150/313], Loss: 2.9389\n",
      "Epoch [20], Batch [200/313], Loss: 2.4595\n",
      "Epoch [20], Batch [250/313], Loss: 0.1765\n",
      "Epoch [20], Batch [300/313], Loss: 1.1663\n",
      "Train set: Epoch: 20, Average loss:1.1738, LR: 0.001000 Top-1 Accuracy: 81.3000%, Top-5 Accuracy: 94.2950%, Time consumed:38.91s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██████████████████▊                                                                           | 20/100 [14:44<58:42, 44.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val set: Epoch: 20, Average loss:1.4604, Top-1 Accuracy: 61.9200%, Top-5 Accuracy: 86.5800%, Time consumed:4.79s\n",
      "\n",
      "EarlyStopping 카운터: 5 / 10\n",
      "Epoch [21], Batch [50/313], Loss: 0.2155\n",
      "Epoch [21], Batch [100/313], Loss: 0.1157\n",
      "Epoch [21], Batch [150/313], Loss: 1.6592\n",
      "Epoch [21], Batch [200/313], Loss: 0.2540\n",
      "Epoch [21], Batch [250/313], Loss: 0.2187\n",
      "Epoch [21], Batch [300/313], Loss: 0.2118\n",
      "Train set: Epoch: 21, Average loss:1.2035, LR: 0.001000 Top-1 Accuracy: 80.2500%, Top-5 Accuracy: 93.3675%, Time consumed:37.97s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|███████████████████▋                                                                          | 21/100 [15:27<57:23, 43.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val set: Epoch: 21, Average loss:1.4766, Top-1 Accuracy: 61.6600%, Top-5 Accuracy: 86.2800%, Time consumed:4.60s\n",
      "\n",
      "EarlyStopping 카운터: 6 / 10\n",
      "Epoch [22], Batch [50/313], Loss: 2.1471\n",
      "Epoch [22], Batch [100/313], Loss: 0.1810\n",
      "Epoch [22], Batch [150/313], Loss: 0.1287\n",
      "Epoch [22], Batch [200/313], Loss: 2.6588\n",
      "Epoch [22], Batch [250/313], Loss: 2.4978\n",
      "Epoch [22], Batch [300/313], Loss: 2.6249\n",
      "Train set: Epoch: 22, Average loss:1.1372, LR: 0.001000 Top-1 Accuracy: 81.1300%, Top-5 Accuracy: 93.6250%, Time consumed:38.44s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|████████████████████▋                                                                         | 22/100 [16:10<56:41, 43.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val set: Epoch: 22, Average loss:1.4733, Top-1 Accuracy: 61.4200%, Top-5 Accuracy: 86.5200%, Time consumed:5.21s\n",
      "\n",
      "EarlyStopping 카운터: 7 / 10\n",
      "Epoch [23], Batch [50/313], Loss: 0.1167\n",
      "Epoch [23], Batch [100/313], Loss: 3.0907\n",
      "Epoch [23], Batch [150/313], Loss: 0.1254\n",
      "Epoch [23], Batch [200/313], Loss: 1.5300\n",
      "Epoch [23], Batch [250/313], Loss: 1.0627\n",
      "Epoch [23], Batch [300/313], Loss: 2.3783\n",
      "Train set: Epoch: 23, Average loss:1.1479, LR: 0.001000 Top-1 Accuracy: 83.3750%, Top-5 Accuracy: 94.9475%, Time consumed:38.08s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|█████████████████████▌                                                                        | 23/100 [16:53<55:39, 43.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val set: Epoch: 23, Average loss:1.4794, Top-1 Accuracy: 61.9600%, Top-5 Accuracy: 86.4800%, Time consumed:4.73s\n",
      "\n",
      "EarlyStopping 카운터: 8 / 10\n",
      "Epoch [24], Batch [50/313], Loss: 0.1095\n",
      "Epoch [24], Batch [100/313], Loss: 0.1184\n",
      "Epoch [24], Batch [150/313], Loss: 3.2137\n",
      "Epoch [24], Batch [200/313], Loss: 0.1758\n",
      "Epoch [24], Batch [250/313], Loss: 0.2048\n",
      "Epoch [24], Batch [300/313], Loss: 0.1454\n",
      "Train set: Epoch: 24, Average loss:1.1364, LR: 0.001000 Top-1 Accuracy: 82.0250%, Top-5 Accuracy: 94.2625%, Time consumed:38.61s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██████████████████████▌                                                                       | 24/100 [17:37<54:58, 43.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val set: Epoch: 24, Average loss:1.4784, Top-1 Accuracy: 62.5400%, Top-5 Accuracy: 85.4800%, Time consumed:4.84s\n",
      "\n",
      "EarlyStopping 카운터: 9 / 10\n",
      "Epoch [25], Batch [50/313], Loss: 0.0812\n",
      "Epoch [25], Batch [100/313], Loss: 2.0912\n",
      "Epoch [25], Batch [150/313], Loss: 2.7426\n",
      "Epoch [25], Batch [200/313], Loss: 0.1393\n",
      "Epoch [25], Batch [250/313], Loss: 1.6891\n",
      "Epoch [25], Batch [300/313], Loss: 0.1409\n",
      "Train set: Epoch: 25, Average loss:1.0345, LR: 0.001000 Top-1 Accuracy: 83.3400%, Top-5 Accuracy: 94.1300%, Time consumed:41.19s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██████████████████████▌                                                                       | 24/100 [18:23<58:14, 45.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val set: Epoch: 25, Average loss:1.4789, Top-1 Accuracy: 62.5200%, Top-5 Accuracy: 86.0400%, Time consumed:5.14s\n",
      "\n",
      "EarlyStopping 카운터: 10 / 10\n",
      "Early stopping triggered. Training stopped.\n",
      "Loading best model from early stopping checkpoint...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Epoch: 100, Average loss:1.4649, Top-1 Accuracy: 63.0000%, Top-5 Accuracy: 86.6200%, Time consumed:4.64s\n",
      "\n",
      "Finish! Best validation top-1 accuracy: 62.72%, Best validation top-5 accuracy: 87.80%\n",
      "Final test top-1 accuracy: 63.00%, Final test top-5 accuracy: 86.62%\n",
      "Total training time: 1108.09 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇▇███</td></tr><tr><td>learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_accuracy_top1</td><td>▁</td></tr><tr><td>test_accuracy_top5</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>total_training_time</td><td>▁</td></tr><tr><td>train_accuracy_top1</td><td>▁▂▃▃▄▄▅▅▅▆▆▆▆▇▇▇▇▇▇██████</td></tr><tr><td>train_accuracy_top5</td><td>▁▃▄▅▅▆▆▆▇▇▇▇▇▇███████████</td></tr><tr><td>train_loss</td><td>█▇▆▅▅▄▄▄▃▃▃▂▃▂▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>val_accuracy_top1</td><td>▁▃▄▅▆▆▆▇▇▇▇▇█████████████</td></tr><tr><td>val_accuracy_top5</td><td>▁▄▅▆▆▇▇███▇██████████████</td></tr><tr><td>val_loss</td><td>█▅▅▄▃▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_val_accuracy_top1</td><td>62.72</td></tr><tr><td>best_val_accuracy_top5</td><td>87.8</td></tr><tr><td>early_stopped</td><td>True</td></tr><tr><td>early_stopped_epoch</td><td>25</td></tr><tr><td>epoch</td><td>25</td></tr><tr><td>learning_rate</td><td>0.001</td></tr><tr><td>test_accuracy_top1</td><td>63</td></tr><tr><td>test_accuracy_top5</td><td>86.62</td></tr><tr><td>test_loss</td><td>1.46495</td></tr><tr><td>total_training_time</td><td>1108.08851</td></tr><tr><td>train_accuracy_top1</td><td>83.34</td></tr><tr><td>train_accuracy_top5</td><td>94.13</td></tr><tr><td>train_loss</td><td>1.03448</td></tr><tr><td>val_accuracy_top1</td><td>62.52</td></tr><tr><td>val_accuracy_top5</td><td>86.04</td></tr><tr><td>val_loss</td><td>1.47892</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">resnet18_cutmix,flip)</strong> at: <a href='https://wandb.ai/sokjh1310-hanyang-university/PBL-2/runs/lcdowxro' target=\"_blank\">https://wandb.ai/sokjh1310-hanyang-university/PBL-2/runs/lcdowxro</a><br> View project at: <a href='https://wandb.ai/sokjh1310-hanyang-university/PBL-2' target=\"_blank\">https://wandb.ai/sokjh1310-hanyang-university/PBL-2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250413_054207-lcdowxro/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.v2 as transforms_v2  # CutMix를 위한 v2 transforms 추가\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from tools.tool import EarlyStopping\n",
    "from models.resnet import resnet18, resnet34, resnet50\n",
    "\n",
    "wandb.login(key=\"ef091b9abcea3186341ddf8995d62bde62d7469e\")\n",
    "wandb.init(project=\"PBL-2\", name=\"resnet18_cutmix,flip)\")  # CutMix 적용 실험임을 명시\n",
    "\n",
    "# WandB 설정\n",
    "config = {\n",
    "    \"model\": \"resnet18\",\n",
    "    \"batch_size\": 128,\n",
    "    \"num_epochs\": 100,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"optimizer\": \"Adam\",\n",
    "    \"seed\": 2025,\n",
    "    \"deterministic\": False,\n",
    "    \"patience\": 10,  # early stopping patience\n",
    "    \"train_ratio\": 0.8,\n",
    "    \"val_ratio\": 0.1,\n",
    "    \"test_ratio\": 0.1,\n",
    "    \"cutmix_alpha\": 1.0,  # CutMix 알파 파라미터 추가\n",
    "    \"cutmix_prob\": 0.5    # CutMix 적용 확률 추가\n",
    "}\n",
    "wandb.config.update(config)\n",
    "\n",
    "# CIFAR-100 데이터셋 로드\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)),\n",
    "])\n",
    "\n",
    "full_trainset = torchvision.datasets.CIFAR100(\n",
    "    root='./data', train=True, download=True, transform=transform_train)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR100(\n",
    "    root='./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "# Stratified 분할을 위한 준비 (train, validation 나누기)\n",
    "# 모든 라벨을 추출\n",
    "targets = np.array(full_trainset.targets)\n",
    "\n",
    "# StratifiedShuffleSplit을 사용하여 8:1:1 비율로 분할\n",
    "# 먼저 train과 validation을 나눔 (full_trainset에서 8:2)\n",
    "train_val_split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=config[\"seed\"])\n",
    "train_idx, temp_idx = next(train_val_split.split(np.zeros(len(targets)), targets))\n",
    "\n",
    "# 그 다음 validation과 test를 나눔 (temp에서 1:1, 전체로 보면 1:1)\n",
    "val_test_targets = targets[temp_idx]\n",
    "val_test_split = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=config[\"seed\"])\n",
    "val_idx_temp, test_idx_temp = next(val_test_split.split(np.zeros(len(val_test_targets)), val_test_targets))\n",
    "\n",
    "# 원래 인덱스로 매핑\n",
    "val_idx = temp_idx[val_idx_temp]\n",
    "test_idx = temp_idx[test_idx_temp]\n",
    "\n",
    "# Subset 생성\n",
    "trainset = Subset(full_trainset, train_idx)\n",
    "valset = Subset(full_trainset, val_idx)\n",
    "testset_split = Subset(full_trainset, test_idx)  # 원래 테스트셋 대신 stratified split에서 나온 테스트셋 사용\n",
    "\n",
    "# DataLoader 생성\n",
    "trainloader = DataLoader(trainset, batch_size=config[\"batch_size\"], shuffle=True, num_workers=16)\n",
    "valloader = DataLoader(valset, batch_size=config[\"batch_size\"], shuffle=False, num_workers=16)\n",
    "testloader = DataLoader(testset_split, batch_size=config[\"batch_size\"], shuffle=False, num_workers=16)\n",
    "\n",
    "print(f\"Train set size: {len(trainset)}\")\n",
    "print(f\"Validation set size: {len(valset)}\")\n",
    "print(f\"Test set size: {len(testset_split)}\")\n",
    "\n",
    "# 추가: CutMix 변환 정의\n",
    "cutmix = transforms_v2.CutMix(alpha=config[\"cutmix_alpha\"], num_classes=100)  # CIFAR-100은 100개 클래스\n",
    "\n",
    "# CutMix용 손실 함수 정의 (원-핫 인코딩된 레이블 처리)\n",
    "def cutmix_criterion(outputs, targets):\n",
    "    \"\"\"\n",
    "    CutMix로 혼합된 레이블을 처리하기 위한 손실 함수\n",
    "    outputs: 모델 출력\n",
    "    targets: CutMix로 생성된 원-핫 인코딩 레이블\n",
    "    \"\"\"\n",
    "    return torch.nn.functional.cross_entropy(outputs, targets)\n",
    "\n",
    "def train(model, trainloader, criterion, optimizer, device, epoch):\n",
    "    \"\"\"\n",
    "    학습 함수 (CutMix 적용)\n",
    "    \"\"\"\n",
    "    model.train()   # 모델을 학습 모드로 설정\n",
    "    start_time = time.time()  # 시간 측정 시작\n",
    "    running_loss = 0.0\n",
    "    correct_top1 = 0\n",
    "    correct_top5 = 0\n",
    "    total = 0\n",
    "    \n",
    "    for i, (inputs, labels) in enumerate(trainloader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # CutMix 확률적 적용\n",
    "        if random.random() < config[\"cutmix_prob\"]:\n",
    "            inputs, labels = cutmix(inputs, labels)\n",
    "            # 이 경우 labels은 원-핫 인코딩 형태로 변환됨\n",
    "            use_cutmix = True\n",
    "        else:\n",
    "            use_cutmix = False\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # CutMix 적용 여부에 따라 손실 함수 선택\n",
    "        if use_cutmix:\n",
    "            # CutMix가 적용된 경우 (원-핫 인코딩된 레이블)\n",
    "            loss = torch.nn.functional.cross_entropy(outputs, labels)\n",
    "        else:\n",
    "            # 일반적인 경우 (정수 인덱스 레이블)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # 정확도 계산 - CutMix 적용 여부에 따라 다르게 처리\n",
    "        if use_cutmix:\n",
    "            # 원-핫 인코딩된 레이블에서 argmax를 사용해 가장 큰 값의 인덱스 추출\n",
    "            _, label_idx = labels.max(1)\n",
    "        else:\n",
    "            # 정수 인덱스 레이블 그대로 사용\n",
    "            label_idx = labels\n",
    "            \n",
    "        # top-1 정확도 계산\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += inputs.size(0)\n",
    "        correct_top1 += predicted.eq(label_idx).sum().item()\n",
    "        \n",
    "        # top-5 정확도 계산\n",
    "        _, top5_idx = outputs.topk(5, 1, largest=True, sorted=True)\n",
    "        correct_top5 += sum([1 for i in range(len(label_idx)) if label_idx[i] in top5_idx[i]])\n",
    "        \n",
    "        if (i + 1) % 50 == 0:  # 50 배치마다 출력\n",
    "            print(f'Epoch [{epoch+1}], Batch [{i+1}/{len(trainloader)}], Loss: {loss.item():.4f}')\n",
    "    \n",
    "    epoch_loss = running_loss / len(trainloader)\n",
    "    accuracy_top1 = 100.0 * correct_top1 / total\n",
    "    accuracy_top5 = 100.0 * correct_top5 / total\n",
    "    \n",
    "    train_time = time.time() - start_time\n",
    "    \n",
    "    # 학습 세트에 대한 성능 출력\n",
    "    print(f'Train set: Epoch: {epoch+1}, Average loss:{epoch_loss:.4f}, LR: {optimizer.param_groups[0][\"lr\"]:.6f} '\n",
    "          f'Top-1 Accuracy: {accuracy_top1:.4f}%, Top-5 Accuracy: {accuracy_top5:.4f}%, Time consumed:{train_time:.2f}s')\n",
    "    \n",
    "    return epoch_loss, accuracy_top1, accuracy_top5\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device, epoch, phase=\"val\"):\n",
    "    \"\"\"\n",
    "    평가 함수\n",
    "    \"\"\"\n",
    "    model.eval()  # 모델을 평가 모드로 설정\n",
    "    start_time = time.time()  # 시간 측정 시작\n",
    "    \n",
    "    eval_loss = 0.0\n",
    "    correct_top1 = 0\n",
    "    correct_top5 = 0\n",
    "    total = 0\n",
    "    \n",
    "    # 그래디언트 계산 비활성화\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # 순전파\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # 손실 계산\n",
    "            loss = criterion(outputs, labels)\n",
    "            eval_loss += loss.item()\n",
    "            \n",
    "            # top-1 정확도 계산\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct_top1 += (predicted == labels).sum().item()\n",
    "            \n",
    "            # top-5 정확도 계산\n",
    "            _, top5_idx = outputs.topk(5, 1, largest=True, sorted=True)\n",
    "            correct_top5 += top5_idx.eq(labels.view(-1, 1).expand_as(top5_idx)).sum().item()\n",
    "    \n",
    "    # 평균 손실 및 정확도 계산\n",
    "    eval_loss = eval_loss / len(dataloader)\n",
    "    accuracy_top1 = 100.0 * correct_top1 / total\n",
    "    accuracy_top5 = 100.0 * correct_top5 / total\n",
    "    \n",
    "    # 평가 시간 계산\n",
    "    eval_time = time.time() - start_time\n",
    "    \n",
    "    # 테스트 세트에 대한 성능 출력\n",
    "    print(f'{phase.capitalize()} set: Epoch: {epoch+1}, Average loss:{eval_loss:.4f}, '\n",
    "          f'Top-1 Accuracy: {accuracy_top1:.4f}%, Top-5 Accuracy: {accuracy_top5:.4f}%, Time consumed:{eval_time:.2f}s')\n",
    "    print()\n",
    "    \n",
    "    return eval_loss, accuracy_top1, accuracy_top5\n",
    "\n",
    "\n",
    "# 메인 학습 루프\n",
    "def main_training_loop(model, trainloader, valloader, testloader, criterion, optimizer, device, num_epochs, patience):\n",
    "    \"\"\"\n",
    "    메인 학습 루프\n",
    "    \"\"\"\n",
    "    early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
    "    \n",
    "    best_acc_top1 = 0.0\n",
    "    best_acc_top5 = 0.0\n",
    "    \n",
    "    # tqdm을 사용한 진행 상황 표시\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        # 학습\n",
    "        train_loss, train_acc_top1, train_acc_top5 = train(model, trainloader, criterion, optimizer, device, epoch)\n",
    "        \n",
    "        # 평가\n",
    "        val_loss, val_acc_top1, val_acc_top5 = evaluate(model, valloader, criterion, device, epoch, phase=\"val\")\n",
    "        \n",
    "        # WandB에 로깅\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"learning_rate\": optimizer.param_groups[0]['lr'],\n",
    "            \"train_loss\": train_loss,\n",
    "            \"train_accuracy_top1\": train_acc_top1,\n",
    "            \"train_accuracy_top5\": train_acc_top5,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"val_accuracy_top1\": val_acc_top1,\n",
    "            \"val_accuracy_top5\": val_acc_top5\n",
    "        })\n",
    "            \n",
    "        # 최고 정확도 모델 저장 (top-1 기준)\n",
    "        if val_acc_top1 > best_acc_top1:\n",
    "            best_acc_top1 = val_acc_top1\n",
    "            best_acc_top5_at_best_top1 = val_acc_top5\n",
    "            print(f'New best top-1 accuracy: {best_acc_top1:.2f}%, top-5 accuracy: {best_acc_top5_at_best_top1:.2f}%')\n",
    "            # 모델 저장\n",
    "            model_path = f'best_model_{wandb.run.name}.pth'\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            \n",
    "            # WandB에 모델 아티팩트 저장\n",
    "            wandb.save(model_path)\n",
    "        \n",
    "        # top-5 accuracy 기록 업데이트\n",
    "        if val_acc_top5 > best_acc_top5:\n",
    "            best_acc_top5 = val_acc_top5\n",
    "            print(f'New best top-5 accuracy: {best_acc_top5:.2f}%')\n",
    "\n",
    "        # Early stopping 체크 (validation loss 기준)\n",
    "        early_stopping(val_loss, model)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping triggered. Training stopped.\")\n",
    "            break\n",
    "    \n",
    "    # 훈련 완료 후 모델 평가 (best model 로드)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Loading best model from early stopping checkpoint...\")\n",
    "    else:\n",
    "        print(\"Loading best model based on validation accuracy...\")\n",
    "        model_path = f'best_model_{wandb.run.name}.pth'\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "    # 최종 테스트 세트 평가\n",
    "    test_loss, test_acc_top1, test_acc_top5 = evaluate(model, testloader, criterion, device, num_epochs-1, phase=\"test\")\n",
    "    \n",
    "    # 테스트 결과를 wandb 로그에 추가 - 이 부분이 누락되어 있어서 추가했습니다\n",
    "    wandb.log({\n",
    "        \"epoch\": epoch + 1,  # 마지막 에폭 또는 early stopping된 에폭\n",
    "        \"test_loss\": test_loss,\n",
    "        \"test_accuracy_top1\": test_acc_top1,\n",
    "        \"test_accuracy_top5\": test_acc_top5\n",
    "    })\n",
    "    \n",
    "    print(f'Finish! Best validation top-1 accuracy: {best_acc_top1:.2f}%, Best validation top-5 accuracy: {best_acc_top5:.2f}%')\n",
    "    print(f'Final test top-1 accuracy: {test_acc_top1:.2f}%, Final test top-5 accuracy: {test_acc_top5:.2f}%')\n",
    "    \n",
    "    # WandB에 최종 결과 기록\n",
    "    wandb.run.summary[\"best_val_accuracy_top1\"] = best_acc_top1\n",
    "    wandb.run.summary[\"best_val_accuracy_top5\"] = best_acc_top5\n",
    "    wandb.run.summary[\"test_accuracy_top1\"] = test_acc_top1\n",
    "    wandb.run.summary[\"test_accuracy_top5\"] = test_acc_top5\n",
    "\n",
    "    # Early stopping 정보 저장\n",
    "    if early_stopping.early_stop:\n",
    "        wandb.run.summary[\"early_stopped\"] = True\n",
    "        wandb.run.summary[\"early_stopped_epoch\"] = epoch+1\n",
    "    else:\n",
    "        wandb.run.summary[\"early_stopped\"] = False\n",
    "\n",
    "\n",
    "# 디바이스 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 모델 초기화\n",
    "model = resnet18().to(device)  \n",
    "criterion = nn.CrossEntropyLoss()  # 손실 함수 정의\n",
    "optimizer = optim.Adam(model.parameters(), lr=config[\"learning_rate\"])  # 옵티마이저 정의\n",
    "\n",
    "# WandB에 모델 구조 기록\n",
    "wandb.watch(model, log=\"all\")\n",
    "\n",
    "# GPU 가속\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"{torch.cuda.device_count()}개의 GPU를 사용합니다.\")\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "# 훈련 시작 시간 기록\n",
    "start_time = time.time()\n",
    "\n",
    "# 메인 학습 루프 호출\n",
    "main_training_loop(\n",
    "    model=model,\n",
    "    trainloader=trainloader,\n",
    "    valloader=valloader,\n",
    "    testloader=testloader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    num_epochs=config[\"num_epochs\"],\n",
    "    patience=config[\"patience\"]\n",
    ")\n",
    "\n",
    "# 훈련 종료 시간 기록 및 출력\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "wandb.log({\"total_training_time\": total_time})\n",
    "\n",
    "print(f\"Total training time: {total_time:.2f} seconds\")\n",
    "\n",
    "# WandB 실행 종료\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57b9218-aa24-4297-bb92-473c215e6767",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
