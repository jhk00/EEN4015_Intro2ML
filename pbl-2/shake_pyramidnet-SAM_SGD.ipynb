{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516f9d5f-6361-4b03-98ee-e1ba4de86b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/guswls/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msokjh1310\u001b[0m (\u001b[33msokjh1310-hanyang-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/guswls/EEN4015_Intro2ML/pbl-2/wandb/run-20250419_161225-ek0s39sm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sokjh1310-hanyang-university/PBL-2/runs/ek0s39sm' target=\"_blank\">shake_pyramidnet-cfc,warmup,reduce</a></strong> to <a href='https://wandb.ai/sokjh1310-hanyang-university/PBL-2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sokjh1310-hanyang-university/PBL-2' target=\"_blank\">https://wandb.ai/sokjh1310-hanyang-university/PBL-2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sokjh1310-hanyang-university/PBL-2/runs/ek0s39sm' target=\"_blank\">https://wandb.ai/sokjh1310-hanyang-university/PBL-2/runs/ek0s39sm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Train set size: 50000\n",
      "Test set size: 10000\n",
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2개의 GPU를 사용합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                       | 0/300 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1], Batch [50/782], Loss: 4.6387, LR: 0.001279\n",
      "Epoch [1], Batch [100/782], Loss: 4.5701, LR: 0.002558\n",
      "Epoch [1], Batch [150/782], Loss: 4.5414, LR: 0.003836\n",
      "Epoch [1], Batch [200/782], Loss: 4.5625, LR: 0.005115\n",
      "Epoch [1], Batch [250/782], Loss: 4.5445, LR: 0.006394\n",
      "Epoch [1], Batch [300/782], Loss: 4.5266, LR: 0.007673\n",
      "Epoch [1], Batch [350/782], Loss: 4.3743, LR: 0.008951\n",
      "Epoch [1], Batch [400/782], Loss: 4.3987, LR: 0.010230\n",
      "Epoch [1], Batch [450/782], Loss: 4.5919, LR: 0.011509\n",
      "Epoch [1], Batch [500/782], Loss: 4.4734, LR: 0.012788\n",
      "Epoch [1], Batch [550/782], Loss: 3.9687, LR: 0.014066\n",
      "Epoch [1], Batch [600/782], Loss: 4.3584, LR: 0.015345\n",
      "Epoch [1], Batch [650/782], Loss: 3.9962, LR: 0.016624\n",
      "Epoch [1], Batch [700/782], Loss: 4.0818, LR: 0.017903\n",
      "Epoch [1], Batch [750/782], Loss: 4.0426, LR: 0.019182\n",
      "Train set: Epoch: 1, Average loss:4.3842, LR: 0.020000 Top-1 Accuracy: 3.3100%, Top-5 Accuracy: 13.6380%, Time consumed:673.04s\n",
      "Test set: Epoch: 1, Average loss:3.9858, Top-1 Accuracy: 8.0800%, Top-5 Accuracy: 29.0500%, Time consumed:29.34s\n",
      "\n",
      "새로운 최고 top-1 정확도: 8.08%, top-5 정확도: 29.05%\n",
      "새로운 최고 top-5 정확도: 29.05%\n",
      "Accuracy improved (-inf% --> 8.08%). Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▎                                                                                          | 1/300 [11:42<58:23:06, 702.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2], Batch [50/782], Loss: 4.4007, LR: 0.021279\n",
      "Epoch [2], Batch [100/782], Loss: 4.0774, LR: 0.022558\n",
      "Epoch [2], Batch [150/782], Loss: 4.1010, LR: 0.023836\n",
      "Epoch [2], Batch [200/782], Loss: 4.4947, LR: 0.025115\n",
      "Epoch [2], Batch [250/782], Loss: 4.2292, LR: 0.026394\n",
      "Epoch [2], Batch [300/782], Loss: 3.5357, LR: 0.027673\n",
      "Epoch [2], Batch [350/782], Loss: 4.2982, LR: 0.028951\n",
      "Epoch [2], Batch [400/782], Loss: 3.9835, LR: 0.030230\n",
      "Epoch [2], Batch [450/782], Loss: 3.6899, LR: 0.031509\n",
      "Epoch [2], Batch [500/782], Loss: 4.3082, LR: 0.032788\n",
      "Epoch [2], Batch [550/782], Loss: 3.6770, LR: 0.034066\n",
      "Epoch [2], Batch [600/782], Loss: 3.9636, LR: 0.035345\n",
      "Epoch [2], Batch [650/782], Loss: 3.7888, LR: 0.036624\n",
      "Epoch [2], Batch [700/782], Loss: 4.3958, LR: 0.037903\n",
      "Epoch [2], Batch [750/782], Loss: 4.4776, LR: 0.039182\n",
      "Train set: Epoch: 2, Average loss:4.0234, LR: 0.040000 Top-1 Accuracy: 7.8020%, Top-5 Accuracy: 26.3280%, Time consumed:670.77s\n",
      "Test set: Epoch: 2, Average loss:3.8495, Top-1 Accuracy: 10.8900%, Top-5 Accuracy: 31.5200%, Time consumed:29.10s\n",
      "\n",
      "새로운 최고 top-1 정확도: 10.89%, top-5 정확도: 31.52%\n",
      "새로운 최고 top-5 정확도: 31.52%\n",
      "Accuracy improved (8.08% --> 10.89%). Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▌                                                                                          | 2/300 [23:23<58:04:20, 701.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3], Batch [50/782], Loss: 4.1384, LR: 0.041279\n",
      "Epoch [3], Batch [100/782], Loss: 3.6152, LR: 0.042558\n",
      "Epoch [3], Batch [150/782], Loss: 3.6984, LR: 0.043836\n",
      "Epoch [3], Batch [200/782], Loss: 3.6979, LR: 0.045115\n",
      "Epoch [3], Batch [250/782], Loss: 4.1972, LR: 0.046394\n",
      "Epoch [3], Batch [300/782], Loss: 3.6397, LR: 0.047673\n",
      "Epoch [3], Batch [350/782], Loss: 3.5926, LR: 0.048951\n",
      "Epoch [3], Batch [400/782], Loss: 3.4531, LR: 0.050230\n",
      "Epoch [3], Batch [450/782], Loss: 3.4127, LR: 0.051509\n",
      "Epoch [3], Batch [500/782], Loss: 3.4276, LR: 0.052788\n",
      "Epoch [3], Batch [550/782], Loss: 3.5080, LR: 0.054066\n",
      "Epoch [3], Batch [600/782], Loss: 3.6314, LR: 0.055345\n",
      "Epoch [3], Batch [650/782], Loss: 4.1156, LR: 0.056624\n",
      "Epoch [3], Batch [700/782], Loss: 4.0172, LR: 0.057903\n",
      "Epoch [3], Batch [750/782], Loss: 3.3618, LR: 0.059182\n",
      "Train set: Epoch: 3, Average loss:3.7941, LR: 0.060000 Top-1 Accuracy: 11.6380%, Top-5 Accuracy: 34.4460%, Time consumed:670.64s\n",
      "Test set: Epoch: 3, Average loss:3.4469, Top-1 Accuracy: 16.9400%, Top-5 Accuracy: 44.0500%, Time consumed:28.78s\n",
      "\n",
      "새로운 최고 top-1 정확도: 16.94%, top-5 정확도: 44.05%\n",
      "새로운 최고 top-5 정확도: 44.05%\n",
      "Accuracy improved (10.89% --> 16.94%). Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▉                                                                                          | 3/300 [35:03<57:49:20, 700.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4], Batch [50/782], Loss: 3.3534, LR: 0.061279\n",
      "Epoch [4], Batch [100/782], Loss: 4.0736, LR: 0.062558\n",
      "Epoch [4], Batch [150/782], Loss: 3.2853, LR: 0.063836\n",
      "Epoch [4], Batch [200/782], Loss: 3.3548, LR: 0.065115\n",
      "Epoch [4], Batch [250/782], Loss: 3.6158, LR: 0.066394\n",
      "Epoch [4], Batch [300/782], Loss: 3.2597, LR: 0.067673\n",
      "Epoch [4], Batch [350/782], Loss: 3.5159, LR: 0.068951\n",
      "Epoch [4], Batch [400/782], Loss: 3.4008, LR: 0.070230\n",
      "Epoch [4], Batch [450/782], Loss: 3.0649, LR: 0.071509\n",
      "Epoch [4], Batch [500/782], Loss: 3.0161, LR: 0.072788\n",
      "Epoch [4], Batch [550/782], Loss: 3.8894, LR: 0.074066\n",
      "Epoch [4], Batch [600/782], Loss: 4.2969, LR: 0.075345\n",
      "Epoch [4], Batch [650/782], Loss: 3.1230, LR: 0.076624\n",
      "Epoch [4], Batch [700/782], Loss: 2.9925, LR: 0.077903\n",
      "Epoch [4], Batch [750/782], Loss: 2.9245, LR: 0.079182\n",
      "Train set: Epoch: 4, Average loss:3.5829, LR: 0.080000 Top-1 Accuracy: 15.8140%, Top-5 Accuracy: 41.9600%, Time consumed:672.64s\n",
      "Test set: Epoch: 4, Average loss:3.2039, Top-1 Accuracy: 20.4500%, Top-5 Accuracy: 49.9500%, Time consumed:28.94s\n",
      "\n",
      "새로운 최고 top-1 정확도: 20.45%, top-5 정확도: 49.95%\n",
      "새로운 최고 top-5 정확도: 49.95%\n",
      "Accuracy improved (16.94% --> 20.45%). Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|█▏                                                                                         | 4/300 [46:45<57:40:20, 701.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5], Batch [50/782], Loss: 3.5378, LR: 0.081279\n",
      "Epoch [5], Batch [100/782], Loss: 3.1728, LR: 0.082558\n",
      "Epoch [5], Batch [150/782], Loss: 2.9619, LR: 0.083836\n",
      "Epoch [5], Batch [200/782], Loss: 3.9375, LR: 0.085115\n",
      "Epoch [5], Batch [250/782], Loss: 3.9124, LR: 0.086394\n",
      "Epoch [5], Batch [300/782], Loss: 3.0508, LR: 0.087673\n",
      "Epoch [5], Batch [350/782], Loss: 3.2987, LR: 0.088951\n",
      "Epoch [5], Batch [400/782], Loss: 4.1268, LR: 0.090230\n",
      "Epoch [5], Batch [450/782], Loss: 3.9453, LR: 0.091509\n",
      "Epoch [5], Batch [500/782], Loss: 3.5523, LR: 0.092788\n",
      "Epoch [5], Batch [550/782], Loss: 2.9125, LR: 0.094066\n",
      "Epoch [5], Batch [600/782], Loss: 4.0310, LR: 0.095345\n",
      "Epoch [5], Batch [650/782], Loss: 3.9290, LR: 0.096624\n",
      "Epoch [5], Batch [700/782], Loss: 3.0032, LR: 0.097903\n",
      "Epoch [5], Batch [750/782], Loss: 2.7563, LR: 0.099182\n",
      "Train set: Epoch: 5, Average loss:3.4435, LR: 0.100000 Top-1 Accuracy: 18.7820%, Top-5 Accuracy: 46.7060%, Time consumed:670.28s\n",
      "Test set: Epoch: 5, Average loss:2.8888, Top-1 Accuracy: 26.2100%, Top-5 Accuracy: 58.4400%, Time consumed:30.35s\n",
      "\n",
      "새로운 최고 top-1 정확도: 26.21%, top-5 정확도: 58.44%\n",
      "새로운 최고 top-5 정확도: 58.44%\n",
      "Accuracy improved (20.45% --> 26.21%). Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|█▌                                                                                         | 5/300 [58:27<57:28:24, 701.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6], Batch [50/782], Loss: 3.5350, LR: 0.100000\n",
      "Epoch [6], Batch [100/782], Loss: 3.6219, LR: 0.100000\n",
      "Epoch [6], Batch [150/782], Loss: 2.8878, LR: 0.100000\n",
      "Epoch [6], Batch [200/782], Loss: 2.7642, LR: 0.100000\n",
      "Epoch [6], Batch [250/782], Loss: 2.9983, LR: 0.100000\n",
      "Epoch [6], Batch [300/782], Loss: 2.9956, LR: 0.100000\n",
      "Epoch [6], Batch [350/782], Loss: 2.6945, LR: 0.100000\n",
      "Epoch [6], Batch [400/782], Loss: 2.7238, LR: 0.100000\n",
      "Epoch [6], Batch [450/782], Loss: 3.7881, LR: 0.100000\n",
      "Epoch [6], Batch [500/782], Loss: 2.8341, LR: 0.100000\n",
      "Epoch [6], Batch [550/782], Loss: 2.4931, LR: 0.100000\n",
      "Epoch [6], Batch [600/782], Loss: 2.9995, LR: 0.100000\n",
      "Epoch [6], Batch [650/782], Loss: 2.7435, LR: 0.100000\n",
      "Epoch [6], Batch [700/782], Loss: 2.5449, LR: 0.100000\n",
      "Epoch [6], Batch [750/782], Loss: 2.9203, LR: 0.100000\n",
      "Train set: Epoch: 6, Average loss:3.3077, LR: 0.100000 Top-1 Accuracy: 21.5540%, Top-5 Accuracy: 50.6860%, Time consumed:669.07s\n",
      "Test set: Epoch: 6, Average loss:2.8106, Top-1 Accuracy: 28.1000%, Top-5 Accuracy: 60.4500%, Time consumed:29.28s\n",
      "\n",
      "새로운 최고 top-1 정확도: 28.10%, top-5 정확도: 60.45%\n",
      "새로운 최고 top-5 정확도: 60.45%\n",
      "Accuracy improved (26.21% --> 28.10%). Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|█▊                                                                                       | 6/300 [1:10:06<57:12:47, 700.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7], Batch [50/782], Loss: 4.0532, LR: 0.100000\n",
      "Epoch [7], Batch [100/782], Loss: 3.8639, LR: 0.100000\n",
      "Epoch [7], Batch [150/782], Loss: 3.3306, LR: 0.100000\n",
      "Epoch [7], Batch [200/782], Loss: 4.0715, LR: 0.100000\n",
      "Epoch [7], Batch [250/782], Loss: 2.8130, LR: 0.100000\n",
      "Epoch [7], Batch [300/782], Loss: 2.7880, LR: 0.100000\n",
      "Epoch [7], Batch [350/782], Loss: 2.6782, LR: 0.100000\n",
      "Epoch [7], Batch [400/782], Loss: 2.9217, LR: 0.100000\n",
      "Epoch [7], Batch [450/782], Loss: 3.8349, LR: 0.100000\n",
      "Epoch [7], Batch [500/782], Loss: 2.4763, LR: 0.100000\n",
      "Epoch [7], Batch [550/782], Loss: 3.0629, LR: 0.100000\n",
      "Epoch [7], Batch [600/782], Loss: 2.5462, LR: 0.100000\n",
      "Epoch [7], Batch [650/782], Loss: 2.5600, LR: 0.100000\n",
      "Epoch [7], Batch [700/782], Loss: 2.9930, LR: 0.100000\n",
      "Epoch [7], Batch [750/782], Loss: 2.8057, LR: 0.100000\n",
      "Train set: Epoch: 7, Average loss:3.1501, LR: 0.100000 Top-1 Accuracy: 24.4420%, Top-5 Accuracy: 54.5800%, Time consumed:668.39s\n",
      "Test set: Epoch: 7, Average loss:2.5597, Top-1 Accuracy: 33.1200%, Top-5 Accuracy: 67.0300%, Time consumed:30.86s\n",
      "\n",
      "새로운 최고 top-1 정확도: 33.12%, top-5 정확도: 67.03%\n",
      "새로운 최고 top-5 정확도: 67.03%\n",
      "Accuracy improved (28.10% --> 33.12%). Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|██                                                                                       | 7/300 [1:21:46<57:00:04, 700.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8], Batch [50/782], Loss: 3.1802, LR: 0.100000\n",
      "Epoch [8], Batch [100/782], Loss: 2.6371, LR: 0.100000\n",
      "Epoch [8], Batch [150/782], Loss: 2.5908, LR: 0.100000\n",
      "Epoch [8], Batch [200/782], Loss: 3.7650, LR: 0.100000\n",
      "Epoch [8], Batch [250/782], Loss: 3.4702, LR: 0.100000\n",
      "Epoch [8], Batch [300/782], Loss: 2.4178, LR: 0.100000\n",
      "Epoch [8], Batch [350/782], Loss: 2.1766, LR: 0.100000\n",
      "Epoch [8], Batch [400/782], Loss: 3.5536, LR: 0.100000\n",
      "Epoch [8], Batch [450/782], Loss: 3.4178, LR: 0.100000\n",
      "Epoch [8], Batch [500/782], Loss: 3.9175, LR: 0.100000\n",
      "Epoch [8], Batch [550/782], Loss: 4.0122, LR: 0.100000\n",
      "Epoch [8], Batch [600/782], Loss: 2.8805, LR: 0.100000\n",
      "Epoch [8], Batch [650/782], Loss: 3.6747, LR: 0.100000\n",
      "Epoch [8], Batch [700/782], Loss: 3.5267, LR: 0.100000\n",
      "Epoch [8], Batch [750/782], Loss: 2.4570, LR: 0.100000\n",
      "Train set: Epoch: 8, Average loss:3.0593, LR: 0.100000 Top-1 Accuracy: 27.1980%, Top-5 Accuracy: 57.9980%, Time consumed:669.21s\n",
      "Test set: Epoch: 8, Average loss:2.4242, Top-1 Accuracy: 36.6200%, Top-5 Accuracy: 69.2200%, Time consumed:29.00s\n",
      "\n",
      "새로운 최고 top-1 정확도: 36.62%, top-5 정확도: 69.22%\n",
      "새로운 최고 top-5 정확도: 69.22%\n",
      "Accuracy improved (33.12% --> 36.62%). Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|██▎                                                                                      | 8/300 [1:33:24<56:46:05, 699.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9], Batch [50/782], Loss: 2.5281, LR: 0.100000\n",
      "Epoch [9], Batch [100/782], Loss: 3.9118, LR: 0.100000\n",
      "Epoch [9], Batch [150/782], Loss: 2.6170, LR: 0.100000\n",
      "Epoch [9], Batch [200/782], Loss: 3.4943, LR: 0.100000\n",
      "Epoch [9], Batch [250/782], Loss: 3.8639, LR: 0.100000\n",
      "Epoch [9], Batch [300/782], Loss: 2.4336, LR: 0.100000\n",
      "Epoch [9], Batch [350/782], Loss: 2.6580, LR: 0.100000\n",
      "Epoch [9], Batch [400/782], Loss: 3.7066, LR: 0.100000\n",
      "Epoch [9], Batch [450/782], Loss: 3.1316, LR: 0.100000\n",
      "Epoch [9], Batch [500/782], Loss: 3.4681, LR: 0.100000\n",
      "Epoch [9], Batch [550/782], Loss: 2.7212, LR: 0.100000\n",
      "Epoch [9], Batch [600/782], Loss: 2.3149, LR: 0.100000\n",
      "Epoch [9], Batch [650/782], Loss: 2.0616, LR: 0.100000\n",
      "Epoch [9], Batch [700/782], Loss: 3.2868, LR: 0.100000\n",
      "Epoch [9], Batch [750/782], Loss: 2.5840, LR: 0.100000\n",
      "Train set: Epoch: 9, Average loss:2.9516, LR: 0.100000 Top-1 Accuracy: 29.4720%, Top-5 Accuracy: 60.8420%, Time consumed:669.03s\n",
      "Test set: Epoch: 9, Average loss:2.3288, Top-1 Accuracy: 38.5700%, Top-5 Accuracy: 70.9300%, Time consumed:29.19s\n",
      "\n",
      "새로운 최고 top-1 정확도: 38.57%, top-5 정확도: 70.93%\n",
      "새로운 최고 top-5 정확도: 70.93%\n",
      "Accuracy improved (36.62% --> 38.57%). Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|██▋                                                                                      | 9/300 [1:45:03<56:32:56, 699.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10], Batch [50/782], Loss: 3.6337, LR: 0.100000\n",
      "Epoch [10], Batch [100/782], Loss: 2.5644, LR: 0.100000\n",
      "Epoch [10], Batch [150/782], Loss: 3.7421, LR: 0.100000\n",
      "Epoch [10], Batch [200/782], Loss: 2.4099, LR: 0.100000\n",
      "Epoch [10], Batch [250/782], Loss: 3.4967, LR: 0.100000\n",
      "Epoch [10], Batch [300/782], Loss: 3.5378, LR: 0.100000\n",
      "Epoch [10], Batch [350/782], Loss: 3.4152, LR: 0.100000\n",
      "Epoch [10], Batch [400/782], Loss: 2.7844, LR: 0.100000\n",
      "Epoch [10], Batch [450/782], Loss: 3.6451, LR: 0.100000\n",
      "Epoch [10], Batch [500/782], Loss: 2.3698, LR: 0.100000\n",
      "Epoch [10], Batch [550/782], Loss: 4.0885, LR: 0.100000\n",
      "Epoch [10], Batch [600/782], Loss: 3.7169, LR: 0.100000\n",
      "Epoch [10], Batch [650/782], Loss: 3.8808, LR: 0.100000\n",
      "Epoch [10], Batch [700/782], Loss: 2.2837, LR: 0.100000\n",
      "Epoch [10], Batch [750/782], Loss: 3.3174, LR: 0.100000\n",
      "Train set: Epoch: 10, Average loss:2.9050, LR: 0.100000 Top-1 Accuracy: 30.6320%, Top-5 Accuracy: 61.8900%, Time consumed:668.20s\n",
      "Test set: Epoch: 10, Average loss:2.1980, Top-1 Accuracy: 40.4500%, Top-5 Accuracy: 75.2300%, Time consumed:29.12s\n",
      "\n",
      "새로운 최고 top-1 정확도: 40.45%, top-5 정확도: 75.23%\n",
      "새로운 최고 top-5 정확도: 75.23%\n",
      "Accuracy improved (38.57% --> 40.45%). Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|██▉                                                                                     | 10/300 [1:56:41<56:18:51, 699.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11], Batch [50/782], Loss: 2.5635, LR: 0.100000\n",
      "Epoch [11], Batch [100/782], Loss: 3.7336, LR: 0.100000\n",
      "Epoch [11], Batch [150/782], Loss: 2.9212, LR: 0.100000\n",
      "Epoch [11], Batch [200/782], Loss: 2.4629, LR: 0.100000\n",
      "Epoch [11], Batch [250/782], Loss: 2.4049, LR: 0.100000\n",
      "Epoch [11], Batch [300/782], Loss: 3.6136, LR: 0.100000\n",
      "Epoch [11], Batch [350/782], Loss: 1.9372, LR: 0.100000\n",
      "Epoch [11], Batch [400/782], Loss: 2.3364, LR: 0.100000\n",
      "Epoch [11], Batch [450/782], Loss: 1.8165, LR: 0.100000\n",
      "Epoch [11], Batch [500/782], Loss: 3.5682, LR: 0.100000\n",
      "Epoch [11], Batch [550/782], Loss: 3.8449, LR: 0.100000\n",
      "Epoch [11], Batch [600/782], Loss: 2.3217, LR: 0.100000\n",
      "Epoch [11], Batch [650/782], Loss: 2.2925, LR: 0.100000\n",
      "Epoch [11], Batch [700/782], Loss: 3.0229, LR: 0.100000\n",
      "Epoch [11], Batch [750/782], Loss: 2.1547, LR: 0.100000\n",
      "Train set: Epoch: 11, Average loss:2.8442, LR: 0.100000 Top-1 Accuracy: 32.3280%, Top-5 Accuracy: 63.5420%, Time consumed:670.83s\n",
      "Test set: Epoch: 11, Average loss:2.1324, Top-1 Accuracy: 42.3600%, Top-5 Accuracy: 75.3300%, Time consumed:28.65s\n",
      "\n",
      "새로운 최고 top-1 정확도: 42.36%, top-5 정확도: 75.33%\n",
      "새로운 최고 top-5 정확도: 75.33%\n",
      "Accuracy improved (40.45% --> 42.36%). Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|███▏                                                                                    | 11/300 [2:08:21<56:08:47, 699.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12], Batch [50/782], Loss: 2.3949, LR: 0.100000\n",
      "Epoch [12], Batch [100/782], Loss: 1.9208, LR: 0.100000\n",
      "Epoch [12], Batch [150/782], Loss: 3.5046, LR: 0.100000\n",
      "Epoch [12], Batch [200/782], Loss: 2.8274, LR: 0.100000\n",
      "Epoch [12], Batch [250/782], Loss: 3.3543, LR: 0.100000\n",
      "Epoch [12], Batch [300/782], Loss: 2.3796, LR: 0.100000\n",
      "Epoch [12], Batch [350/782], Loss: 2.0286, LR: 0.100000\n",
      "Epoch [12], Batch [400/782], Loss: 3.3316, LR: 0.100000\n",
      "Epoch [12], Batch [450/782], Loss: 2.2552, LR: 0.100000\n",
      "Epoch [12], Batch [500/782], Loss: 2.1818, LR: 0.100000\n",
      "Epoch [12], Batch [550/782], Loss: 3.7003, LR: 0.100000\n",
      "Epoch [12], Batch [600/782], Loss: 2.0201, LR: 0.100000\n",
      "Epoch [12], Batch [650/782], Loss: 2.0531, LR: 0.100000\n",
      "Epoch [12], Batch [700/782], Loss: 2.1756, LR: 0.100000\n",
      "Epoch [12], Batch [750/782], Loss: 2.2034, LR: 0.100000\n",
      "Train set: Epoch: 12, Average loss:2.8279, LR: 0.100000 Top-1 Accuracy: 33.0620%, Top-5 Accuracy: 64.0540%, Time consumed:666.70s\n",
      "Test set: Epoch: 12, Average loss:2.1052, Top-1 Accuracy: 43.5100%, Top-5 Accuracy: 75.3000%, Time consumed:30.42s\n",
      "\n",
      "새로운 최고 top-1 정확도: 43.51%, top-5 정확도: 75.30%\n",
      "Accuracy improved (42.36% --> 43.51%). Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|███▌                                                                                    | 12/300 [2:19:59<55:54:46, 698.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13], Batch [50/782], Loss: 2.2479, LR: 0.100000\n",
      "Epoch [13], Batch [100/782], Loss: 2.0746, LR: 0.100000\n",
      "Epoch [13], Batch [150/782], Loss: 2.2273, LR: 0.100000\n",
      "Epoch [13], Batch [200/782], Loss: 3.1833, LR: 0.100000\n",
      "Epoch [13], Batch [250/782], Loss: 3.3387, LR: 0.100000\n",
      "Epoch [13], Batch [300/782], Loss: 2.3324, LR: 0.100000\n",
      "Epoch [13], Batch [350/782], Loss: 4.4172, LR: 0.100000\n",
      "Epoch [13], Batch [400/782], Loss: 2.5103, LR: 0.100000\n",
      "Epoch [13], Batch [450/782], Loss: 2.1583, LR: 0.100000\n",
      "Epoch [13], Batch [500/782], Loss: 3.7022, LR: 0.100000\n",
      "Epoch [13], Batch [550/782], Loss: 2.1421, LR: 0.100000\n",
      "Epoch [13], Batch [600/782], Loss: 3.7475, LR: 0.100000\n",
      "Epoch [13], Batch [650/782], Loss: 1.9050, LR: 0.100000\n",
      "Epoch [13], Batch [700/782], Loss: 1.7749, LR: 0.100000\n",
      "Epoch [13], Batch [750/782], Loss: 3.3688, LR: 0.100000\n",
      "Train set: Epoch: 13, Average loss:2.7527, LR: 0.100000 Top-1 Accuracy: 34.2440%, Top-5 Accuracy: 65.3720%, Time consumed:668.05s\n",
      "Test set: Epoch: 13, Average loss:2.0660, Top-1 Accuracy: 44.2100%, Top-5 Accuracy: 76.2100%, Time consumed:29.12s\n",
      "\n",
      "새로운 최고 top-1 정확도: 44.21%, top-5 정확도: 76.21%\n",
      "새로운 최고 top-5 정확도: 76.21%\n",
      "Accuracy improved (43.51% --> 44.21%). Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|███▊                                                                                    | 13/300 [2:31:37<55:41:33, 698.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14], Batch [50/782], Loss: 3.8341, LR: 0.100000\n",
      "Epoch [14], Batch [100/782], Loss: 2.6764, LR: 0.100000\n",
      "Epoch [14], Batch [150/782], Loss: 3.3810, LR: 0.100000\n",
      "Epoch [14], Batch [200/782], Loss: 3.1646, LR: 0.100000\n",
      "Epoch [14], Batch [250/782], Loss: 3.7600, LR: 0.100000\n",
      "Epoch [14], Batch [300/782], Loss: 2.6442, LR: 0.100000\n",
      "Epoch [14], Batch [350/782], Loss: 2.4215, LR: 0.100000\n",
      "Epoch [14], Batch [400/782], Loss: 2.0723, LR: 0.100000\n",
      "Epoch [14], Batch [450/782], Loss: 2.1014, LR: 0.100000\n",
      "Epoch [14], Batch [500/782], Loss: 2.0509, LR: 0.100000\n",
      "Epoch [14], Batch [550/782], Loss: 3.8713, LR: 0.100000\n",
      "Epoch [14], Batch [600/782], Loss: 2.4741, LR: 0.100000\n",
      "Epoch [14], Batch [650/782], Loss: 2.0331, LR: 0.100000\n",
      "Epoch [14], Batch [700/782], Loss: 2.2581, LR: 0.100000\n",
      "Epoch [14], Batch [750/782], Loss: 3.4364, LR: 0.100000\n",
      "Train set: Epoch: 14, Average loss:2.7344, LR: 0.100000 Top-1 Accuracy: 35.0820%, Top-5 Accuracy: 66.3160%, Time consumed:669.46s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|████                                                                                    | 14/300 [2:43:18<55:33:14, 699.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Epoch: 14, Average loss:2.1986, Top-1 Accuracy: 41.6400%, Top-5 Accuracy: 73.8700%, Time consumed:31.42s\n",
      "\n",
      "EarlyStopping 카운터: 1 / 30\n",
      "Epoch [15], Batch [50/782], Loss: 4.0621, LR: 0.100000\n",
      "Epoch [15], Batch [100/782], Loss: 3.3260, LR: 0.100000\n",
      "Epoch [15], Batch [150/782], Loss: 2.3350, LR: 0.100000\n",
      "Epoch [15], Batch [200/782], Loss: 2.1804, LR: 0.100000\n",
      "Epoch [15], Batch [250/782], Loss: 3.1059, LR: 0.100000\n",
      "Epoch [15], Batch [300/782], Loss: 3.3760, LR: 0.100000\n",
      "Epoch [15], Batch [350/782], Loss: 3.4934, LR: 0.100000\n",
      "Epoch [15], Batch [400/782], Loss: 3.7613, LR: 0.100000\n",
      "Epoch [15], Batch [450/782], Loss: 3.6614, LR: 0.100000\n",
      "Epoch [15], Batch [500/782], Loss: 2.1221, LR: 0.100000\n",
      "Epoch [15], Batch [550/782], Loss: 1.9142, LR: 0.100000\n",
      "Epoch [15], Batch [600/782], Loss: 2.1672, LR: 0.100000\n",
      "Epoch [15], Batch [650/782], Loss: 2.7553, LR: 0.100000\n",
      "Epoch [15], Batch [700/782], Loss: 3.6126, LR: 0.100000\n",
      "Epoch [15], Batch [750/782], Loss: 3.2849, LR: 0.100000\n",
      "Train set: Epoch: 15, Average loss:2.6691, LR: 0.100000 Top-1 Accuracy: 36.0860%, Top-5 Accuracy: 67.4660%, Time consumed:669.15s\n",
      "Test set: Epoch: 15, Average loss:2.0709, Top-1 Accuracy: 44.8700%, Top-5 Accuracy: 76.1900%, Time consumed:28.98s\n",
      "\n",
      "새로운 최고 top-1 정확도: 44.87%, top-5 정확도: 76.19%\n",
      "Accuracy improved (44.21% --> 44.87%). Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|████▍                                                                                   | 15/300 [2:54:57<55:20:54, 699.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16], Batch [50/782], Loss: 1.9240, LR: 0.100000\n",
      "Epoch [16], Batch [100/782], Loss: 2.3342, LR: 0.100000\n",
      "Epoch [16], Batch [150/782], Loss: 1.9424, LR: 0.100000\n",
      "Epoch [16], Batch [200/782], Loss: 3.8568, LR: 0.100000\n",
      "Epoch [16], Batch [250/782], Loss: 2.0644, LR: 0.100000\n",
      "Epoch [16], Batch [300/782], Loss: 1.7374, LR: 0.100000\n",
      "Epoch [16], Batch [350/782], Loss: 1.8930, LR: 0.100000\n",
      "Epoch [16], Batch [400/782], Loss: 3.1250, LR: 0.100000\n",
      "Epoch [16], Batch [450/782], Loss: 2.4174, LR: 0.100000\n",
      "Epoch [16], Batch [500/782], Loss: 2.1284, LR: 0.100000\n",
      "Epoch [16], Batch [550/782], Loss: 1.7052, LR: 0.100000\n",
      "Epoch [16], Batch [600/782], Loss: 2.1745, LR: 0.100000\n",
      "Epoch [16], Batch [650/782], Loss: 2.3135, LR: 0.100000\n",
      "Epoch [16], Batch [700/782], Loss: 2.9453, LR: 0.100000\n",
      "Epoch [16], Batch [750/782], Loss: 2.2427, LR: 0.100000\n",
      "Train set: Epoch: 16, Average loss:2.6342, LR: 0.100000 Top-1 Accuracy: 37.2880%, Top-5 Accuracy: 68.5620%, Time consumed:671.65s\n",
      "Test set: Epoch: 16, Average loss:1.9242, Top-1 Accuracy: 47.4200%, Top-5 Accuracy: 79.1200%, Time consumed:30.77s\n",
      "\n",
      "새로운 최고 top-1 정확도: 47.42%, top-5 정확도: 79.12%\n",
      "새로운 최고 top-5 정확도: 79.12%\n",
      "Accuracy improved (44.87% --> 47.42%). Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|████▋                                                                                   | 16/300 [3:06:40<55:14:52, 700.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17], Batch [50/782], Loss: 3.2785, LR: 0.100000\n",
      "Epoch [17], Batch [100/782], Loss: 2.1697, LR: 0.100000\n",
      "Epoch [17], Batch [150/782], Loss: 3.3910, LR: 0.100000\n",
      "Epoch [17], Batch [200/782], Loss: 2.1566, LR: 0.100000\n",
      "Epoch [17], Batch [250/782], Loss: 2.0086, LR: 0.100000\n",
      "Epoch [17], Batch [300/782], Loss: 3.4165, LR: 0.100000\n",
      "Epoch [17], Batch [350/782], Loss: 2.4012, LR: 0.100000\n",
      "Epoch [17], Batch [400/782], Loss: 3.4555, LR: 0.100000\n",
      "Epoch [17], Batch [450/782], Loss: 2.0436, LR: 0.100000\n",
      "Epoch [17], Batch [500/782], Loss: 3.7007, LR: 0.100000\n",
      "Epoch [17], Batch [550/782], Loss: 3.4354, LR: 0.100000\n",
      "Epoch [17], Batch [600/782], Loss: 2.0759, LR: 0.100000\n",
      "Epoch [17], Batch [650/782], Loss: 3.8920, LR: 0.100000\n",
      "Epoch [17], Batch [700/782], Loss: 3.4936, LR: 0.100000\n",
      "Epoch [17], Batch [750/782], Loss: 1.8127, LR: 0.100000\n",
      "Train set: Epoch: 17, Average loss:2.6591, LR: 0.100000 Top-1 Accuracy: 37.0940%, Top-5 Accuracy: 68.1080%, Time consumed:670.70s\n",
      "Test set: Epoch: 17, Average loss:1.8696, Top-1 Accuracy: 48.6000%, Top-5 Accuracy: 80.8200%, Time consumed:29.22s\n",
      "\n",
      "새로운 최고 top-1 정확도: 48.60%, top-5 정확도: 80.82%\n",
      "새로운 최고 top-5 정확도: 80.82%\n",
      "Accuracy improved (47.42% --> 48.60%). Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|████▉                                                                                   | 17/300 [3:18:20<55:03:33, 700.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18], Batch [50/782], Loss: 2.0739, LR: 0.100000\n",
      "Epoch [18], Batch [100/782], Loss: 3.3384, LR: 0.100000\n",
      "Epoch [18], Batch [150/782], Loss: 2.0314, LR: 0.100000\n",
      "Epoch [18], Batch [200/782], Loss: 1.8198, LR: 0.100000\n",
      "Epoch [18], Batch [250/782], Loss: 2.7420, LR: 0.100000\n",
      "Epoch [18], Batch [300/782], Loss: 2.2191, LR: 0.100000\n",
      "Epoch [18], Batch [350/782], Loss: 3.8446, LR: 0.100000\n",
      "Epoch [18], Batch [400/782], Loss: 2.5097, LR: 0.100000\n",
      "Epoch [18], Batch [450/782], Loss: 2.3322, LR: 0.100000\n",
      "Epoch [18], Batch [500/782], Loss: 3.1807, LR: 0.100000\n",
      "Epoch [18], Batch [550/782], Loss: 1.9113, LR: 0.100000\n",
      "Epoch [18], Batch [600/782], Loss: 3.5943, LR: 0.100000\n",
      "Epoch [18], Batch [650/782], Loss: 1.8119, LR: 0.100000\n",
      "Epoch [18], Batch [700/782], Loss: 2.6367, LR: 0.100000\n",
      "Epoch [18], Batch [750/782], Loss: 2.2437, LR: 0.100000\n",
      "Train set: Epoch: 18, Average loss:2.6205, LR: 0.100000 Top-1 Accuracy: 37.9140%, Top-5 Accuracy: 69.0620%, Time consumed:668.24s\n",
      "Test set: Epoch: 18, Average loss:1.8720, Top-1 Accuracy: 49.1300%, Top-5 Accuracy: 80.4900%, Time consumed:28.97s\n",
      "\n",
      "새로운 최고 top-1 정확도: 49.13%, top-5 정확도: 80.49%\n",
      "Accuracy improved (48.60% --> 49.13%). Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|█████▎                                                                                  | 18/300 [3:29:58<54:48:20, 699.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19], Batch [50/782], Loss: 3.3329, LR: 0.100000\n",
      "Epoch [19], Batch [100/782], Loss: 4.0310, LR: 0.100000\n",
      "Epoch [19], Batch [150/782], Loss: 1.9218, LR: 0.100000\n",
      "Epoch [19], Batch [200/782], Loss: 1.7183, LR: 0.100000\n",
      "Epoch [19], Batch [250/782], Loss: 3.5495, LR: 0.100000\n",
      "Epoch [19], Batch [300/782], Loss: 3.8015, LR: 0.100000\n",
      "Epoch [19], Batch [350/782], Loss: 2.1172, LR: 0.100000\n",
      "Epoch [19], Batch [400/782], Loss: 2.1234, LR: 0.100000\n",
      "Epoch [19], Batch [450/782], Loss: 1.9313, LR: 0.100000\n",
      "Epoch [19], Batch [500/782], Loss: 1.8513, LR: 0.100000\n",
      "Epoch [19], Batch [550/782], Loss: 1.9049, LR: 0.100000\n",
      "Epoch [19], Batch [600/782], Loss: 2.0419, LR: 0.100000\n",
      "Epoch [19], Batch [650/782], Loss: 1.9693, LR: 0.100000\n",
      "Epoch [19], Batch [700/782], Loss: 1.6788, LR: 0.100000\n",
      "Epoch [19], Batch [750/782], Loss: 3.5754, LR: 0.100000\n",
      "Train set: Epoch: 19, Average loss:2.5663, LR: 0.100000 Top-1 Accuracy: 38.5320%, Top-5 Accuracy: 69.6460%, Time consumed:667.47s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|█████▌                                                                                  | 19/300 [3:41:35<54:32:34, 698.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Epoch: 19, Average loss:1.9131, Top-1 Accuracy: 48.4900%, Top-5 Accuracy: 79.3100%, Time consumed:29.25s\n",
      "\n",
      "EarlyStopping 카운터: 1 / 30\n",
      "Epoch [20], Batch [50/782], Loss: 2.9306, LR: 0.100000\n",
      "Epoch [20], Batch [100/782], Loss: 1.8206, LR: 0.100000\n",
      "Epoch [20], Batch [150/782], Loss: 2.7620, LR: 0.100000\n",
      "Epoch [20], Batch [200/782], Loss: 2.4839, LR: 0.100000\n",
      "Epoch [20], Batch [250/782], Loss: 1.8467, LR: 0.100000\n",
      "Epoch [20], Batch [300/782], Loss: 2.5067, LR: 0.100000\n",
      "Epoch [20], Batch [350/782], Loss: 2.1363, LR: 0.100000\n",
      "Epoch [20], Batch [400/782], Loss: 2.1765, LR: 0.100000\n",
      "Epoch [20], Batch [450/782], Loss: 3.9475, LR: 0.100000\n",
      "Epoch [20], Batch [500/782], Loss: 1.8347, LR: 0.100000\n",
      "Epoch [20], Batch [550/782], Loss: 2.2319, LR: 0.100000\n",
      "Epoch [20], Batch [600/782], Loss: 3.8247, LR: 0.100000\n",
      "Epoch [20], Batch [650/782], Loss: 3.7129, LR: 0.100000\n",
      "Epoch [20], Batch [700/782], Loss: 1.8171, LR: 0.100000\n",
      "Epoch [20], Batch [750/782], Loss: 3.7954, LR: 0.100000\n",
      "Train set: Epoch: 20, Average loss:2.5459, LR: 0.100000 Top-1 Accuracy: 39.0020%, Top-5 Accuracy: 70.5400%, Time consumed:669.49s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|█████▊                                                                                  | 20/300 [3:53:13<54:20:27, 698.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Epoch: 20, Average loss:1.8987, Top-1 Accuracy: 48.0600%, Top-5 Accuracy: 79.5900%, Time consumed:28.93s\n",
      "\n",
      "EarlyStopping 카운터: 2 / 30\n",
      "Epoch [21], Batch [50/782], Loss: 2.6597, LR: 0.100000\n",
      "Epoch [21], Batch [100/782], Loss: 3.5629, LR: 0.100000\n",
      "Epoch [21], Batch [150/782], Loss: 3.2092, LR: 0.100000\n",
      "Epoch [21], Batch [200/782], Loss: 3.4712, LR: 0.100000\n",
      "Epoch [21], Batch [250/782], Loss: 2.0163, LR: 0.100000\n",
      "Epoch [21], Batch [300/782], Loss: 1.9345, LR: 0.100000\n",
      "Epoch [21], Batch [350/782], Loss: 3.3276, LR: 0.100000\n",
      "Epoch [21], Batch [400/782], Loss: 3.5500, LR: 0.100000\n",
      "Epoch [21], Batch [450/782], Loss: 3.4488, LR: 0.100000\n",
      "Epoch [21], Batch [500/782], Loss: 2.0743, LR: 0.100000\n",
      "Epoch [21], Batch [550/782], Loss: 2.5976, LR: 0.100000\n",
      "Epoch [21], Batch [600/782], Loss: 2.8782, LR: 0.100000\n",
      "Epoch [21], Batch [650/782], Loss: 3.0817, LR: 0.100000\n",
      "Epoch [21], Batch [700/782], Loss: 1.9586, LR: 0.100000\n",
      "Epoch [21], Batch [750/782], Loss: 3.6483, LR: 0.100000\n",
      "Train set: Epoch: 21, Average loss:2.6358, LR: 0.100000 Top-1 Accuracy: 38.0240%, Top-5 Accuracy: 68.6380%, Time consumed:669.18s\n",
      "Test set: Epoch: 21, Average loss:1.7999, Top-1 Accuracy: 50.2700%, Top-5 Accuracy: 81.8600%, Time consumed:30.76s\n",
      "\n",
      "새로운 최고 top-1 정확도: 50.27%, top-5 정확도: 81.86%\n",
      "새로운 최고 top-5 정확도: 81.86%\n",
      "Accuracy improved (49.13% --> 50.27%). Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|██████▏                                                                                 | 21/300 [4:04:54<54:11:31, 699.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22], Batch [50/782], Loss: 3.0184, LR: 0.100000\n",
      "Epoch [22], Batch [100/782], Loss: 2.2719, LR: 0.100000\n",
      "Epoch [22], Batch [150/782], Loss: 2.0572, LR: 0.100000\n",
      "Epoch [22], Batch [200/782], Loss: 1.8560, LR: 0.100000\n",
      "Epoch [22], Batch [250/782], Loss: 1.9023, LR: 0.100000\n",
      "Epoch [22], Batch [300/782], Loss: 2.1640, LR: 0.100000\n",
      "Epoch [22], Batch [350/782], Loss: 1.8532, LR: 0.100000\n",
      "Epoch [22], Batch [400/782], Loss: 1.8510, LR: 0.100000\n",
      "Epoch [22], Batch [450/782], Loss: 3.3925, LR: 0.100000\n",
      "Epoch [22], Batch [500/782], Loss: 2.0613, LR: 0.100000\n",
      "Epoch [22], Batch [550/782], Loss: 3.4640, LR: 0.100000\n",
      "Epoch [22], Batch [600/782], Loss: 2.7970, LR: 0.100000\n",
      "Epoch [22], Batch [650/782], Loss: 3.1833, LR: 0.100000\n",
      "Epoch [22], Batch [700/782], Loss: 1.7090, LR: 0.100000\n",
      "Epoch [22], Batch [750/782], Loss: 1.7953, LR: 0.100000\n",
      "Train set: Epoch: 22, Average loss:2.5898, LR: 0.100000 Top-1 Accuracy: 38.5880%, Top-5 Accuracy: 69.5480%, Time consumed:674.14s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|██████▍                                                                                 | 22/300 [4:16:37<54:05:33, 700.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Epoch: 22, Average loss:1.9527, Top-1 Accuracy: 47.4500%, Top-5 Accuracy: 78.4200%, Time consumed:29.19s\n",
      "\n",
      "EarlyStopping 카운터: 1 / 30\n",
      "Epoch [23], Batch [50/782], Loss: 3.5425, LR: 0.100000\n",
      "Epoch [23], Batch [100/782], Loss: 3.5375, LR: 0.100000\n",
      "Epoch [23], Batch [150/782], Loss: 3.5236, LR: 0.100000\n",
      "Epoch [23], Batch [200/782], Loss: 2.1105, LR: 0.100000\n",
      "Epoch [23], Batch [250/782], Loss: 2.7919, LR: 0.100000\n",
      "Epoch [23], Batch [300/782], Loss: 1.8120, LR: 0.100000\n",
      "Epoch [23], Batch [350/782], Loss: 1.6998, LR: 0.100000\n",
      "Epoch [23], Batch [400/782], Loss: 3.6556, LR: 0.100000\n",
      "Epoch [23], Batch [450/782], Loss: 2.3173, LR: 0.100000\n",
      "Epoch [23], Batch [500/782], Loss: 3.7771, LR: 0.100000\n",
      "Epoch [23], Batch [550/782], Loss: 2.2430, LR: 0.100000\n",
      "Epoch [23], Batch [600/782], Loss: 2.0038, LR: 0.100000\n",
      "Epoch [23], Batch [650/782], Loss: 3.3923, LR: 0.100000\n",
      "Epoch [23], Batch [700/782], Loss: 1.9496, LR: 0.100000\n",
      "Epoch [23], Batch [750/782], Loss: 2.0269, LR: 0.100000\n",
      "Train set: Epoch: 23, Average loss:2.5681, LR: 0.100000 Top-1 Accuracy: 38.9420%, Top-5 Accuracy: 69.2760%, Time consumed:672.48s\n",
      "Test set: Epoch: 23, Average loss:1.8164, Top-1 Accuracy: 50.4200%, Top-5 Accuracy: 81.3000%, Time consumed:30.66s\n",
      "\n",
      "새로운 최고 top-1 정확도: 50.42%, top-5 정확도: 81.30%\n",
      "Accuracy improved (50.27% --> 50.42%). Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|██████▋                                                                                 | 23/300 [4:28:21<53:58:28, 701.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [24], Batch [50/782], Loss: 1.8728, LR: 0.100000\n",
      "Epoch [24], Batch [100/782], Loss: 3.5476, LR: 0.100000\n",
      "Epoch [24], Batch [150/782], Loss: 3.0249, LR: 0.100000\n",
      "Epoch [24], Batch [200/782], Loss: 2.9834, LR: 0.100000\n",
      "Epoch [24], Batch [250/782], Loss: 1.8763, LR: 0.100000\n",
      "Epoch [24], Batch [300/782], Loss: 3.1419, LR: 0.100000\n",
      "Epoch [24], Batch [350/782], Loss: 1.9453, LR: 0.100000\n",
      "Epoch [24], Batch [400/782], Loss: 2.2424, LR: 0.100000\n",
      "Epoch [24], Batch [450/782], Loss: 3.4501, LR: 0.100000\n",
      "Epoch [24], Batch [500/782], Loss: 1.5755, LR: 0.100000\n",
      "Epoch [24], Batch [550/782], Loss: 2.9753, LR: 0.100000\n",
      "Epoch [24], Batch [600/782], Loss: 3.3230, LR: 0.100000\n",
      "Epoch [24], Batch [650/782], Loss: 1.9427, LR: 0.100000\n",
      "Epoch [24], Batch [700/782], Loss: 2.0577, LR: 0.100000\n",
      "Epoch [24], Batch [750/782], Loss: 3.2681, LR: 0.100000\n",
      "Train set: Epoch: 24, Average loss:2.5544, LR: 0.100000 Top-1 Accuracy: 39.3120%, Top-5 Accuracy: 69.9580%, Time consumed:670.85s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|███████                                                                                 | 24/300 [4:40:01<53:44:45, 701.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Epoch: 24, Average loss:1.8569, Top-1 Accuracy: 48.7800%, Top-5 Accuracy: 80.4400%, Time consumed:29.14s\n",
      "\n",
      "EarlyStopping 카운터: 1 / 30\n",
      "Epoch [25], Batch [50/782], Loss: 3.0206, LR: 0.100000\n",
      "Epoch [25], Batch [100/782], Loss: 3.4396, LR: 0.100000\n",
      "Epoch [25], Batch [150/782], Loss: 1.8948, LR: 0.100000\n",
      "Epoch [25], Batch [200/782], Loss: 2.0598, LR: 0.100000\n",
      "Epoch [25], Batch [250/782], Loss: 1.7463, LR: 0.100000\n",
      "Epoch [25], Batch [300/782], Loss: 3.3486, LR: 0.100000\n",
      "Epoch [25], Batch [350/782], Loss: 1.8261, LR: 0.100000\n",
      "Epoch [25], Batch [400/782], Loss: 2.1338, LR: 0.100000\n",
      "Epoch [25], Batch [450/782], Loss: 2.1397, LR: 0.100000\n",
      "Epoch [25], Batch [500/782], Loss: 3.4349, LR: 0.100000\n",
      "Epoch [25], Batch [550/782], Loss: 3.7212, LR: 0.100000\n",
      "Epoch [25], Batch [600/782], Loss: 1.8523, LR: 0.100000\n",
      "Epoch [25], Batch [650/782], Loss: 2.0825, LR: 0.100000\n",
      "Epoch [25], Batch [700/782], Loss: 3.1665, LR: 0.100000\n",
      "Epoch [25], Batch [750/782], Loss: 2.4134, LR: 0.100000\n",
      "Train set: Epoch: 25, Average loss:2.5294, LR: 0.100000 Top-1 Accuracy: 39.7060%, Top-5 Accuracy: 70.6720%, Time consumed:669.15s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|███████▎                                                                                | 25/300 [4:51:41<53:31:56, 700.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Epoch: 25, Average loss:1.9305, Top-1 Accuracy: 47.6600%, Top-5 Accuracy: 78.8100%, Time consumed:31.05s\n",
      "\n",
      "EarlyStopping 카운터: 2 / 30\n",
      "Epoch [26], Batch [50/782], Loss: 3.2528, LR: 0.100000\n",
      "Epoch [26], Batch [100/782], Loss: 1.6333, LR: 0.100000\n",
      "Epoch [26], Batch [150/782], Loss: 1.8451, LR: 0.100000\n",
      "Epoch [26], Batch [200/782], Loss: 1.9895, LR: 0.100000\n",
      "Epoch [26], Batch [250/782], Loss: 3.4000, LR: 0.100000\n",
      "Epoch [26], Batch [300/782], Loss: 1.9070, LR: 0.100000\n",
      "Epoch [26], Batch [350/782], Loss: 1.9347, LR: 0.100000\n",
      "Epoch [26], Batch [400/782], Loss: 2.0176, LR: 0.100000\n",
      "Epoch [26], Batch [450/782], Loss: 1.9680, LR: 0.100000\n",
      "Epoch [26], Batch [500/782], Loss: 1.7948, LR: 0.100000\n",
      "Epoch [26], Batch [550/782], Loss: 3.1126, LR: 0.100000\n",
      "Epoch [26], Batch [600/782], Loss: 3.2959, LR: 0.100000\n",
      "Epoch [26], Batch [650/782], Loss: 2.1557, LR: 0.100000\n",
      "Epoch [26], Batch [700/782], Loss: 2.0586, LR: 0.100000\n",
      "Epoch [26], Batch [750/782], Loss: 3.6433, LR: 0.100000\n",
      "Train set: Epoch: 26, Average loss:2.5645, LR: 0.100000 Top-1 Accuracy: 39.3000%, Top-5 Accuracy: 70.0420%, Time consumed:666.66s\n",
      "Test set: Epoch: 26, Average loss:1.7652, Top-1 Accuracy: 50.7400%, Top-5 Accuracy: 82.0700%, Time consumed:28.84s\n",
      "\n",
      "새로운 최고 top-1 정확도: 50.74%, top-5 정확도: 82.07%\n",
      "새로운 최고 top-5 정확도: 82.07%\n",
      "Accuracy improved (50.42% --> 50.74%). Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|███████▋                                                                                | 26/300 [5:03:18<53:14:16, 699.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [27], Batch [50/782], Loss: 1.8843, LR: 0.100000\n",
      "Epoch [27], Batch [100/782], Loss: 3.4245, LR: 0.100000\n",
      "Epoch [27], Batch [150/782], Loss: 3.6874, LR: 0.100000\n",
      "Epoch [27], Batch [200/782], Loss: 2.9984, LR: 0.100000\n",
      "Epoch [27], Batch [250/782], Loss: 3.5249, LR: 0.100000\n",
      "Epoch [27], Batch [300/782], Loss: 3.3718, LR: 0.100000\n",
      "Epoch [27], Batch [350/782], Loss: 1.7487, LR: 0.100000\n",
      "Epoch [27], Batch [400/782], Loss: 3.6343, LR: 0.100000\n",
      "Epoch [27], Batch [450/782], Loss: 3.1939, LR: 0.100000\n",
      "Epoch [27], Batch [500/782], Loss: 2.8560, LR: 0.100000\n",
      "Epoch [27], Batch [550/782], Loss: 2.0773, LR: 0.100000\n",
      "Epoch [27], Batch [600/782], Loss: 1.7717, LR: 0.100000\n",
      "Epoch [27], Batch [650/782], Loss: 2.1978, LR: 0.100000\n",
      "Epoch [27], Batch [700/782], Loss: 1.8396, LR: 0.100000\n",
      "Epoch [27], Batch [750/782], Loss: 2.8921, LR: 0.100000\n",
      "Train set: Epoch: 27, Average loss:2.5448, LR: 0.100000 Top-1 Accuracy: 40.0740%, Top-5 Accuracy: 70.5820%, Time consumed:668.57s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|███████▉                                                                                | 27/300 [5:14:55<52:59:54, 698.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Epoch: 27, Average loss:1.9144, Top-1 Accuracy: 48.4400%, Top-5 Accuracy: 79.3700%, Time consumed:28.91s\n",
      "\n",
      "EarlyStopping 카운터: 1 / 30\n",
      "Epoch [28], Batch [50/782], Loss: 1.7652, LR: 0.050000\n",
      "Epoch [28], Batch [100/782], Loss: 3.0421, LR: 0.050000\n",
      "Epoch [28], Batch [150/782], Loss: 1.5341, LR: 0.050000\n",
      "Epoch [28], Batch [200/782], Loss: 1.7836, LR: 0.050000\n",
      "Epoch [28], Batch [250/782], Loss: 1.6726, LR: 0.050000\n",
      "Epoch [28], Batch [300/782], Loss: 2.7881, LR: 0.050000\n",
      "Epoch [28], Batch [350/782], Loss: 1.6996, LR: 0.050000\n",
      "Epoch [28], Batch [400/782], Loss: 1.4039, LR: 0.050000\n",
      "Epoch [28], Batch [450/782], Loss: 3.1133, LR: 0.050000\n",
      "Epoch [28], Batch [500/782], Loss: 3.8953, LR: 0.050000\n",
      "Epoch [28], Batch [550/782], Loss: 3.4379, LR: 0.050000\n",
      "Epoch [28], Batch [600/782], Loss: 3.0673, LR: 0.050000\n",
      "Epoch [28], Batch [650/782], Loss: 1.6367, LR: 0.050000\n",
      "Epoch [28], Batch [700/782], Loss: 1.3757, LR: 0.050000\n",
      "Epoch [28], Batch [750/782], Loss: 1.6680, LR: 0.050000\n",
      "Train set: Epoch: 28, Average loss:2.2950, LR: 0.050000 Top-1 Accuracy: 46.0220%, Top-5 Accuracy: 75.7320%, Time consumed:671.81s\n",
      "Test set: Epoch: 28, Average loss:1.4509, Top-1 Accuracy: 58.9200%, Top-5 Accuracy: 87.0700%, Time consumed:29.53s\n",
      "\n",
      "새로운 최고 top-1 정확도: 58.92%, top-5 정확도: 87.07%\n",
      "새로운 최고 top-5 정확도: 87.07%\n",
      "Accuracy improved (50.74% --> 58.92%). Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|████████▏                                                                               | 28/300 [5:26:37<52:52:30, 699.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [29], Batch [50/782], Loss: 1.6082, LR: 0.050000\n",
      "Epoch [29], Batch [100/782], Loss: 2.3266, LR: 0.050000\n",
      "Epoch [29], Batch [150/782], Loss: 1.7859, LR: 0.050000\n",
      "Epoch [29], Batch [200/782], Loss: 1.9400, LR: 0.050000\n",
      "Epoch [29], Batch [250/782], Loss: 1.6156, LR: 0.050000\n",
      "Epoch [29], Batch [300/782], Loss: 3.3800, LR: 0.050000\n",
      "Epoch [29], Batch [350/782], Loss: 2.6188, LR: 0.050000\n",
      "Epoch [29], Batch [400/782], Loss: 1.9146, LR: 0.050000\n",
      "Epoch [29], Batch [450/782], Loss: 1.4886, LR: 0.050000\n",
      "Epoch [29], Batch [500/782], Loss: 1.4510, LR: 0.050000\n",
      "Epoch [29], Batch [550/782], Loss: 1.3543, LR: 0.050000\n",
      "Epoch [29], Batch [600/782], Loss: 1.6004, LR: 0.050000\n",
      "Epoch [29], Batch [650/782], Loss: 1.7817, LR: 0.050000\n",
      "Epoch [29], Batch [700/782], Loss: 3.0644, LR: 0.050000\n",
      "Epoch [29], Batch [750/782], Loss: 1.8476, LR: 0.050000\n",
      "Train set: Epoch: 29, Average loss:2.2830, LR: 0.050000 Top-1 Accuracy: 45.9740%, Top-5 Accuracy: 75.3740%, Time consumed:671.01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|████████▌                                                                               | 29/300 [5:38:17<52:41:06, 699.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Epoch: 29, Average loss:1.4997, Top-1 Accuracy: 57.8000%, Top-5 Accuracy: 85.7300%, Time consumed:28.99s\n",
      "\n",
      "EarlyStopping 카운터: 1 / 30\n",
      "Epoch [30], Batch [50/782], Loss: 2.0678, LR: 0.050000\n",
      "Epoch [30], Batch [100/782], Loss: 3.3745, LR: 0.050000\n",
      "Epoch [30], Batch [150/782], Loss: 1.6135, LR: 0.050000\n",
      "Epoch [30], Batch [200/782], Loss: 1.4922, LR: 0.050000\n",
      "Epoch [30], Batch [250/782], Loss: 3.2197, LR: 0.050000\n",
      "Epoch [30], Batch [300/782], Loss: 3.3580, LR: 0.050000\n",
      "Epoch [30], Batch [350/782], Loss: 3.0262, LR: 0.050000\n",
      "Epoch [30], Batch [400/782], Loss: 1.2834, LR: 0.050000\n",
      "Epoch [30], Batch [450/782], Loss: 1.5750, LR: 0.050000\n",
      "Epoch [30], Batch [500/782], Loss: 3.6706, LR: 0.050000\n",
      "Epoch [30], Batch [550/782], Loss: 1.6922, LR: 0.050000\n",
      "Epoch [30], Batch [600/782], Loss: 2.0568, LR: 0.050000\n",
      "Epoch [30], Batch [650/782], Loss: 1.4671, LR: 0.050000\n",
      "Epoch [30], Batch [700/782], Loss: 1.7893, LR: 0.050000\n",
      "Epoch [30], Batch [750/782], Loss: 1.5872, LR: 0.050000\n",
      "Train set: Epoch: 30, Average loss:2.2652, LR: 0.050000 Top-1 Accuracy: 46.9020%, Top-5 Accuracy: 76.3240%, Time consumed:672.53s\n",
      "Test set: Epoch: 30, Average loss:1.4227, Top-1 Accuracy: 60.1500%, Top-5 Accuracy: 87.3000%, Time consumed:28.99s\n",
      "\n",
      "새로운 최고 top-1 정확도: 60.15%, top-5 정확도: 87.30%\n",
      "새로운 최고 top-5 정확도: 87.30%\n",
      "Accuracy improved (58.92% --> 60.15%). Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|████████▊                                                                               | 30/300 [5:50:00<52:32:55, 700.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [31], Batch [50/782], Loss: 3.6845, LR: 0.050000\n",
      "Epoch [31], Batch [100/782], Loss: 2.9015, LR: 0.050000\n",
      "Epoch [31], Batch [150/782], Loss: 1.6296, LR: 0.050000\n",
      "Epoch [31], Batch [200/782], Loss: 2.0613, LR: 0.050000\n",
      "Epoch [31], Batch [250/782], Loss: 3.4307, LR: 0.050000\n",
      "Epoch [31], Batch [300/782], Loss: 2.0376, LR: 0.050000\n",
      "Epoch [31], Batch [350/782], Loss: 1.4417, LR: 0.050000\n",
      "Epoch [31], Batch [400/782], Loss: 1.6043, LR: 0.050000\n",
      "Epoch [31], Batch [450/782], Loss: 3.5623, LR: 0.050000\n",
      "Epoch [31], Batch [500/782], Loss: 3.3699, LR: 0.050000\n",
      "Epoch [31], Batch [550/782], Loss: 1.4756, LR: 0.050000\n",
      "Epoch [31], Batch [600/782], Loss: 3.0104, LR: 0.050000\n",
      "Epoch [31], Batch [650/782], Loss: 3.3220, LR: 0.050000\n",
      "Epoch [31], Batch [700/782], Loss: 2.6936, LR: 0.050000\n",
      "Epoch [31], Batch [750/782], Loss: 1.7717, LR: 0.050000\n",
      "Train set: Epoch: 31, Average loss:2.2582, LR: 0.050000 Top-1 Accuracy: 47.2580%, Top-5 Accuracy: 76.6680%, Time consumed:672.13s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█████████                                                                               | 31/300 [6:01:41<52:21:54, 700.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Epoch: 31, Average loss:1.4869, Top-1 Accuracy: 58.3800%, Top-5 Accuracy: 86.3300%, Time consumed:29.00s\n",
      "\n",
      "EarlyStopping 카운터: 1 / 30\n",
      "Epoch [32], Batch [50/782], Loss: 1.9456, LR: 0.050000\n",
      "Epoch [32], Batch [100/782], Loss: 3.3640, LR: 0.050000\n",
      "Epoch [32], Batch [150/782], Loss: 3.0389, LR: 0.050000\n",
      "Epoch [32], Batch [200/782], Loss: 1.4198, LR: 0.050000\n",
      "Epoch [32], Batch [250/782], Loss: 3.6646, LR: 0.050000\n",
      "Epoch [32], Batch [300/782], Loss: 2.2594, LR: 0.050000\n",
      "Epoch [32], Batch [350/782], Loss: 1.9057, LR: 0.050000\n",
      "Epoch [32], Batch [400/782], Loss: 1.6146, LR: 0.050000\n",
      "Epoch [32], Batch [450/782], Loss: 3.3876, LR: 0.050000\n",
      "Epoch [32], Batch [500/782], Loss: 1.5120, LR: 0.050000\n",
      "Epoch [32], Batch [550/782], Loss: 1.3947, LR: 0.050000\n",
      "Epoch [32], Batch [600/782], Loss: 3.6867, LR: 0.050000\n",
      "Epoch [32], Batch [650/782], Loss: 1.3092, LR: 0.050000\n",
      "Epoch [32], Batch [700/782], Loss: 2.5243, LR: 0.050000\n",
      "Epoch [32], Batch [750/782], Loss: 3.5025, LR: 0.050000\n",
      "Train set: Epoch: 32, Average loss:2.2892, LR: 0.050000 Top-1 Accuracy: 46.4240%, Top-5 Accuracy: 75.8360%, Time consumed:672.59s\n",
      "Test set: Epoch: 32, Average loss:1.3869, Top-1 Accuracy: 60.6400%, Top-5 Accuracy: 88.3000%, Time consumed:31.97s\n",
      "\n",
      "새로운 최고 top-1 정확도: 60.64%, top-5 정확도: 88.30%\n",
      "새로운 최고 top-5 정확도: 88.30%\n",
      "Accuracy improved (60.15% --> 60.64%). Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█████████▍                                                                              | 32/300 [6:13:26<52:16:08, 702.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [33], Batch [50/782], Loss: 3.7056, LR: 0.050000\n",
      "Epoch [33], Batch [100/782], Loss: 1.5399, LR: 0.050000\n",
      "Epoch [33], Batch [150/782], Loss: 1.4475, LR: 0.050000\n",
      "Epoch [33], Batch [200/782], Loss: 3.0074, LR: 0.050000\n",
      "Epoch [33], Batch [250/782], Loss: 1.6768, LR: 0.050000\n",
      "Epoch [33], Batch [300/782], Loss: 1.4734, LR: 0.050000\n",
      "Epoch [33], Batch [350/782], Loss: 1.4992, LR: 0.050000\n",
      "Epoch [33], Batch [400/782], Loss: 1.3805, LR: 0.050000\n",
      "Epoch [33], Batch [450/782], Loss: 1.8580, LR: 0.050000\n",
      "Epoch [33], Batch [500/782], Loss: 1.8402, LR: 0.050000\n",
      "Epoch [33], Batch [550/782], Loss: 3.1057, LR: 0.050000\n",
      "Epoch [33], Batch [600/782], Loss: 1.4795, LR: 0.050000\n",
      "Epoch [33], Batch [650/782], Loss: 3.4611, LR: 0.050000\n",
      "Epoch [33], Batch [700/782], Loss: 1.6504, LR: 0.050000\n",
      "Epoch [33], Batch [750/782], Loss: 3.1480, LR: 0.050000\n",
      "Train set: Epoch: 33, Average loss:2.2648, LR: 0.050000 Top-1 Accuracy: 46.5200%, Top-5 Accuracy: 75.7080%, Time consumed:673.44s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█████████▋                                                                              | 33/300 [6:25:09<52:04:54, 702.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Epoch: 33, Average loss:1.4990, Top-1 Accuracy: 58.4700%, Top-5 Accuracy: 86.3700%, Time consumed:29.02s\n",
      "\n",
      "EarlyStopping 카운터: 1 / 30\n",
      "Epoch [34], Batch [50/782], Loss: 3.4758, LR: 0.050000\n",
      "Epoch [34], Batch [100/782], Loss: 1.6055, LR: 0.050000\n",
      "Epoch [34], Batch [150/782], Loss: 3.0210, LR: 0.050000\n",
      "Epoch [34], Batch [200/782], Loss: 3.4883, LR: 0.050000\n",
      "Epoch [34], Batch [250/782], Loss: 3.1797, LR: 0.050000\n",
      "Epoch [34], Batch [300/782], Loss: 1.6516, LR: 0.050000\n",
      "Epoch [34], Batch [350/782], Loss: 2.4308, LR: 0.050000\n",
      "Epoch [34], Batch [400/782], Loss: 1.5535, LR: 0.050000\n",
      "Epoch [34], Batch [450/782], Loss: 3.7919, LR: 0.050000\n",
      "Epoch [34], Batch [500/782], Loss: 1.9345, LR: 0.050000\n",
      "Epoch [34], Batch [550/782], Loss: 3.4840, LR: 0.050000\n",
      "Epoch [34], Batch [600/782], Loss: 1.8069, LR: 0.050000\n",
      "Epoch [34], Batch [650/782], Loss: 1.6072, LR: 0.050000\n",
      "Epoch [34], Batch [700/782], Loss: 1.4595, LR: 0.050000\n",
      "Epoch [34], Batch [750/782], Loss: 3.5542, LR: 0.050000\n",
      "Train set: Epoch: 34, Average loss:2.2743, LR: 0.050000 Top-1 Accuracy: 46.5560%, Top-5 Accuracy: 75.7340%, Time consumed:671.87s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█████████▉                                                                              | 34/300 [6:36:51<51:53:23, 702.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Epoch: 34, Average loss:1.4224, Top-1 Accuracy: 60.5500%, Top-5 Accuracy: 87.7400%, Time consumed:30.48s\n",
      "\n",
      "EarlyStopping 카운터: 2 / 30\n",
      "Epoch [35], Batch [50/782], Loss: 2.5959, LR: 0.050000\n",
      "Epoch [35], Batch [100/782], Loss: 3.1099, LR: 0.050000\n",
      "Epoch [35], Batch [150/782], Loss: 1.4877, LR: 0.050000\n",
      "Epoch [35], Batch [200/782], Loss: 3.3117, LR: 0.050000\n",
      "Epoch [35], Batch [250/782], Loss: 2.7094, LR: 0.050000\n",
      "Epoch [35], Batch [300/782], Loss: 3.3199, LR: 0.050000\n",
      "Epoch [35], Batch [350/782], Loss: 2.8819, LR: 0.050000\n",
      "Epoch [35], Batch [400/782], Loss: 2.7630, LR: 0.050000\n",
      "Epoch [35], Batch [450/782], Loss: 1.4981, LR: 0.050000\n",
      "Epoch [35], Batch [500/782], Loss: 1.3798, LR: 0.050000\n",
      "Epoch [35], Batch [550/782], Loss: 3.2474, LR: 0.050000\n",
      "Epoch [35], Batch [600/782], Loss: 1.5297, LR: 0.050000\n",
      "Epoch [35], Batch [650/782], Loss: 1.4035, LR: 0.050000\n",
      "Epoch [35], Batch [700/782], Loss: 1.3985, LR: 0.050000\n",
      "Epoch [35], Batch [750/782], Loss: 3.4485, LR: 0.050000\n",
      "Train set: Epoch: 35, Average loss:2.2890, LR: 0.050000 Top-1 Accuracy: 46.6800%, Top-5 Accuracy: 75.9720%, Time consumed:666.80s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|██████████▎                                                                             | 35/300 [6:48:27<51:33:01, 700.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Epoch: 35, Average loss:1.4172, Top-1 Accuracy: 59.4200%, Top-5 Accuracy: 87.5000%, Time consumed:28.92s\n",
      "\n",
      "EarlyStopping 카운터: 3 / 30\n",
      "Epoch [36], Batch [50/782], Loss: 1.8463, LR: 0.050000\n",
      "Epoch [36], Batch [100/782], Loss: 3.1136, LR: 0.050000\n",
      "Epoch [36], Batch [150/782], Loss: 1.6365, LR: 0.050000\n",
      "Epoch [36], Batch [200/782], Loss: 2.2473, LR: 0.050000\n",
      "Epoch [36], Batch [250/782], Loss: 2.2176, LR: 0.050000\n",
      "Epoch [36], Batch [300/782], Loss: 1.6436, LR: 0.050000\n",
      "Epoch [36], Batch [350/782], Loss: 1.6385, LR: 0.050000\n",
      "Epoch [36], Batch [400/782], Loss: 1.7855, LR: 0.050000\n",
      "Epoch [36], Batch [450/782], Loss: 1.3440, LR: 0.050000\n",
      "Epoch [36], Batch [500/782], Loss: 1.4183, LR: 0.050000\n",
      "Epoch [36], Batch [550/782], Loss: 1.6779, LR: 0.050000\n",
      "Epoch [36], Batch [600/782], Loss: 1.6379, LR: 0.050000\n",
      "Epoch [36], Batch [650/782], Loss: 1.7430, LR: 0.050000\n",
      "Epoch [36], Batch [700/782], Loss: 3.0397, LR: 0.050000\n",
      "Epoch [36], Batch [750/782], Loss: 1.3944, LR: 0.050000\n",
      "Train set: Epoch: 36, Average loss:2.1848, LR: 0.050000 Top-1 Accuracy: 48.3080%, Top-5 Accuracy: 77.5660%, Time consumed:667.74s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|██████████▌                                                                             | 36/300 [7:00:04<51:17:29, 699.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Epoch: 36, Average loss:1.4209, Top-1 Accuracy: 60.4000%, Top-5 Accuracy: 87.7500%, Time consumed:29.62s\n",
      "\n",
      "EarlyStopping 카운터: 4 / 30\n",
      "Epoch [37], Batch [50/782], Loss: 1.5809, LR: 0.025000\n",
      "Epoch [37], Batch [100/782], Loss: 1.2889, LR: 0.025000\n",
      "Epoch [37], Batch [150/782], Loss: 2.8359, LR: 0.025000\n",
      "Epoch [37], Batch [200/782], Loss: 1.5729, LR: 0.025000\n",
      "Epoch [37], Batch [250/782], Loss: 1.1544, LR: 0.025000\n",
      "Epoch [37], Batch [300/782], Loss: 2.9074, LR: 0.025000\n",
      "Epoch [37], Batch [350/782], Loss: 1.6400, LR: 0.025000\n",
      "Epoch [37], Batch [400/782], Loss: 1.3527, LR: 0.025000\n",
      "Epoch [37], Batch [450/782], Loss: 3.1107, LR: 0.025000\n",
      "Epoch [37], Batch [500/782], Loss: 3.2079, LR: 0.025000\n",
      "Epoch [37], Batch [550/782], Loss: 1.1121, LR: 0.025000\n",
      "Epoch [37], Batch [600/782], Loss: 1.5165, LR: 0.025000\n",
      "Epoch [37], Batch [650/782], Loss: 3.0323, LR: 0.025000\n",
      "Epoch [37], Batch [700/782], Loss: 3.0485, LR: 0.025000\n",
      "Epoch [37], Batch [750/782], Loss: 1.2305, LR: 0.025000\n",
      "Train set: Epoch: 37, Average loss:2.0526, LR: 0.025000 Top-1 Accuracy: 53.1240%, Top-5 Accuracy: 80.7160%, Time consumed:665.73s\n",
      "Test set: Epoch: 37, Average loss:1.1922, Top-1 Accuracy: 65.8300%, Top-5 Accuracy: 90.7500%, Time consumed:28.68s\n",
      "\n",
      "새로운 최고 top-1 정확도: 65.83%, top-5 정확도: 90.75%\n",
      "새로운 최고 top-5 정확도: 90.75%\n",
      "Accuracy improved (60.64% --> 65.83%). Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|██████████▊                                                                             | 37/300 [7:11:39<51:00:05, 698.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [38], Batch [50/782], Loss: 0.9903, LR: 0.025000\n",
      "Epoch [38], Batch [100/782], Loss: 3.3637, LR: 0.025000\n",
      "Epoch [38], Batch [150/782], Loss: 3.0540, LR: 0.025000\n",
      "Epoch [38], Batch [200/782], Loss: 1.1636, LR: 0.025000\n",
      "Epoch [38], Batch [250/782], Loss: 2.1287, LR: 0.025000\n",
      "Epoch [38], Batch [300/782], Loss: 1.9372, LR: 0.025000\n",
      "Epoch [38], Batch [350/782], Loss: 1.3156, LR: 0.025000\n",
      "Epoch [38], Batch [400/782], Loss: 3.2773, LR: 0.025000\n",
      "Epoch [38], Batch [450/782], Loss: 2.3504, LR: 0.025000\n",
      "Epoch [38], Batch [500/782], Loss: 1.4179, LR: 0.025000\n",
      "Epoch [38], Batch [550/782], Loss: 3.0189, LR: 0.025000\n",
      "Epoch [38], Batch [600/782], Loss: 1.2936, LR: 0.025000\n",
      "Epoch [38], Batch [650/782], Loss: 2.8701, LR: 0.025000\n",
      "Epoch [38], Batch [700/782], Loss: 2.9995, LR: 0.025000\n",
      "Epoch [38], Batch [750/782], Loss: 1.3648, LR: 0.025000\n",
      "Train set: Epoch: 38, Average loss:2.0324, LR: 0.025000 Top-1 Accuracy: 53.0520%, Top-5 Accuracy: 80.6260%, Time consumed:664.98s\n",
      "Test set: Epoch: 38, Average loss:1.1548, Top-1 Accuracy: 66.5600%, Top-5 Accuracy: 91.1700%, Time consumed:29.21s\n",
      "\n",
      "새로운 최고 top-1 정확도: 66.56%, top-5 정확도: 91.17%\n",
      "새로운 최고 top-5 정확도: 91.17%\n",
      "Accuracy improved (65.83% --> 66.56%). Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|███████████▏                                                                            | 38/300 [7:23:14<50:44:10, 697.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [39], Batch [50/782], Loss: 2.7325, LR: 0.025000\n",
      "Epoch [39], Batch [100/782], Loss: 1.9026, LR: 0.025000\n",
      "Epoch [39], Batch [150/782], Loss: 2.7263, LR: 0.025000\n",
      "Epoch [39], Batch [200/782], Loss: 3.0363, LR: 0.025000\n",
      "Epoch [39], Batch [250/782], Loss: 2.2287, LR: 0.025000\n",
      "Epoch [39], Batch [300/782], Loss: 1.5255, LR: 0.025000\n",
      "Epoch [39], Batch [350/782], Loss: 1.1075, LR: 0.025000\n",
      "Epoch [39], Batch [400/782], Loss: 2.8074, LR: 0.025000\n",
      "Epoch [39], Batch [450/782], Loss: 3.5051, LR: 0.025000\n",
      "Epoch [39], Batch [500/782], Loss: 1.1610, LR: 0.025000\n",
      "Epoch [39], Batch [550/782], Loss: 1.0278, LR: 0.025000\n",
      "Epoch [39], Batch [600/782], Loss: 3.1465, LR: 0.025000\n",
      "Epoch [39], Batch [650/782], Loss: 1.0455, LR: 0.025000\n",
      "Epoch [39], Batch [700/782], Loss: 1.2565, LR: 0.025000\n",
      "Epoch [39], Batch [750/782], Loss: 1.2747, LR: 0.025000\n",
      "Train set: Epoch: 39, Average loss:1.9599, LR: 0.025000 Top-1 Accuracy: 54.3620%, Top-5 Accuracy: 81.6960%, Time consumed:666.51s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|███████████▍                                                                            | 39/300 [7:34:51<50:32:22, 697.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Epoch: 39, Average loss:1.2375, Top-1 Accuracy: 64.6500%, Top-5 Accuracy: 89.7700%, Time consumed:30.48s\n",
      "\n",
      "EarlyStopping 카운터: 1 / 30\n",
      "Epoch [40], Batch [50/782], Loss: 1.0965, LR: 0.025000\n",
      "Epoch [40], Batch [100/782], Loss: 1.1762, LR: 0.025000\n",
      "Epoch [40], Batch [150/782], Loss: 3.2288, LR: 0.025000\n",
      "Epoch [40], Batch [200/782], Loss: 2.8906, LR: 0.025000\n",
      "Epoch [40], Batch [250/782], Loss: 1.4963, LR: 0.025000\n",
      "Epoch [40], Batch [300/782], Loss: 1.1849, LR: 0.025000\n",
      "Epoch [40], Batch [350/782], Loss: 3.0028, LR: 0.025000\n",
      "Epoch [40], Batch [400/782], Loss: 3.1103, LR: 0.025000\n",
      "Epoch [40], Batch [450/782], Loss: 1.6362, LR: 0.025000\n",
      "Epoch [40], Batch [500/782], Loss: 1.4518, LR: 0.025000\n",
      "Epoch [40], Batch [550/782], Loss: 2.0512, LR: 0.025000\n",
      "Epoch [40], Batch [600/782], Loss: 1.6135, LR: 0.025000\n",
      "Epoch [40], Batch [650/782], Loss: 1.1608, LR: 0.025000\n",
      "Epoch [40], Batch [700/782], Loss: 3.1655, LR: 0.025000\n",
      "Epoch [40], Batch [750/782], Loss: 2.4230, LR: 0.025000\n",
      "Train set: Epoch: 40, Average loss:2.0066, LR: 0.025000 Top-1 Accuracy: 53.6240%, Top-5 Accuracy: 81.4020%, Time consumed:668.83s\n",
      "Test set: Epoch: 40, Average loss:1.1186, Top-1 Accuracy: 67.8000%, Top-5 Accuracy: 91.4200%, Time consumed:29.75s\n",
      "\n",
      "새로운 최고 top-1 정확도: 67.80%, top-5 정확도: 91.42%\n",
      "새로운 최고 top-5 정확도: 91.42%\n",
      "Accuracy improved (66.56% --> 67.80%). Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|███████████▋                                                                            | 40/300 [7:46:30<50:23:29, 697.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [41], Batch [50/782], Loss: 1.2089, LR: 0.025000\n",
      "Epoch [41], Batch [100/782], Loss: 0.9479, LR: 0.025000\n",
      "Epoch [41], Batch [150/782], Loss: 1.2534, LR: 0.025000\n",
      "Epoch [41], Batch [200/782], Loss: 1.2923, LR: 0.025000\n",
      "Epoch [41], Batch [250/782], Loss: 1.3537, LR: 0.025000\n",
      "Epoch [41], Batch [300/782], Loss: 2.8217, LR: 0.025000\n",
      "Epoch [41], Batch [350/782], Loss: 1.1892, LR: 0.025000\n",
      "Epoch [41], Batch [400/782], Loss: 1.1626, LR: 0.025000\n",
      "Epoch [41], Batch [450/782], Loss: 1.4351, LR: 0.025000\n",
      "Epoch [41], Batch [500/782], Loss: 3.0496, LR: 0.025000\n",
      "Epoch [41], Batch [550/782], Loss: 3.2507, LR: 0.025000\n",
      "Epoch [41], Batch [600/782], Loss: 1.6546, LR: 0.025000\n",
      "Epoch [41], Batch [650/782], Loss: 1.3187, LR: 0.025000\n",
      "Epoch [41], Batch [700/782], Loss: 2.0929, LR: 0.025000\n",
      "Epoch [41], Batch [750/782], Loss: 3.1485, LR: 0.025000\n",
      "Train set: Epoch: 41, Average loss:2.0032, LR: 0.025000 Top-1 Accuracy: 53.4880%, Top-5 Accuracy: 80.7420%, Time consumed:674.77s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|████████████                                                                            | 41/300 [7:58:16<50:21:50, 700.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Epoch: 41, Average loss:1.1506, Top-1 Accuracy: 66.9500%, Top-5 Accuracy: 91.5500%, Time consumed:30.64s\n",
      "\n",
      "새로운 최고 top-5 정확도: 91.55%\n",
      "EarlyStopping 카운터: 1 / 30\n",
      "Epoch [42], Batch [50/782], Loss: 1.3279, LR: 0.025000\n",
      "Epoch [42], Batch [100/782], Loss: 2.0234, LR: 0.025000\n",
      "Epoch [42], Batch [150/782], Loss: 1.2692, LR: 0.025000\n",
      "Epoch [42], Batch [200/782], Loss: 1.2404, LR: 0.025000\n",
      "Epoch [42], Batch [250/782], Loss: 3.3097, LR: 0.025000\n",
      "Epoch [42], Batch [300/782], Loss: 1.2901, LR: 0.025000\n",
      "Epoch [42], Batch [350/782], Loss: 1.1921, LR: 0.025000\n",
      "Epoch [42], Batch [400/782], Loss: 0.9841, LR: 0.025000\n",
      "Epoch [42], Batch [450/782], Loss: 1.7001, LR: 0.025000\n",
      "Epoch [42], Batch [500/782], Loss: 2.1747, LR: 0.025000\n",
      "Epoch [42], Batch [550/782], Loss: 1.3321, LR: 0.025000\n",
      "Epoch [42], Batch [600/782], Loss: 1.2688, LR: 0.025000\n",
      "Epoch [42], Batch [650/782], Loss: 1.2747, LR: 0.025000\n",
      "Epoch [42], Batch [700/782], Loss: 2.4758, LR: 0.025000\n",
      "Epoch [42], Batch [750/782], Loss: 1.7621, LR: 0.025000\n",
      "Train set: Epoch: 42, Average loss:1.9496, LR: 0.025000 Top-1 Accuracy: 54.4720%, Top-5 Accuracy: 81.5300%, Time consumed:671.17s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|████████████▎                                                                           | 42/300 [8:09:56<50:10:28, 700.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Epoch: 42, Average loss:1.1356, Top-1 Accuracy: 67.4500%, Top-5 Accuracy: 91.4900%, Time consumed:29.09s\n",
      "\n",
      "EarlyStopping 카운터: 2 / 30\n",
      "Epoch [43], Batch [50/782], Loss: 2.9108, LR: 0.025000\n",
      "Epoch [43], Batch [100/782], Loss: 2.3663, LR: 0.025000\n",
      "Epoch [43], Batch [150/782], Loss: 1.1957, LR: 0.025000\n",
      "Epoch [43], Batch [200/782], Loss: 2.2305, LR: 0.025000\n",
      "Epoch [43], Batch [250/782], Loss: 3.0357, LR: 0.025000\n",
      "Epoch [43], Batch [300/782], Loss: 1.3847, LR: 0.025000\n",
      "Epoch [43], Batch [350/782], Loss: 1.2692, LR: 0.025000\n",
      "Epoch [43], Batch [400/782], Loss: 1.5050, LR: 0.025000\n",
      "Epoch [43], Batch [450/782], Loss: 1.2563, LR: 0.025000\n",
      "Epoch [43], Batch [500/782], Loss: 1.4323, LR: 0.025000\n",
      "Epoch [43], Batch [550/782], Loss: 2.8390, LR: 0.025000\n",
      "Epoch [43], Batch [600/782], Loss: 1.3249, LR: 0.025000\n",
      "Epoch [43], Batch [650/782], Loss: 2.9827, LR: 0.025000\n",
      "Epoch [43], Batch [700/782], Loss: 3.3095, LR: 0.025000\n",
      "Epoch [43], Batch [750/782], Loss: 3.0200, LR: 0.025000\n",
      "Train set: Epoch: 43, Average loss:2.0300, LR: 0.025000 Top-1 Accuracy: 53.7960%, Top-5 Accuracy: 81.2040%, Time consumed:673.88s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|████████████▌                                                                           | 43/300 [8:21:40<50:04:01, 701.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Epoch: 43, Average loss:1.1546, Top-1 Accuracy: 67.3400%, Top-5 Accuracy: 91.2800%, Time consumed:30.28s\n",
      "\n",
      "EarlyStopping 카운터: 3 / 30\n",
      "Epoch [44], Batch [50/782], Loss: 2.9075, LR: 0.025000\n",
      "Epoch [44], Batch [100/782], Loss: 3.3374, LR: 0.025000\n",
      "Epoch [44], Batch [150/782], Loss: 1.1652, LR: 0.025000\n",
      "Epoch [44], Batch [200/782], Loss: 3.1531, LR: 0.025000\n",
      "Epoch [44], Batch [250/782], Loss: 1.4848, LR: 0.025000\n",
      "Epoch [44], Batch [300/782], Loss: 1.4458, LR: 0.025000\n",
      "Epoch [44], Batch [350/782], Loss: 3.0289, LR: 0.025000\n",
      "Epoch [44], Batch [400/782], Loss: 1.1062, LR: 0.025000\n",
      "Epoch [44], Batch [450/782], Loss: 2.4555, LR: 0.025000\n",
      "Epoch [44], Batch [500/782], Loss: 3.1622, LR: 0.025000\n",
      "Epoch [44], Batch [550/782], Loss: 3.2575, LR: 0.025000\n",
      "Epoch [44], Batch [600/782], Loss: 1.1704, LR: 0.025000\n",
      "Epoch [44], Batch [650/782], Loss: 1.3446, LR: 0.025000\n",
      "Epoch [44], Batch [700/782], Loss: 3.0340, LR: 0.025000\n",
      "Epoch [44], Batch [750/782], Loss: 1.0381, LR: 0.025000\n",
      "Train set: Epoch: 44, Average loss:1.9816, LR: 0.025000 Top-1 Accuracy: 54.4260%, Top-5 Accuracy: 81.8680%, Time consumed:673.71s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|████████████▉                                                                           | 44/300 [8:33:23<49:54:37, 701.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Epoch: 44, Average loss:1.2199, Top-1 Accuracy: 65.5600%, Top-5 Accuracy: 90.3200%, Time consumed:29.40s\n",
      "\n",
      "EarlyStopping 카운터: 4 / 30\n",
      "Epoch [45], Batch [50/782], Loss: 1.8119, LR: 0.025000\n",
      "Epoch [45], Batch [100/782], Loss: 1.8380, LR: 0.025000\n",
      "Epoch [45], Batch [150/782], Loss: 1.0052, LR: 0.025000\n",
      "Epoch [45], Batch [200/782], Loss: 1.1200, LR: 0.025000\n",
      "Epoch [45], Batch [250/782], Loss: 2.7679, LR: 0.025000\n",
      "Epoch [45], Batch [300/782], Loss: 1.2333, LR: 0.025000\n",
      "Epoch [45], Batch [350/782], Loss: 1.3384, LR: 0.025000\n",
      "Epoch [45], Batch [400/782], Loss: 2.6581, LR: 0.025000\n",
      "Epoch [45], Batch [450/782], Loss: 2.2557, LR: 0.025000\n",
      "Epoch [45], Batch [500/782], Loss: 3.0350, LR: 0.025000\n",
      "Epoch [45], Batch [550/782], Loss: 1.2703, LR: 0.025000\n",
      "Epoch [45], Batch [600/782], Loss: 3.3201, LR: 0.025000\n",
      "Epoch [45], Batch [650/782], Loss: 1.4199, LR: 0.025000\n",
      "Epoch [45], Batch [700/782], Loss: 2.8690, LR: 0.025000\n",
      "Epoch [45], Batch [750/782], Loss: 3.0070, LR: 0.025000\n",
      "Train set: Epoch: 45, Average loss:2.0080, LR: 0.025000 Top-1 Accuracy: 53.9260%, Top-5 Accuracy: 81.0500%, Time consumed:673.02s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█████████████▏                                                                          | 45/300 [8:45:06<49:43:30, 702.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Epoch: 45, Average loss:1.1459, Top-1 Accuracy: 67.0600%, Top-5 Accuracy: 91.2800%, Time consumed:29.29s\n",
      "\n",
      "EarlyStopping 카운터: 5 / 30\n",
      "Epoch [46], Batch [50/782], Loss: 2.9201, LR: 0.025000\n",
      "Epoch [46], Batch [100/782], Loss: 3.2968, LR: 0.025000\n",
      "Epoch [46], Batch [150/782], Loss: 2.5882, LR: 0.025000\n",
      "Epoch [46], Batch [200/782], Loss: 1.2303, LR: 0.025000\n",
      "Epoch [46], Batch [250/782], Loss: 3.1495, LR: 0.025000\n",
      "Epoch [46], Batch [300/782], Loss: 2.8088, LR: 0.025000\n",
      "Epoch [46], Batch [350/782], Loss: 1.2471, LR: 0.025000\n",
      "Epoch [46], Batch [400/782], Loss: 1.2831, LR: 0.025000\n",
      "Epoch [46], Batch [450/782], Loss: 2.4153, LR: 0.025000\n",
      "Epoch [46], Batch [500/782], Loss: 3.2486, LR: 0.025000\n",
      "Epoch [46], Batch [550/782], Loss: 2.9977, LR: 0.025000\n",
      "Epoch [46], Batch [600/782], Loss: 0.9814, LR: 0.025000\n",
      "Epoch [46], Batch [650/782], Loss: 3.1809, LR: 0.025000\n",
      "Epoch [46], Batch [700/782], Loss: 2.8292, LR: 0.025000\n",
      "Epoch [46], Batch [750/782], Loss: 3.2367, LR: 0.025000\n",
      "Train set: Epoch: 46, Average loss:1.9147, LR: 0.025000 Top-1 Accuracy: 55.8480%, Top-5 Accuracy: 82.6400%, Time consumed:666.90s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█████████████▍                                                                          | 46/300 [8:56:41<49:24:04, 700.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Epoch: 46, Average loss:1.1354, Top-1 Accuracy: 67.2300%, Top-5 Accuracy: 91.4100%, Time consumed:28.99s\n",
      "\n",
      "EarlyStopping 카운터: 6 / 30\n",
      "Epoch [47], Batch [50/782], Loss: 0.8822, LR: 0.012500\n",
      "Epoch [47], Batch [100/782], Loss: 3.0018, LR: 0.012500\n",
      "Epoch [47], Batch [150/782], Loss: 0.9420, LR: 0.012500\n",
      "Epoch [47], Batch [200/782], Loss: 1.1011, LR: 0.012500\n",
      "Epoch [47], Batch [250/782], Loss: 1.0091, LR: 0.012500\n",
      "Epoch [47], Batch [300/782], Loss: 2.7343, LR: 0.012500\n",
      "Epoch [47], Batch [350/782], Loss: 1.1302, LR: 0.012500\n",
      "Epoch [47], Batch [400/782], Loss: 1.0523, LR: 0.012500\n",
      "Epoch [47], Batch [450/782], Loss: 1.1909, LR: 0.012500\n",
      "Epoch [47], Batch [500/782], Loss: 1.4297, LR: 0.012500\n",
      "Epoch [47], Batch [550/782], Loss: 1.0831, LR: 0.012500\n",
      "Epoch [47], Batch [600/782], Loss: 2.6647, LR: 0.012500\n",
      "Epoch [47], Batch [650/782], Loss: 0.8941, LR: 0.012500\n",
      "Epoch [47], Batch [700/782], Loss: 1.0507, LR: 0.012500\n",
      "Epoch [47], Batch [750/782], Loss: 2.7292, LR: 0.012500\n",
      "Train set: Epoch: 47, Average loss:1.8385, LR: 0.012500 Top-1 Accuracy: 58.7220%, Top-5 Accuracy: 84.2660%, Time consumed:672.71s\n",
      "Test set: Epoch: 47, Average loss:0.9731, Top-1 Accuracy: 72.0200%, Top-5 Accuracy: 93.4700%, Time consumed:29.99s\n",
      "\n",
      "새로운 최고 top-1 정확도: 72.02%, top-5 정확도: 93.47%\n",
      "새로운 최고 top-5 정확도: 93.47%\n",
      "Accuracy improved (67.80% --> 72.02%). Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█████████████▊                                                                          | 47/300 [9:08:25<49:16:27, 701.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [48], Batch [50/782], Loss: 1.2813, LR: 0.012500\n",
      "Epoch [48], Batch [100/782], Loss: 0.9767, LR: 0.012500\n",
      "Epoch [48], Batch [150/782], Loss: 2.1030, LR: 0.012500\n",
      "Epoch [48], Batch [200/782], Loss: 2.6230, LR: 0.012500\n",
      "Epoch [48], Batch [250/782], Loss: 0.9053, LR: 0.012500\n",
      "Epoch [48], Batch [300/782], Loss: 2.7483, LR: 0.012500\n",
      "Epoch [48], Batch [350/782], Loss: 1.0150, LR: 0.012500\n",
      "Epoch [48], Batch [400/782], Loss: 2.4270, LR: 0.012500\n",
      "Epoch [48], Batch [450/782], Loss: 1.8716, LR: 0.012500\n",
      "Epoch [48], Batch [500/782], Loss: 1.1186, LR: 0.012500\n",
      "Epoch [48], Batch [550/782], Loss: 0.7298, LR: 0.012500\n",
      "Epoch [48], Batch [600/782], Loss: 1.1680, LR: 0.012500\n",
      "Epoch [48], Batch [650/782], Loss: 1.1862, LR: 0.012500\n",
      "Epoch [48], Batch [700/782], Loss: 1.2259, LR: 0.012500\n",
      "Epoch [48], Batch [750/782], Loss: 2.4148, LR: 0.012500\n",
      "Train set: Epoch: 48, Average loss:1.7563, LR: 0.012500 Top-1 Accuracy: 60.7740%, Top-5 Accuracy: 85.7820%, Time consumed:670.76s\n",
      "Test set: Epoch: 48, Average loss:0.9450, Top-1 Accuracy: 72.5700%, Top-5 Accuracy: 93.7300%, Time consumed:29.02s\n",
      "\n",
      "새로운 최고 top-1 정확도: 72.57%, top-5 정확도: 93.73%\n",
      "새로운 최고 top-5 정확도: 93.73%\n",
      "Accuracy improved (72.02% --> 72.57%). Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|██████████████                                                                          | 48/300 [9:20:05<49:03:57, 700.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [49], Batch [50/782], Loss: 0.9446, LR: 0.012500\n",
      "Epoch [49], Batch [100/782], Loss: 2.8555, LR: 0.012500\n",
      "Epoch [49], Batch [150/782], Loss: 1.2451, LR: 0.012500\n",
      "Epoch [49], Batch [200/782], Loss: 2.8737, LR: 0.012500\n",
      "Epoch [49], Batch [250/782], Loss: 1.0622, LR: 0.012500\n",
      "Epoch [49], Batch [300/782], Loss: 1.2191, LR: 0.012500\n",
      "Epoch [49], Batch [350/782], Loss: 1.1612, LR: 0.012500\n",
      "Epoch [49], Batch [400/782], Loss: 1.2436, LR: 0.012500\n",
      "Epoch [49], Batch [450/782], Loss: 2.8708, LR: 0.012500\n",
      "Epoch [49], Batch [500/782], Loss: 1.2184, LR: 0.012500\n",
      "Epoch [49], Batch [550/782], Loss: 1.8248, LR: 0.012500\n",
      "Epoch [49], Batch [600/782], Loss: 2.9814, LR: 0.012500\n",
      "Epoch [49], Batch [650/782], Loss: 1.2655, LR: 0.012500\n",
      "Epoch [49], Batch [700/782], Loss: 1.1108, LR: 0.012500\n",
      "Epoch [49], Batch [750/782], Loss: 1.1344, LR: 0.012500\n",
      "Train set: Epoch: 49, Average loss:1.7903, LR: 0.012500 Top-1 Accuracy: 60.0640%, Top-5 Accuracy: 85.1300%, Time consumed:673.00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|██████████████▎                                                                         | 49/300 [9:31:47<48:53:37, 701.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Epoch: 49, Average loss:0.9501, Top-1 Accuracy: 72.2800%, Top-5 Accuracy: 93.4700%, Time consumed:29.00s\n",
      "\n",
      "EarlyStopping 카운터: 1 / 30\n",
      "Epoch [50], Batch [50/782], Loss: 0.9604, LR: 0.012500\n",
      "Epoch [50], Batch [100/782], Loss: 0.9860, LR: 0.012500\n",
      "Epoch [50], Batch [150/782], Loss: 2.6469, LR: 0.012500\n",
      "Epoch [50], Batch [200/782], Loss: 0.9758, LR: 0.012500\n",
      "Epoch [50], Batch [250/782], Loss: 1.0585, LR: 0.012500\n",
      "Epoch [50], Batch [300/782], Loss: 1.0781, LR: 0.012500\n",
      "Epoch [50], Batch [350/782], Loss: 1.7679, LR: 0.012500\n",
      "Epoch [50], Batch [400/782], Loss: 1.1141, LR: 0.012500\n",
      "Epoch [50], Batch [450/782], Loss: 2.7442, LR: 0.012500\n",
      "Epoch [50], Batch [500/782], Loss: 2.0461, LR: 0.012500\n",
      "Epoch [50], Batch [550/782], Loss: 2.7524, LR: 0.012500\n",
      "Epoch [50], Batch [600/782], Loss: 1.7326, LR: 0.012500\n",
      "Epoch [50], Batch [650/782], Loss: 0.9792, LR: 0.012500\n",
      "Epoch [50], Batch [700/782], Loss: 3.0350, LR: 0.012500\n",
      "Epoch [50], Batch [750/782], Loss: 0.9026, LR: 0.012500\n",
      "Train set: Epoch: 50, Average loss:1.7514, LR: 0.012500 Top-1 Accuracy: 60.9480%, Top-5 Accuracy: 85.5360%, Time consumed:669.88s\n",
      "Test set: Epoch: 50, Average loss:0.9424, Top-1 Accuracy: 72.6400%, Top-5 Accuracy: 93.8000%, Time consumed:30.75s\n",
      "\n",
      "새로운 최고 top-1 정확도: 72.64%, top-5 정확도: 93.80%\n",
      "새로운 최고 top-5 정확도: 93.80%\n",
      "Accuracy improved (72.57% --> 72.64%). Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|██████████████▋                                                                         | 50/300 [9:43:29<48:41:59, 701.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [51], Batch [50/782], Loss: 1.6188, LR: 0.012500\n",
      "Epoch [51], Batch [100/782], Loss: 2.3907, LR: 0.012500\n",
      "Epoch [51], Batch [150/782], Loss: 0.9532, LR: 0.012500\n",
      "Epoch [51], Batch [200/782], Loss: 2.8911, LR: 0.012500\n",
      "Epoch [51], Batch [250/782], Loss: 1.1789, LR: 0.012500\n",
      "Epoch [51], Batch [300/782], Loss: 0.9587, LR: 0.012500\n",
      "Epoch [51], Batch [350/782], Loss: 2.9449, LR: 0.012500\n",
      "Epoch [51], Batch [400/782], Loss: 1.1405, LR: 0.012500\n",
      "Epoch [51], Batch [450/782], Loss: 0.8727, LR: 0.012500\n",
      "Epoch [51], Batch [500/782], Loss: 2.5468, LR: 0.012500\n",
      "Epoch [51], Batch [550/782], Loss: 2.4877, LR: 0.012500\n",
      "Epoch [51], Batch [600/782], Loss: 1.0908, LR: 0.012500\n",
      "Epoch [51], Batch [650/782], Loss: 2.9722, LR: 0.012500\n",
      "Epoch [51], Batch [700/782], Loss: 1.0100, LR: 0.012500\n",
      "Epoch [51], Batch [750/782], Loss: 1.6679, LR: 0.012500\n",
      "Train set: Epoch: 51, Average loss:1.7645, LR: 0.012500 Top-1 Accuracy: 60.3520%, Top-5 Accuracy: 85.1440%, Time consumed:665.62s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|██████████████▉                                                                         | 51/300 [9:55:03<48:21:56, 699.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Epoch: 51, Average loss:0.9587, Top-1 Accuracy: 72.1600%, Top-5 Accuracy: 93.5200%, Time consumed:28.93s\n",
      "\n",
      "EarlyStopping 카운터: 1 / 30\n",
      "Epoch [52], Batch [50/782], Loss: 1.0821, LR: 0.012500\n",
      "Epoch [52], Batch [100/782], Loss: 0.9739, LR: 0.012500\n",
      "Epoch [52], Batch [150/782], Loss: 1.2320, LR: 0.012500\n",
      "Epoch [52], Batch [200/782], Loss: 1.4236, LR: 0.012500\n",
      "Epoch [52], Batch [250/782], Loss: 3.1043, LR: 0.012500\n",
      "Epoch [52], Batch [300/782], Loss: 2.5820, LR: 0.012500\n",
      "Epoch [52], Batch [350/782], Loss: 1.9181, LR: 0.012500\n",
      "Epoch [52], Batch [400/782], Loss: 2.6557, LR: 0.012500\n",
      "Epoch [52], Batch [450/782], Loss: 2.4130, LR: 0.012500\n",
      "Epoch [52], Batch [500/782], Loss: 0.9353, LR: 0.012500\n",
      "Epoch [52], Batch [550/782], Loss: 0.7489, LR: 0.012500\n",
      "Epoch [52], Batch [600/782], Loss: 2.4481, LR: 0.012500\n",
      "Epoch [52], Batch [650/782], Loss: 2.8192, LR: 0.012500\n",
      "Epoch [52], Batch [700/782], Loss: 2.9915, LR: 0.012500\n",
      "Epoch [52], Batch [750/782], Loss: 0.7376, LR: 0.012500\n",
      "Train set: Epoch: 52, Average loss:1.7707, LR: 0.012500 Top-1 Accuracy: 60.4240%, Top-5 Accuracy: 85.5480%, Time consumed:666.71s\n",
      "Test set: Epoch: 52, Average loss:0.9368, Top-1 Accuracy: 72.8800%, Top-5 Accuracy: 93.8700%, Time consumed:29.08s\n",
      "\n",
      "새로운 최고 top-1 정확도: 72.88%, top-5 정확도: 93.87%\n",
      "새로운 최고 top-5 정확도: 93.87%\n",
      "Accuracy improved (72.64% --> 72.88%). Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|███████████████                                                                        | 52/300 [10:06:40<48:06:48, 698.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [53], Batch [50/782], Loss: 3.2265, LR: 0.012500\n",
      "Epoch [53], Batch [100/782], Loss: 1.1559, LR: 0.012500\n",
      "Epoch [53], Batch [150/782], Loss: 2.2462, LR: 0.012500\n",
      "Epoch [53], Batch [200/782], Loss: 2.5734, LR: 0.012500\n",
      "Epoch [53], Batch [250/782], Loss: 0.9955, LR: 0.012500\n",
      "Epoch [53], Batch [300/782], Loss: 2.5893, LR: 0.012500\n",
      "Epoch [53], Batch [350/782], Loss: 0.8022, LR: 0.012500\n",
      "Epoch [53], Batch [400/782], Loss: 2.8574, LR: 0.012500\n",
      "Epoch [53], Batch [450/782], Loss: 2.2145, LR: 0.012500\n",
      "Epoch [53], Batch [500/782], Loss: 0.8910, LR: 0.012500\n",
      "Epoch [53], Batch [550/782], Loss: 3.1737, LR: 0.012500\n",
      "Epoch [53], Batch [600/782], Loss: 0.8682, LR: 0.012500\n",
      "Epoch [53], Batch [650/782], Loss: 1.0030, LR: 0.012500\n",
      "Epoch [53], Batch [700/782], Loss: 1.2256, LR: 0.012500\n",
      "Epoch [53], Batch [750/782], Loss: 2.7113, LR: 0.012500\n",
      "Train set: Epoch: 53, Average loss:1.7991, LR: 0.012500 Top-1 Accuracy: 59.9920%, Top-5 Accuracy: 85.0420%, Time consumed:667.75s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|███████████████▎                                                                       | 53/300 [10:18:16<47:53:04, 697.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Epoch: 53, Average loss:0.9719, Top-1 Accuracy: 72.2300%, Top-5 Accuracy: 93.1400%, Time consumed:28.95s\n",
      "\n",
      "EarlyStopping 카운터: 1 / 30\n",
      "Epoch [54], Batch [50/782], Loss: 0.9076, LR: 0.012500\n",
      "Epoch [54], Batch [100/782], Loss: 0.6543, LR: 0.012500\n",
      "Epoch [54], Batch [150/782], Loss: 1.1107, LR: 0.012500\n",
      "Epoch [54], Batch [200/782], Loss: 0.7947, LR: 0.012500\n",
      "Epoch [54], Batch [250/782], Loss: 3.0759, LR: 0.012500\n",
      "Epoch [54], Batch [300/782], Loss: 0.8441, LR: 0.012500\n",
      "Epoch [54], Batch [350/782], Loss: 3.0880, LR: 0.012500\n",
      "Epoch [54], Batch [400/782], Loss: 2.9604, LR: 0.012500\n",
      "Epoch [54], Batch [450/782], Loss: 1.1058, LR: 0.012500\n",
      "Epoch [54], Batch [500/782], Loss: 0.9317, LR: 0.012500\n",
      "Epoch [54], Batch [550/782], Loss: 2.9514, LR: 0.012500\n",
      "Epoch [54], Batch [600/782], Loss: 1.0727, LR: 0.012500\n",
      "Epoch [54], Batch [650/782], Loss: 2.7496, LR: 0.012500\n",
      "Epoch [54], Batch [700/782], Loss: 2.4084, LR: 0.012500\n",
      "Epoch [54], Batch [750/782], Loss: 0.8760, LR: 0.012500\n",
      "Train set: Epoch: 54, Average loss:1.7355, LR: 0.012500 Top-1 Accuracy: 61.4920%, Top-5 Accuracy: 85.9620%, Time consumed:668.11s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|███████████████▋                                                                       | 54/300 [10:29:54<47:41:12, 697.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Epoch: 54, Average loss:0.9513, Top-1 Accuracy: 72.4800%, Top-5 Accuracy: 93.5600%, Time consumed:29.60s\n",
      "\n",
      "EarlyStopping 카운터: 2 / 30\n",
      "Epoch [55], Batch [50/782], Loss: 2.8631, LR: 0.012500\n",
      "Epoch [55], Batch [100/782], Loss: 2.8257, LR: 0.012500\n",
      "Epoch [55], Batch [150/782], Loss: 1.0712, LR: 0.012500\n",
      "Epoch [55], Batch [200/782], Loss: 3.1445, LR: 0.012500\n",
      "Epoch [55], Batch [250/782], Loss: 0.9836, LR: 0.012500\n",
      "Epoch [55], Batch [300/782], Loss: 3.1199, LR: 0.012500\n",
      "Epoch [55], Batch [350/782], Loss: 2.9226, LR: 0.012500\n",
      "Epoch [55], Batch [400/782], Loss: 2.7417, LR: 0.012500\n",
      "Epoch [55], Batch [450/782], Loss: 1.0181, LR: 0.012500\n",
      "Epoch [55], Batch [500/782], Loss: 1.8602, LR: 0.012500\n",
      "Epoch [55], Batch [550/782], Loss: 0.9898, LR: 0.012500\n",
      "Epoch [55], Batch [600/782], Loss: 1.0061, LR: 0.012500\n",
      "Epoch [55], Batch [650/782], Loss: 2.6527, LR: 0.012500\n",
      "Epoch [55], Batch [700/782], Loss: 2.0102, LR: 0.012500\n",
      "Epoch [55], Batch [750/782], Loss: 0.8351, LR: 0.012500\n",
      "Train set: Epoch: 55, Average loss:1.7770, LR: 0.012500 Top-1 Accuracy: 60.6500%, Top-5 Accuracy: 85.3280%, Time consumed:666.52s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|███████████████▉                                                                       | 55/300 [10:41:31<47:28:39, 697.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Epoch: 55, Average loss:0.9506, Top-1 Accuracy: 72.1000%, Top-5 Accuracy: 93.6400%, Time consumed:30.58s\n",
      "\n",
      "EarlyStopping 카운터: 3 / 30\n",
      "Epoch [56], Batch [50/782], Loss: 1.8574, LR: 0.012500\n",
      "Epoch [56], Batch [100/782], Loss: 0.6858, LR: 0.012500\n",
      "Epoch [56], Batch [150/782], Loss: 1.1484, LR: 0.012500\n",
      "Epoch [56], Batch [200/782], Loss: 1.5911, LR: 0.012500\n",
      "Epoch [56], Batch [250/782], Loss: 2.7519, LR: 0.012500\n",
      "Epoch [56], Batch [300/782], Loss: 1.1026, LR: 0.012500\n",
      "Epoch [56], Batch [350/782], Loss: 0.9846, LR: 0.012500\n",
      "Epoch [56], Batch [400/782], Loss: 0.9329, LR: 0.012500\n",
      "Epoch [56], Batch [450/782], Loss: 1.7486, LR: 0.012500\n",
      "Epoch [56], Batch [500/782], Loss: 2.7297, LR: 0.012500\n",
      "Epoch [56], Batch [550/782], Loss: 0.9189, LR: 0.012500\n",
      "Epoch [56], Batch [600/782], Loss: 0.8371, LR: 0.012500\n",
      "Epoch [56], Batch [650/782], Loss: 0.6842, LR: 0.012500\n",
      "Epoch [56], Batch [700/782], Loss: 1.0126, LR: 0.012500\n",
      "Epoch [56], Batch [750/782], Loss: 1.6799, LR: 0.012500\n",
      "Train set: Epoch: 56, Average loss:1.7042, LR: 0.012500 Top-1 Accuracy: 62.0360%, Top-5 Accuracy: 86.5520%, Time consumed:669.18s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|████████████████▏                                                                      | 56/300 [10:53:10<47:18:42, 698.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Epoch: 56, Average loss:0.9952, Top-1 Accuracy: 71.3800%, Top-5 Accuracy: 93.0200%, Time consumed:29.80s\n",
      "\n",
      "EarlyStopping 카운터: 4 / 30\n",
      "Epoch [57], Batch [50/782], Loss: 0.9695, LR: 0.012500\n",
      "Epoch [57], Batch [100/782], Loss: 0.7323, LR: 0.012500\n",
      "Epoch [57], Batch [150/782], Loss: 0.9319, LR: 0.012500\n",
      "Epoch [57], Batch [200/782], Loss: 2.2676, LR: 0.012500\n",
      "Epoch [57], Batch [250/782], Loss: 2.6499, LR: 0.012500\n",
      "Epoch [57], Batch [300/782], Loss: 1.7577, LR: 0.012500\n",
      "Epoch [57], Batch [350/782], Loss: 1.2518, LR: 0.012500\n",
      "Epoch [57], Batch [400/782], Loss: 1.0218, LR: 0.012500\n",
      "Epoch [57], Batch [450/782], Loss: 2.5238, LR: 0.012500\n",
      "Epoch [57], Batch [500/782], Loss: 2.7129, LR: 0.012500\n",
      "Epoch [57], Batch [550/782], Loss: 1.1057, LR: 0.012500\n",
      "Epoch [57], Batch [600/782], Loss: 0.7952, LR: 0.012500\n",
      "Epoch [57], Batch [650/782], Loss: 3.1117, LR: 0.012500\n",
      "Epoch [57], Batch [700/782], Loss: 0.9749, LR: 0.012500\n",
      "Epoch [57], Batch [750/782], Loss: 2.8629, LR: 0.012500\n",
      "Train set: Epoch: 57, Average loss:1.6839, LR: 0.012500 Top-1 Accuracy: 62.3560%, Top-5 Accuracy: 86.4640%, Time consumed:667.28s\n",
      "Test set: Epoch: 57, Average loss:0.8978, Top-1 Accuracy: 73.8000%, Top-5 Accuracy: 94.2900%, Time consumed:30.21s\n",
      "\n",
      "새로운 최고 top-1 정확도: 73.80%, top-5 정확도: 94.29%\n",
      "새로운 최고 top-5 정확도: 94.29%\n",
      "Accuracy improved (72.88% --> 73.80%). Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|████████████████▌                                                                      | 57/300 [11:04:48<47:07:11, 698.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [58], Batch [50/782], Loss: 0.9276, LR: 0.012500\n",
      "Epoch [58], Batch [100/782], Loss: 1.5815, LR: 0.012500\n",
      "Epoch [58], Batch [150/782], Loss: 0.7318, LR: 0.012500\n",
      "Epoch [58], Batch [200/782], Loss: 0.7471, LR: 0.012500\n",
      "Epoch [58], Batch [250/782], Loss: 1.1226, LR: 0.012500\n",
      "Epoch [58], Batch [300/782], Loss: 3.3766, LR: 0.012500\n",
      "Epoch [58], Batch [350/782], Loss: 0.7380, LR: 0.012500\n",
      "Epoch [58], Batch [400/782], Loss: 2.9416, LR: 0.012500\n",
      "Epoch [58], Batch [450/782], Loss: 1.2915, LR: 0.012500\n",
      "Epoch [58], Batch [500/782], Loss: 2.7968, LR: 0.012500\n",
      "Epoch [58], Batch [550/782], Loss: 0.9947, LR: 0.012500\n",
      "Epoch [58], Batch [600/782], Loss: 0.9531, LR: 0.012500\n",
      "Epoch [58], Batch [650/782], Loss: 2.6073, LR: 0.012500\n",
      "Epoch [58], Batch [700/782], Loss: 0.7988, LR: 0.012500\n",
      "Epoch [58], Batch [750/782], Loss: 2.7591, LR: 0.012500\n",
      "Train set: Epoch: 58, Average loss:1.7210, LR: 0.012500 Top-1 Accuracy: 61.3100%, Top-5 Accuracy: 85.8360%, Time consumed:667.98s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|████████████████▊                                                                      | 58/300 [11:16:26<46:54:32, 697.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Epoch: 58, Average loss:0.9028, Top-1 Accuracy: 73.2700%, Top-5 Accuracy: 94.2500%, Time consumed:29.24s\n",
      "\n",
      "EarlyStopping 카운터: 1 / 30\n",
      "Epoch [59], Batch [50/782], Loss: 1.5934, LR: 0.012500\n",
      "Epoch [59], Batch [100/782], Loss: 2.7784, LR: 0.012500\n",
      "Epoch [59], Batch [150/782], Loss: 1.9654, LR: 0.012500\n",
      "Epoch [59], Batch [200/782], Loss: 1.5151, LR: 0.012500\n",
      "Epoch [59], Batch [250/782], Loss: 0.8649, LR: 0.012500\n",
      "Epoch [59], Batch [300/782], Loss: 1.3016, LR: 0.012500\n",
      "Epoch [59], Batch [350/782], Loss: 0.8665, LR: 0.012500\n",
      "Epoch [59], Batch [400/782], Loss: 2.7481, LR: 0.012500\n",
      "Epoch [59], Batch [450/782], Loss: 1.4557, LR: 0.012500\n",
      "Epoch [59], Batch [500/782], Loss: 3.0849, LR: 0.012500\n",
      "Epoch [59], Batch [550/782], Loss: 1.0829, LR: 0.012500\n",
      "Epoch [59], Batch [600/782], Loss: 2.6131, LR: 0.012500\n",
      "Epoch [59], Batch [650/782], Loss: 2.9410, LR: 0.012500\n",
      "Epoch [59], Batch [700/782], Loss: 2.5735, LR: 0.012500\n",
      "Epoch [59], Batch [750/782], Loss: 2.2925, LR: 0.012500\n",
      "Train set: Epoch: 59, Average loss:1.6574, LR: 0.012500 Top-1 Accuracy: 62.6520%, Top-5 Accuracy: 86.9880%, Time consumed:666.13s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█████████████████                                                                      | 59/300 [11:28:03<46:42:09, 697.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Epoch: 59, Average loss:0.9155, Top-1 Accuracy: 73.0200%, Top-5 Accuracy: 94.1000%, Time consumed:31.06s\n",
      "\n",
      "EarlyStopping 카운터: 2 / 30\n",
      "Epoch [60], Batch [50/782], Loss: 2.9330, LR: 0.012500\n",
      "Epoch [60], Batch [100/782], Loss: 0.9409, LR: 0.012500\n",
      "Epoch [60], Batch [150/782], Loss: 2.4411, LR: 0.012500\n",
      "Epoch [60], Batch [200/782], Loss: 1.2580, LR: 0.012500\n",
      "Epoch [60], Batch [250/782], Loss: 1.2711, LR: 0.012500\n",
      "Epoch [60], Batch [300/782], Loss: 1.1995, LR: 0.012500\n",
      "Epoch [60], Batch [350/782], Loss: 1.9824, LR: 0.012500\n",
      "Epoch [60], Batch [400/782], Loss: 0.9950, LR: 0.012500\n",
      "Epoch [60], Batch [450/782], Loss: 3.0098, LR: 0.012500\n",
      "Epoch [60], Batch [500/782], Loss: 3.2127, LR: 0.012500\n",
      "Epoch [60], Batch [550/782], Loss: 0.9908, LR: 0.012500\n",
      "Epoch [60], Batch [600/782], Loss: 0.7428, LR: 0.012500\n",
      "Epoch [60], Batch [650/782], Loss: 1.0015, LR: 0.012500\n",
      "Epoch [60], Batch [700/782], Loss: 1.9834, LR: 0.012500\n",
      "Epoch [60], Batch [750/782], Loss: 0.8225, LR: 0.012500\n",
      "Train set: Epoch: 60, Average loss:1.7426, LR: 0.012500 Top-1 Accuracy: 61.0500%, Top-5 Accuracy: 85.7240%, Time consumed:667.46s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█████████████████▍                                                                     | 60/300 [11:39:39<46:29:10, 697.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Epoch: 60, Average loss:0.9289, Top-1 Accuracy: 73.2500%, Top-5 Accuracy: 93.7300%, Time consumed:29.03s\n",
      "\n",
      "EarlyStopping 카운터: 3 / 30\n",
      "Epoch [61], Batch [50/782], Loss: 0.7402, LR: 0.012500\n",
      "Epoch [61], Batch [100/782], Loss: 1.0406, LR: 0.012500\n",
      "Epoch [61], Batch [150/782], Loss: 2.8834, LR: 0.012500\n",
      "Epoch [61], Batch [200/782], Loss: 0.9112, LR: 0.012500\n",
      "Epoch [61], Batch [250/782], Loss: 2.5367, LR: 0.012500\n",
      "Epoch [61], Batch [300/782], Loss: 1.2774, LR: 0.012500\n",
      "Epoch [61], Batch [350/782], Loss: 0.9962, LR: 0.012500\n",
      "Epoch [61], Batch [400/782], Loss: 2.4282, LR: 0.012500\n",
      "Epoch [61], Batch [450/782], Loss: 1.8983, LR: 0.012500\n",
      "Epoch [61], Batch [500/782], Loss: 1.3346, LR: 0.012500\n",
      "Epoch [61], Batch [550/782], Loss: 1.1017, LR: 0.012500\n",
      "Epoch [61], Batch [600/782], Loss: 2.1007, LR: 0.012500\n",
      "Epoch [61], Batch [650/782], Loss: 1.5994, LR: 0.012500\n",
      "Epoch [61], Batch [700/782], Loss: 0.7938, LR: 0.012500\n",
      "Epoch [61], Batch [750/782], Loss: 1.9015, LR: 0.012500\n",
      "Train set: Epoch: 61, Average loss:1.6596, LR: 0.012500 Top-1 Accuracy: 62.6080%, Top-5 Accuracy: 86.8680%, Time consumed:669.29s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█████████████████▋                                                                     | 61/300 [11:51:18<46:18:43, 697.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Epoch: 61, Average loss:0.9327, Top-1 Accuracy: 73.3000%, Top-5 Accuracy: 93.4700%, Time consumed:28.96s\n",
      "\n",
      "EarlyStopping 카운터: 4 / 30\n",
      "Epoch [62], Batch [50/782], Loss: 0.8444, LR: 0.012500\n",
      "Epoch [62], Batch [100/782], Loss: 0.6490, LR: 0.012500\n",
      "Epoch [62], Batch [150/782], Loss: 0.8593, LR: 0.012500\n",
      "Epoch [62], Batch [200/782], Loss: 0.8877, LR: 0.012500\n",
      "Epoch [62], Batch [250/782], Loss: 2.0596, LR: 0.012500\n",
      "Epoch [62], Batch [300/782], Loss: 1.1862, LR: 0.012500\n",
      "Epoch [62], Batch [350/782], Loss: 0.9202, LR: 0.012500\n",
      "Epoch [62], Batch [400/782], Loss: 0.8824, LR: 0.012500\n",
      "Epoch [62], Batch [450/782], Loss: 2.9792, LR: 0.012500\n",
      "Epoch [62], Batch [500/782], Loss: 1.0138, LR: 0.012500\n",
      "Epoch [62], Batch [550/782], Loss: 1.1244, LR: 0.012500\n",
      "Epoch [62], Batch [600/782], Loss: 3.0025, LR: 0.012500\n",
      "Epoch [62], Batch [650/782], Loss: 0.8218, LR: 0.012500\n",
      "Epoch [62], Batch [700/782], Loss: 0.9039, LR: 0.012500\n",
      "Epoch [62], Batch [750/782], Loss: 2.4966, LR: 0.012500\n",
      "Train set: Epoch: 62, Average loss:1.7066, LR: 0.012500 Top-1 Accuracy: 61.8960%, Top-5 Accuracy: 86.1140%, Time consumed:673.99s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|█████████████████▉                                                                     | 62/300 [12:03:00<46:13:14, 699.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Epoch: 62, Average loss:0.9438, Top-1 Accuracy: 72.9200%, Top-5 Accuracy: 93.4000%, Time consumed:28.76s\n",
      "\n",
      "EarlyStopping 카운터: 5 / 30\n",
      "Epoch [63], Batch [50/782], Loss: 0.9385, LR: 0.012500\n",
      "Epoch [63], Batch [100/782], Loss: 0.9632, LR: 0.012500\n",
      "Epoch [63], Batch [150/782], Loss: 1.0800, LR: 0.012500\n",
      "Epoch [63], Batch [200/782], Loss: 0.8773, LR: 0.012500\n",
      "Epoch [63], Batch [250/782], Loss: 3.0356, LR: 0.012500\n",
      "Epoch [63], Batch [300/782], Loss: 0.8834, LR: 0.012500\n",
      "Epoch [63], Batch [350/782], Loss: 1.0591, LR: 0.012500\n",
      "Epoch [63], Batch [400/782], Loss: 1.0634, LR: 0.012500\n",
      "Epoch [63], Batch [450/782], Loss: 3.1902, LR: 0.012500\n",
      "Epoch [63], Batch [500/782], Loss: 0.7567, LR: 0.012500\n",
      "Epoch [63], Batch [550/782], Loss: 0.8370, LR: 0.012500\n",
      "Epoch [63], Batch [600/782], Loss: 0.9304, LR: 0.012500\n",
      "Epoch [63], Batch [650/782], Loss: 2.4995, LR: 0.012500\n",
      "Epoch [63], Batch [700/782], Loss: 1.1164, LR: 0.012500\n",
      "Epoch [63], Batch [750/782], Loss: 2.9934, LR: 0.012500\n",
      "Train set: Epoch: 63, Average loss:1.6290, LR: 0.012500 Top-1 Accuracy: 63.8880%, Top-5 Accuracy: 88.1440%, Time consumed:675.52s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██████████████████▎                                                                    | 63/300 [12:14:45<46:07:53, 700.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Epoch: 63, Average loss:0.9165, Top-1 Accuracy: 73.2500%, Top-5 Accuracy: 93.8800%, Time consumed:28.93s\n",
      "\n",
      "EarlyStopping 카운터: 6 / 30\n",
      "Epoch [64], Batch [50/782], Loss: 2.6102, LR: 0.006250\n",
      "Epoch [64], Batch [100/782], Loss: 3.0514, LR: 0.006250\n",
      "Epoch [64], Batch [150/782], Loss: 3.2764, LR: 0.006250\n",
      "Epoch [64], Batch [200/782], Loss: 2.7835, LR: 0.006250\n",
      "Epoch [64], Batch [250/782], Loss: 0.6634, LR: 0.006250\n",
      "Epoch [64], Batch [300/782], Loss: 1.8247, LR: 0.006250\n",
      "Epoch [64], Batch [350/782], Loss: 0.6945, LR: 0.006250\n",
      "Epoch [64], Batch [400/782], Loss: 1.5869, LR: 0.006250\n",
      "Epoch [64], Batch [450/782], Loss: 2.1429, LR: 0.006250\n",
      "Epoch [64], Batch [500/782], Loss: 2.2452, LR: 0.006250\n",
      "Epoch [64], Batch [550/782], Loss: 3.1929, LR: 0.006250\n",
      "Epoch [64], Batch [600/782], Loss: 0.6936, LR: 0.006250\n",
      "Epoch [64], Batch [650/782], Loss: 2.9733, LR: 0.006250\n",
      "Epoch [64], Batch [700/782], Loss: 2.3817, LR: 0.006250\n",
      "Epoch [64], Batch [750/782], Loss: 3.0200, LR: 0.006250\n",
      "Train set: Epoch: 64, Average loss:1.5863, LR: 0.006250 Top-1 Accuracy: 65.3000%, Top-5 Accuracy: 88.0980%, Time consumed:668.67s\n",
      "Test set: Epoch: 64, Average loss:0.8215, Top-1 Accuracy: 76.0400%, Top-5 Accuracy: 95.0600%, Time consumed:30.95s\n",
      "\n",
      "새로운 최고 top-1 정확도: 76.04%, top-5 정확도: 95.06%\n",
      "새로운 최고 top-5 정확도: 95.06%\n",
      "Accuracy improved (73.80% --> 76.04%). Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██████████████████▌                                                                    | 64/300 [12:26:25<45:55:40, 700.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [65], Batch [50/782], Loss: 1.0137, LR: 0.006250\n",
      "Epoch [65], Batch [100/782], Loss: 2.4525, LR: 0.006250\n",
      "Epoch [65], Batch [150/782], Loss: 0.6671, LR: 0.006250\n",
      "Epoch [65], Batch [200/782], Loss: 0.6865, LR: 0.006250\n",
      "Epoch [65], Batch [250/782], Loss: 0.8772, LR: 0.006250\n",
      "Epoch [65], Batch [300/782], Loss: 2.8514, LR: 0.006250\n",
      "Epoch [65], Batch [350/782], Loss: 0.6441, LR: 0.006250\n",
      "Epoch [65], Batch [400/782], Loss: 2.0470, LR: 0.006250\n",
      "Epoch [65], Batch [450/782], Loss: 2.4843, LR: 0.006250\n",
      "Epoch [65], Batch [500/782], Loss: 0.8438, LR: 0.006250\n",
      "Epoch [65], Batch [550/782], Loss: 0.7656, LR: 0.006250\n",
      "Epoch [65], Batch [600/782], Loss: 2.8998, LR: 0.006250\n",
      "Epoch [65], Batch [650/782], Loss: 0.5314, LR: 0.006250\n",
      "Epoch [65], Batch [700/782], Loss: 0.9945, LR: 0.006250\n",
      "Epoch [65], Batch [750/782], Loss: 0.6656, LR: 0.006250\n",
      "Train set: Epoch: 65, Average loss:1.5191, LR: 0.006250 Top-1 Accuracy: 66.7980%, Top-5 Accuracy: 88.7480%, Time consumed:667.63s\n",
      "Test set: Epoch: 65, Average loss:0.8065, Top-1 Accuracy: 76.2500%, Top-5 Accuracy: 94.8200%, Time consumed:28.95s\n",
      "\n",
      "새로운 최고 top-1 정확도: 76.25%, top-5 정확도: 94.82%\n",
      "Accuracy improved (76.04% --> 76.25%). Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██████████████████▊                                                                    | 65/300 [12:38:02<45:40:03, 699.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [66], Batch [50/782], Loss: 0.7237, LR: 0.006250\n",
      "Epoch [66], Batch [100/782], Loss: 0.7440, LR: 0.006250\n",
      "Epoch [66], Batch [150/782], Loss: 0.7921, LR: 0.006250\n",
      "Epoch [66], Batch [200/782], Loss: 0.9958, LR: 0.006250\n",
      "Epoch [66], Batch [250/782], Loss: 0.7701, LR: 0.006250\n",
      "Epoch [66], Batch [300/782], Loss: 0.9152, LR: 0.006250\n",
      "Epoch [66], Batch [350/782], Loss: 2.2245, LR: 0.006250\n",
      "Epoch [66], Batch [400/782], Loss: 2.0758, LR: 0.006250\n",
      "Epoch [66], Batch [450/782], Loss: 2.8531, LR: 0.006250\n",
      "Epoch [66], Batch [500/782], Loss: 0.6968, LR: 0.006250\n",
      "Epoch [66], Batch [550/782], Loss: 2.8180, LR: 0.006250\n",
      "Epoch [66], Batch [600/782], Loss: 2.8203, LR: 0.006250\n",
      "Epoch [66], Batch [650/782], Loss: 0.6908, LR: 0.006250\n",
      "Epoch [66], Batch [700/782], Loss: 0.7915, LR: 0.006250\n",
      "Epoch [66], Batch [750/782], Loss: 0.8466, LR: 0.006250\n",
      "Train set: Epoch: 66, Average loss:1.5189, LR: 0.006250 Top-1 Accuracy: 67.0180%, Top-5 Accuracy: 89.2520%, Time consumed:667.71s\n",
      "Test set: Epoch: 66, Average loss:0.7945, Top-1 Accuracy: 76.8000%, Top-5 Accuracy: 95.2000%, Time consumed:30.48s\n",
      "\n",
      "새로운 최고 top-1 정확도: 76.80%, top-5 정확도: 95.20%\n",
      "새로운 최고 top-5 정확도: 95.20%\n",
      "Accuracy improved (76.25% --> 76.80%). Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|███████████████████▏                                                                   | 66/300 [12:49:41<45:27:31, 699.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [67], Batch [50/782], Loss: 1.7613, LR: 0.006250\n",
      "Epoch [67], Batch [100/782], Loss: 2.6879, LR: 0.006250\n",
      "Epoch [67], Batch [150/782], Loss: 0.7342, LR: 0.006250\n",
      "Epoch [67], Batch [200/782], Loss: 2.3511, LR: 0.006250\n",
      "Epoch [67], Batch [250/782], Loss: 1.9252, LR: 0.006250\n",
      "Epoch [67], Batch [300/782], Loss: 2.0735, LR: 0.006250\n",
      "Epoch [67], Batch [350/782], Loss: 0.7129, LR: 0.006250\n",
      "Epoch [67], Batch [400/782], Loss: 0.5749, LR: 0.006250\n",
      "Epoch [67], Batch [450/782], Loss: 1.1071, LR: 0.006250\n",
      "Epoch [67], Batch [500/782], Loss: 1.0722, LR: 0.006250\n",
      "Epoch [67], Batch [550/782], Loss: 2.7970, LR: 0.006250\n",
      "Epoch [67], Batch [600/782], Loss: 0.8926, LR: 0.006250\n",
      "Epoch [67], Batch [650/782], Loss: 0.5459, LR: 0.006250\n",
      "Epoch [67], Batch [700/782], Loss: 2.2322, LR: 0.006250\n",
      "Epoch [67], Batch [750/782], Loss: 1.1930, LR: 0.006250\n",
      "Train set: Epoch: 67, Average loss:1.5060, LR: 0.006250 Top-1 Accuracy: 67.6120%, Top-5 Accuracy: 89.5500%, Time consumed:667.16s\n",
      "Test set: Epoch: 67, Average loss:0.7986, Top-1 Accuracy: 76.8500%, Top-5 Accuracy: 94.9600%, Time consumed:29.16s\n",
      "\n",
      "새로운 최고 top-1 정확도: 76.85%, top-5 정확도: 94.96%\n",
      "Accuracy improved (76.80% --> 76.85%). Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|███████████████████▍                                                                   | 67/300 [13:01:18<45:13:06, 698.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [68], Batch [50/782], Loss: 0.6572, LR: 0.006250\n",
      "Epoch [68], Batch [100/782], Loss: 2.5640, LR: 0.006250\n",
      "Epoch [68], Batch [150/782], Loss: 0.7151, LR: 0.006250\n",
      "Epoch [68], Batch [200/782], Loss: 2.4484, LR: 0.006250\n",
      "Epoch [68], Batch [250/782], Loss: 0.7221, LR: 0.006250\n",
      "Epoch [68], Batch [300/782], Loss: 0.6462, LR: 0.006250\n",
      "Epoch [68], Batch [350/782], Loss: 0.8037, LR: 0.006250\n",
      "Epoch [68], Batch [400/782], Loss: 0.5564, LR: 0.006250\n",
      "Epoch [68], Batch [450/782], Loss: 0.7155, LR: 0.006250\n",
      "Epoch [68], Batch [500/782], Loss: 2.1562, LR: 0.006250\n",
      "Epoch [68], Batch [550/782], Loss: 0.7396, LR: 0.006250\n",
      "Epoch [68], Batch [600/782], Loss: 0.7283, LR: 0.006250\n",
      "Epoch [68], Batch [650/782], Loss: 0.8932, LR: 0.006250\n",
      "Epoch [68], Batch [700/782], Loss: 2.7265, LR: 0.006250\n",
      "Epoch [68], Batch [750/782], Loss: 0.6463, LR: 0.006250\n",
      "Train set: Epoch: 68, Average loss:1.5152, LR: 0.006250 Top-1 Accuracy: 67.4640%, Top-5 Accuracy: 89.2900%, Time consumed:667.20s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|███████████████████▋                                                                   | 68/300 [13:12:56<45:00:16, 698.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Epoch: 68, Average loss:0.8143, Top-1 Accuracy: 76.5500%, Top-5 Accuracy: 94.9500%, Time consumed:30.42s\n",
      "\n",
      "EarlyStopping 카운터: 1 / 30\n",
      "Epoch [69], Batch [50/782], Loss: 2.4801, LR: 0.006250\n",
      "Epoch [69], Batch [100/782], Loss: 0.8273, LR: 0.006250\n",
      "Epoch [69], Batch [150/782], Loss: 1.2839, LR: 0.006250\n",
      "Epoch [69], Batch [200/782], Loss: 1.0508, LR: 0.006250\n",
      "Epoch [69], Batch [250/782], Loss: 2.4856, LR: 0.006250\n",
      "Epoch [69], Batch [300/782], Loss: 2.6487, LR: 0.006250\n",
      "Epoch [69], Batch [350/782], Loss: 0.6736, LR: 0.006250\n",
      "Epoch [69], Batch [400/782], Loss: 2.7442, LR: 0.006250\n",
      "Epoch [69], Batch [450/782], Loss: 2.9181, LR: 0.006250\n",
      "Epoch [69], Batch [500/782], Loss: 2.8759, LR: 0.006250\n",
      "Epoch [69], Batch [550/782], Loss: 0.5331, LR: 0.006250\n",
      "Epoch [69], Batch [600/782], Loss: 2.6502, LR: 0.006250\n",
      "Epoch [69], Batch [650/782], Loss: 0.7835, LR: 0.006250\n",
      "Epoch [69], Batch [700/782], Loss: 0.6791, LR: 0.006250\n",
      "Epoch [69], Batch [750/782], Loss: 1.6331, LR: 0.006250\n",
      "Train set: Epoch: 69, Average loss:1.4993, LR: 0.006250 Top-1 Accuracy: 67.1340%, Top-5 Accuracy: 89.2160%, Time consumed:665.07s\n",
      "Test set: Epoch: 69, Average loss:0.8126, Top-1 Accuracy: 76.9400%, Top-5 Accuracy: 95.0900%, Time consumed:28.88s\n",
      "\n",
      "새로운 최고 top-1 정확도: 76.94%, top-5 정확도: 95.09%\n",
      "Accuracy improved (76.85% --> 76.94%). Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|████████████████████                                                                   | 69/300 [13:24:30<44:44:20, 697.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [70], Batch [50/782], Loss: 0.5330, LR: 0.006250\n",
      "Epoch [70], Batch [100/782], Loss: 1.8973, LR: 0.006250\n",
      "Epoch [70], Batch [150/782], Loss: 2.4768, LR: 0.006250\n",
      "Epoch [70], Batch [200/782], Loss: 0.7263, LR: 0.006250\n",
      "Epoch [70], Batch [250/782], Loss: 2.8511, LR: 0.006250\n",
      "Epoch [70], Batch [300/782], Loss: 0.7925, LR: 0.006250\n",
      "Epoch [70], Batch [350/782], Loss: 2.1799, LR: 0.006250\n",
      "Epoch [70], Batch [400/782], Loss: 0.6944, LR: 0.006250\n",
      "Epoch [70], Batch [450/782], Loss: 2.6519, LR: 0.006250\n",
      "Epoch [70], Batch [500/782], Loss: 2.7687, LR: 0.006250\n",
      "Epoch [70], Batch [550/782], Loss: 0.7660, LR: 0.006250\n",
      "Epoch [70], Batch [600/782], Loss: 0.8661, LR: 0.006250\n",
      "Epoch [70], Batch [650/782], Loss: 2.5615, LR: 0.006250\n",
      "Epoch [70], Batch [700/782], Loss: 1.0773, LR: 0.006250\n",
      "Epoch [70], Batch [750/782], Loss: 2.3950, LR: 0.006250\n",
      "Train set: Epoch: 70, Average loss:1.5171, LR: 0.006250 Top-1 Accuracy: 67.2040%, Top-5 Accuracy: 89.1460%, Time consumed:667.10s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|████████████████████▎                                                                  | 70/300 [13:36:07<44:31:35, 696.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Epoch: 70, Average loss:0.7992, Top-1 Accuracy: 76.6700%, Top-5 Accuracy: 95.1600%, Time consumed:29.13s\n",
      "\n",
      "EarlyStopping 카운터: 1 / 30\n",
      "Epoch [71], Batch [50/782], Loss: 0.8915, LR: 0.006250\n",
      "Epoch [71], Batch [100/782], Loss: 1.9502, LR: 0.006250\n",
      "Epoch [71], Batch [150/782], Loss: 0.6921, LR: 0.006250\n",
      "Epoch [71], Batch [200/782], Loss: 1.0155, LR: 0.006250\n",
      "Epoch [71], Batch [250/782], Loss: 0.6103, LR: 0.006250\n",
      "Epoch [71], Batch [300/782], Loss: 1.8686, LR: 0.006250\n",
      "Epoch [71], Batch [350/782], Loss: 0.9200, LR: 0.006250\n",
      "Epoch [71], Batch [400/782], Loss: 3.0711, LR: 0.006250\n",
      "Epoch [71], Batch [450/782], Loss: 0.7863, LR: 0.006250\n",
      "Epoch [71], Batch [500/782], Loss: 0.7748, LR: 0.006250\n",
      "Epoch [71], Batch [550/782], Loss: 0.7303, LR: 0.006250\n",
      "Epoch [71], Batch [600/782], Loss: 1.6482, LR: 0.006250\n",
      "Epoch [71], Batch [650/782], Loss: 2.3372, LR: 0.006250\n",
      "Epoch [71], Batch [700/782], Loss: 2.5656, LR: 0.006250\n",
      "Epoch [71], Batch [750/782], Loss: 2.8382, LR: 0.006250\n",
      "Train set: Epoch: 71, Average loss:1.5036, LR: 0.006250 Top-1 Accuracy: 67.5100%, Top-5 Accuracy: 89.3860%, Time consumed:666.76s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|████████████████████▌                                                                  | 71/300 [13:47:42<44:18:30, 696.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Epoch: 71, Average loss:0.8225, Top-1 Accuracy: 76.7300%, Top-5 Accuracy: 94.8400%, Time consumed:28.89s\n",
      "\n",
      "EarlyStopping 카운터: 2 / 30\n",
      "Epoch [72], Batch [50/782], Loss: 0.9481, LR: 0.006250\n",
      "Epoch [72], Batch [100/782], Loss: 0.7658, LR: 0.006250\n",
      "Epoch [72], Batch [150/782], Loss: 0.8665, LR: 0.006250\n",
      "Epoch [72], Batch [200/782], Loss: 0.8377, LR: 0.006250\n",
      "Epoch [72], Batch [250/782], Loss: 2.9539, LR: 0.006250\n",
      "Epoch [72], Batch [300/782], Loss: 0.8622, LR: 0.006250\n",
      "Epoch [72], Batch [350/782], Loss: 2.2607, LR: 0.006250\n",
      "Epoch [72], Batch [400/782], Loss: 1.7199, LR: 0.006250\n",
      "Epoch [72], Batch [450/782], Loss: 0.6240, LR: 0.006250\n",
      "Epoch [72], Batch [500/782], Loss: 2.5517, LR: 0.006250\n",
      "Epoch [72], Batch [550/782], Loss: 0.8968, LR: 0.006250\n",
      "Epoch [72], Batch [600/782], Loss: 2.7568, LR: 0.006250\n",
      "Epoch [72], Batch [650/782], Loss: 0.6058, LR: 0.006250\n",
      "Epoch [72], Batch [700/782], Loss: 2.4334, LR: 0.006250\n",
      "Epoch [72], Batch [750/782], Loss: 0.5794, LR: 0.006250\n",
      "Train set: Epoch: 72, Average loss:1.4421, LR: 0.006250 Top-1 Accuracy: 69.0180%, Top-5 Accuracy: 90.2360%, Time consumed:667.16s\n",
      "Test set: Epoch: 72, Average loss:0.7851, Top-1 Accuracy: 77.4700%, Top-5 Accuracy: 95.1200%, Time consumed:29.07s\n",
      "\n",
      "새로운 최고 top-1 정확도: 77.47%, top-5 정확도: 95.12%\n",
      "Accuracy improved (76.94% --> 77.47%). Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|████████████████████▉                                                                  | 72/300 [13:59:19<44:07:16, 696.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [73], Batch [50/782], Loss: 0.7561, LR: 0.006250\n",
      "Epoch [73], Batch [100/782], Loss: 0.5615, LR: 0.006250\n",
      "Epoch [73], Batch [150/782], Loss: 1.2698, LR: 0.006250\n",
      "Epoch [73], Batch [200/782], Loss: 0.5496, LR: 0.006250\n",
      "Epoch [73], Batch [250/782], Loss: 1.9325, LR: 0.006250\n",
      "Epoch [73], Batch [300/782], Loss: 0.7300, LR: 0.006250\n",
      "Epoch [73], Batch [350/782], Loss: 0.5529, LR: 0.006250\n",
      "Epoch [73], Batch [400/782], Loss: 0.5731, LR: 0.006250\n",
      "Epoch [73], Batch [450/782], Loss: 0.9508, LR: 0.006250\n",
      "Epoch [73], Batch [500/782], Loss: 0.7250, LR: 0.006250\n",
      "Epoch [73], Batch [550/782], Loss: 2.5922, LR: 0.006250\n",
      "Epoch [73], Batch [600/782], Loss: 0.4582, LR: 0.006250\n",
      "Epoch [73], Batch [650/782], Loss: 1.7519, LR: 0.006250\n",
      "Epoch [73], Batch [700/782], Loss: 1.5297, LR: 0.006250\n",
      "Epoch [73], Batch [750/782], Loss: 0.5319, LR: 0.006250\n",
      "Train set: Epoch: 73, Average loss:1.4304, LR: 0.006250 Top-1 Accuracy: 69.1460%, Top-5 Accuracy: 90.4680%, Time consumed:669.25s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|█████████████████████▏                                                                 | 73/300 [14:10:57<43:56:54, 696.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Epoch: 73, Average loss:0.7817, Top-1 Accuracy: 77.2800%, Top-5 Accuracy: 95.1700%, Time consumed:28.48s\n",
      "\n",
      "EarlyStopping 카운터: 1 / 30\n",
      "Epoch [74], Batch [50/782], Loss: 0.5700, LR: 0.003125\n",
      "Epoch [74], Batch [100/782], Loss: 0.5835, LR: 0.003125\n",
      "Epoch [74], Batch [150/782], Loss: 2.5077, LR: 0.003125\n",
      "Epoch [74], Batch [200/782], Loss: 0.5163, LR: 0.003125\n",
      "Epoch [74], Batch [250/782], Loss: 2.8485, LR: 0.003125\n",
      "Epoch [74], Batch [300/782], Loss: 0.6048, LR: 0.003125\n",
      "Epoch [74], Batch [350/782], Loss: 0.5151, LR: 0.003125\n",
      "Epoch [74], Batch [400/782], Loss: 0.6355, LR: 0.003125\n",
      "Epoch [74], Batch [450/782], Loss: 0.5229, LR: 0.003125\n",
      "Epoch [74], Batch [500/782], Loss: 0.6070, LR: 0.003125\n",
      "Epoch [74], Batch [550/782], Loss: 0.5670, LR: 0.003125\n",
      "Epoch [74], Batch [600/782], Loss: 0.3894, LR: 0.003125\n",
      "Epoch [74], Batch [650/782], Loss: 2.1158, LR: 0.003125\n",
      "Epoch [74], Batch [700/782], Loss: 1.9360, LR: 0.003125\n",
      "Epoch [74], Batch [750/782], Loss: 0.3420, LR: 0.003125\n",
      "Train set: Epoch: 74, Average loss:1.3719, LR: 0.003125 Top-1 Accuracy: 71.2220%, Top-5 Accuracy: 91.1600%, Time consumed:666.68s\n",
      "Test set: Epoch: 74, Average loss:0.7176, Top-1 Accuracy: 79.0100%, Top-5 Accuracy: 95.8900%, Time consumed:30.03s\n",
      "\n",
      "새로운 최고 top-1 정확도: 79.01%, top-5 정확도: 95.89%\n",
      "새로운 최고 top-5 정확도: 95.89%\n",
      "Accuracy improved (77.47% --> 79.01%). Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|█████████████████████▍                                                                 | 74/300 [14:22:34<43:45:42, 697.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [75], Batch [50/782], Loss: 2.6542, LR: 0.003125\n",
      "Epoch [75], Batch [100/782], Loss: 2.7778, LR: 0.003125\n",
      "Epoch [75], Batch [150/782], Loss: 1.0774, LR: 0.003125\n",
      "Epoch [75], Batch [200/782], Loss: 2.4671, LR: 0.003125\n",
      "Epoch [75], Batch [250/782], Loss: 0.6534, LR: 0.003125\n",
      "Epoch [75], Batch [300/782], Loss: 0.5888, LR: 0.003125\n",
      "Epoch [75], Batch [350/782], Loss: 0.6536, LR: 0.003125\n",
      "Epoch [75], Batch [400/782], Loss: 3.0374, LR: 0.003125\n",
      "Epoch [75], Batch [450/782], Loss: 0.5025, LR: 0.003125\n",
      "Epoch [75], Batch [500/782], Loss: 0.6275, LR: 0.003125\n",
      "Epoch [75], Batch [550/782], Loss: 0.4898, LR: 0.003125\n",
      "Epoch [75], Batch [600/782], Loss: 2.2777, LR: 0.003125\n",
      "Epoch [75], Batch [650/782], Loss: 2.1861, LR: 0.003125\n",
      "Epoch [75], Batch [700/782], Loss: 0.6557, LR: 0.003125\n",
      "Epoch [75], Batch [750/782], Loss: 0.3290, LR: 0.003125\n",
      "Train set: Epoch: 75, Average loss:1.3315, LR: 0.003125 Top-1 Accuracy: 71.7580%, Top-5 Accuracy: 91.4760%, Time consumed:669.49s\n",
      "Test set: Epoch: 75, Average loss:0.7210, Top-1 Accuracy: 79.4300%, Top-5 Accuracy: 95.9200%, Time consumed:30.90s\n",
      "\n",
      "새로운 최고 top-1 정확도: 79.43%, top-5 정확도: 95.92%\n",
      "새로운 최고 top-5 정확도: 95.92%\n",
      "Accuracy improved (79.01% --> 79.43%). Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|█████████████████████▊                                                                 | 75/300 [14:34:15<43:38:34, 698.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [76], Batch [50/782], Loss: 0.4584, LR: 0.003125\n",
      "Epoch [76], Batch [100/782], Loss: 0.5505, LR: 0.003125\n",
      "Epoch [76], Batch [150/782], Loss: 2.6703, LR: 0.003125\n",
      "Epoch [76], Batch [200/782], Loss: 0.5500, LR: 0.003125\n",
      "Epoch [76], Batch [250/782], Loss: 0.4263, LR: 0.003125\n",
      "Epoch [76], Batch [300/782], Loss: 2.6889, LR: 0.003125\n",
      "Epoch [76], Batch [350/782], Loss: 3.0162, LR: 0.003125\n",
      "Epoch [76], Batch [400/782], Loss: 0.5524, LR: 0.003125\n",
      "Epoch [76], Batch [450/782], Loss: 0.5968, LR: 0.003125\n",
      "Epoch [76], Batch [500/782], Loss: 1.2919, LR: 0.003125\n",
      "Epoch [76], Batch [550/782], Loss: 0.3605, LR: 0.003125\n",
      "Epoch [76], Batch [600/782], Loss: 0.6539, LR: 0.003125\n",
      "Epoch [76], Batch [650/782], Loss: 0.7995, LR: 0.003125\n",
      "Epoch [76], Batch [700/782], Loss: 0.5634, LR: 0.003125\n",
      "Epoch [76], Batch [750/782], Loss: 1.6796, LR: 0.003125\n",
      "Train set: Epoch: 76, Average loss:1.3729, LR: 0.003125 Top-1 Accuracy: 71.0580%, Top-5 Accuracy: 90.8980%, Time consumed:668.93s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██████████████████████                                                                 | 76/300 [14:45:55<43:27:59, 698.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Epoch: 76, Average loss:0.7334, Top-1 Accuracy: 78.6700%, Top-5 Accuracy: 95.8100%, Time consumed:30.28s\n",
      "\n",
      "EarlyStopping 카운터: 1 / 30\n",
      "Epoch [77], Batch [50/782], Loss: 2.1087, LR: 0.003125\n",
      "Epoch [77], Batch [100/782], Loss: 0.5034, LR: 0.003125\n",
      "Epoch [77], Batch [150/782], Loss: 0.4336, LR: 0.003125\n",
      "Epoch [77], Batch [200/782], Loss: 2.2522, LR: 0.003125\n",
      "Epoch [77], Batch [250/782], Loss: 1.0139, LR: 0.003125\n",
      "Epoch [77], Batch [300/782], Loss: 0.4705, LR: 0.003125\n",
      "Epoch [77], Batch [350/782], Loss: 0.6349, LR: 0.003125\n",
      "Epoch [77], Batch [400/782], Loss: 0.6043, LR: 0.003125\n",
      "Epoch [77], Batch [450/782], Loss: 0.7030, LR: 0.003125\n",
      "Epoch [77], Batch [500/782], Loss: 0.6560, LR: 0.003125\n",
      "Epoch [77], Batch [550/782], Loss: 2.9388, LR: 0.003125\n",
      "Epoch [77], Batch [600/782], Loss: 0.5614, LR: 0.003125\n",
      "Epoch [77], Batch [650/782], Loss: 0.8234, LR: 0.003125\n",
      "Epoch [77], Batch [700/782], Loss: 0.4947, LR: 0.003125\n",
      "Epoch [77], Batch [750/782], Loss: 0.5177, LR: 0.003125\n",
      "Train set: Epoch: 77, Average loss:1.3118, LR: 0.003125 Top-1 Accuracy: 72.3420%, Top-5 Accuracy: 91.6820%, Time consumed:670.04s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██████████████████████▎                                                                | 77/300 [14:57:33<43:16:26, 698.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Epoch: 77, Average loss:0.7166, Top-1 Accuracy: 78.8900%, Top-5 Accuracy: 95.6400%, Time consumed:28.60s\n",
      "\n",
      "EarlyStopping 카운터: 2 / 30\n",
      "Epoch [78], Batch [50/782], Loss: 2.6777, LR: 0.003125\n",
      "Epoch [78], Batch [100/782], Loss: 1.3436, LR: 0.003125\n",
      "Epoch [78], Batch [150/782], Loss: 2.3857, LR: 0.003125\n",
      "Epoch [78], Batch [200/782], Loss: 1.4585, LR: 0.003125\n",
      "Epoch [78], Batch [250/782], Loss: 0.4863, LR: 0.003125\n",
      "Epoch [78], Batch [300/782], Loss: 2.4175, LR: 0.003125\n",
      "Epoch [78], Batch [350/782], Loss: 0.5187, LR: 0.003125\n",
      "Epoch [78], Batch [400/782], Loss: 2.4781, LR: 0.003125\n",
      "Epoch [78], Batch [450/782], Loss: 1.3885, LR: 0.003125\n",
      "Epoch [78], Batch [500/782], Loss: 2.8234, LR: 0.003125\n",
      "Epoch [78], Batch [550/782], Loss: 0.5347, LR: 0.003125\n",
      "Epoch [78], Batch [600/782], Loss: 0.7346, LR: 0.003125\n",
      "Epoch [78], Batch [650/782], Loss: 2.3286, LR: 0.003125\n",
      "Epoch [78], Batch [700/782], Loss: 0.4728, LR: 0.003125\n",
      "Epoch [78], Batch [750/782], Loss: 0.4609, LR: 0.003125\n",
      "Train set: Epoch: 78, Average loss:1.3305, LR: 0.003125 Top-1 Accuracy: 72.2720%, Top-5 Accuracy: 91.7220%, Time consumed:670.35s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██████████████████████▌                                                                | 78/300 [15:09:12<43:05:05, 698.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Epoch: 78, Average loss:0.7578, Top-1 Accuracy: 79.1800%, Top-5 Accuracy: 95.5800%, Time consumed:28.49s\n",
      "\n",
      "EarlyStopping 카운터: 3 / 30\n",
      "Epoch [79], Batch [50/782], Loss: 2.6241, LR: 0.003125\n",
      "Epoch [79], Batch [100/782], Loss: 2.2629, LR: 0.003125\n",
      "Epoch [79], Batch [150/782], Loss: 1.4618, LR: 0.003125\n",
      "Epoch [79], Batch [200/782], Loss: 0.4803, LR: 0.003125\n",
      "Epoch [79], Batch [250/782], Loss: 1.6173, LR: 0.003125\n",
      "Epoch [79], Batch [300/782], Loss: 2.7736, LR: 0.003125\n",
      "Epoch [79], Batch [350/782], Loss: 0.9274, LR: 0.003125\n",
      "Epoch [79], Batch [400/782], Loss: 0.8408, LR: 0.003125\n",
      "Epoch [79], Batch [450/782], Loss: 0.5647, LR: 0.003125\n",
      "Epoch [79], Batch [500/782], Loss: 0.7887, LR: 0.003125\n",
      "Epoch [79], Batch [550/782], Loss: 0.5487, LR: 0.003125\n",
      "Epoch [79], Batch [600/782], Loss: 1.9274, LR: 0.003125\n",
      "Epoch [79], Batch [650/782], Loss: 2.2055, LR: 0.003125\n",
      "Epoch [79], Batch [700/782], Loss: 0.5677, LR: 0.003125\n",
      "Epoch [79], Batch [750/782], Loss: 0.5841, LR: 0.003125\n",
      "Train set: Epoch: 79, Average loss:1.2945, LR: 0.003125 Top-1 Accuracy: 72.9000%, Top-5 Accuracy: 91.7860%, Time consumed:658.98s\n",
      "Test set: Epoch: 79, Average loss:0.7200, Top-1 Accuracy: 79.5400%, Top-5 Accuracy: 95.7200%, Time consumed:28.34s\n",
      "\n",
      "새로운 최고 top-1 정확도: 79.54%, top-5 정확도: 95.72%\n",
      "Accuracy improved (79.43% --> 79.54%). Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██████████████████████▉                                                                | 79/300 [15:20:40<42:41:36, 695.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [80], Batch [50/782], Loss: 2.5146, LR: 0.003125\n",
      "Epoch [80], Batch [100/782], Loss: 0.6685, LR: 0.003125\n",
      "Epoch [80], Batch [150/782], Loss: 2.6538, LR: 0.003125\n",
      "Epoch [80], Batch [200/782], Loss: 0.4220, LR: 0.003125\n",
      "Epoch [80], Batch [250/782], Loss: 0.9161, LR: 0.003125\n",
      "Epoch [80], Batch [300/782], Loss: 2.5947, LR: 0.003125\n",
      "Epoch [80], Batch [350/782], Loss: 0.6743, LR: 0.003125\n",
      "Epoch [80], Batch [400/782], Loss: 0.3933, LR: 0.003125\n",
      "Epoch [80], Batch [450/782], Loss: 0.4615, LR: 0.003125\n",
      "Epoch [80], Batch [500/782], Loss: 0.5705, LR: 0.003125\n",
      "Epoch [80], Batch [550/782], Loss: 0.3359, LR: 0.003125\n",
      "Epoch [80], Batch [600/782], Loss: 2.5099, LR: 0.003125\n",
      "Epoch [80], Batch [650/782], Loss: 0.4414, LR: 0.003125\n",
      "Epoch [80], Batch [700/782], Loss: 2.6587, LR: 0.003125\n",
      "Epoch [80], Batch [750/782], Loss: 1.9411, LR: 0.003125\n",
      "Train set: Epoch: 80, Average loss:1.3320, LR: 0.003125 Top-1 Accuracy: 72.6200%, Top-5 Accuracy: 91.6340%, Time consumed:666.83s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|███████████████████████▏                                                               | 80/300 [15:32:16<42:30:44, 695.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Epoch: 80, Average loss:0.7097, Top-1 Accuracy: 79.2600%, Top-5 Accuracy: 95.9500%, Time consumed:29.27s\n",
      "\n",
      "새로운 최고 top-5 정확도: 95.95%\n",
      "EarlyStopping 카운터: 1 / 30\n",
      "Epoch [81], Batch [50/782], Loss: 2.5016, LR: 0.001562\n",
      "Epoch [81], Batch [100/782], Loss: 2.5380, LR: 0.001562\n",
      "Epoch [81], Batch [150/782], Loss: 0.4943, LR: 0.001562\n",
      "Epoch [81], Batch [200/782], Loss: 0.5446, LR: 0.001562\n",
      "Epoch [81], Batch [250/782], Loss: 0.5737, LR: 0.001562\n",
      "Epoch [81], Batch [300/782], Loss: 0.6763, LR: 0.001562\n",
      "Epoch [81], Batch [350/782], Loss: 0.5448, LR: 0.001562\n",
      "Epoch [81], Batch [400/782], Loss: 1.5282, LR: 0.001562\n",
      "Epoch [81], Batch [450/782], Loss: 1.9272, LR: 0.001562\n",
      "Epoch [81], Batch [500/782], Loss: 1.7502, LR: 0.001562\n",
      "Epoch [81], Batch [550/782], Loss: 0.3976, LR: 0.001562\n",
      "Epoch [81], Batch [600/782], Loss: 2.3589, LR: 0.001562\n",
      "Epoch [81], Batch [650/782], Loss: 2.2378, LR: 0.001562\n",
      "Epoch [81], Batch [700/782], Loss: 0.6799, LR: 0.001562\n",
      "Epoch [81], Batch [750/782], Loss: 0.2777, LR: 0.001562\n",
      "Train set: Epoch: 81, Average loss:1.3035, LR: 0.001562 Top-1 Accuracy: 73.7160%, Top-5 Accuracy: 92.3160%, Time consumed:670.54s\n",
      "Test set: Epoch: 81, Average loss:0.6865, Top-1 Accuracy: 80.1500%, Top-5 Accuracy: 96.1300%, Time consumed:29.59s\n",
      "\n",
      "새로운 최고 top-1 정확도: 80.15%, top-5 정확도: 96.13%\n",
      "새로운 최고 top-5 정확도: 96.13%\n",
      "Accuracy improved (79.54% --> 80.15%). Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|███████████████████████▍                                                               | 81/300 [15:43:57<42:24:44, 697.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [82], Batch [50/782], Loss: 0.4295, LR: 0.001562\n",
      "Epoch [82], Batch [100/782], Loss: 0.3802, LR: 0.001562\n",
      "Epoch [82], Batch [150/782], Loss: 2.2453, LR: 0.001562\n",
      "Epoch [82], Batch [200/782], Loss: 0.3198, LR: 0.001562\n",
      "Epoch [82], Batch [250/782], Loss: 2.7531, LR: 0.001562\n",
      "Epoch [82], Batch [300/782], Loss: 0.3547, LR: 0.001562\n",
      "Epoch [82], Batch [350/782], Loss: 0.4990, LR: 0.001562\n",
      "Epoch [82], Batch [400/782], Loss: 0.8265, LR: 0.001562\n",
      "Epoch [82], Batch [450/782], Loss: 0.4144, LR: 0.001562\n",
      "Epoch [82], Batch [500/782], Loss: 2.9246, LR: 0.001562\n",
      "Epoch [82], Batch [550/782], Loss: 0.4150, LR: 0.001562\n",
      "Epoch [82], Batch [600/782], Loss: 0.3603, LR: 0.001562\n",
      "Epoch [82], Batch [650/782], Loss: 0.5377, LR: 0.001562\n",
      "Epoch [82], Batch [700/782], Loss: 0.4657, LR: 0.001562\n",
      "Epoch [82], Batch [750/782], Loss: 2.6074, LR: 0.001562\n",
      "Train set: Epoch: 82, Average loss:1.2516, LR: 0.001562 Top-1 Accuracy: 74.2900%, Top-5 Accuracy: 92.3980%, Time consumed:666.72s\n",
      "Test set: Epoch: 82, Average loss:0.6694, Top-1 Accuracy: 80.8000%, Top-5 Accuracy: 96.2400%, Time consumed:30.49s\n",
      "\n",
      "새로운 최고 top-1 정확도: 80.80%, top-5 정확도: 96.24%\n",
      "새로운 최고 top-5 정확도: 96.24%\n",
      "Accuracy improved (80.15% --> 80.80%). Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|███████████████████████▊                                                               | 82/300 [15:55:35<42:13:53, 697.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [83], Batch [50/782], Loss: 1.4042, LR: 0.001562\n",
      "Epoch [83], Batch [100/782], Loss: 2.1074, LR: 0.001562\n",
      "Epoch [83], Batch [150/782], Loss: 0.5235, LR: 0.001562\n",
      "Epoch [83], Batch [200/782], Loss: 0.2995, LR: 0.001562\n",
      "Epoch [83], Batch [250/782], Loss: 0.2870, LR: 0.001562\n",
      "Epoch [83], Batch [300/782], Loss: 0.3166, LR: 0.001562\n",
      "Epoch [83], Batch [350/782], Loss: 2.3381, LR: 0.001562\n",
      "Epoch [83], Batch [400/782], Loss: 1.6002, LR: 0.001562\n",
      "Epoch [83], Batch [450/782], Loss: 2.1458, LR: 0.001562\n",
      "Epoch [83], Batch [500/782], Loss: 0.4216, LR: 0.001562\n",
      "Epoch [83], Batch [550/782], Loss: 2.0582, LR: 0.001562\n",
      "Epoch [83], Batch [600/782], Loss: 0.7250, LR: 0.001562\n",
      "Epoch [83], Batch [650/782], Loss: 2.3495, LR: 0.001562\n",
      "Epoch [83], Batch [700/782], Loss: 1.9376, LR: 0.001562\n",
      "Epoch [83], Batch [750/782], Loss: 2.5001, LR: 0.001562\n",
      "Train set: Epoch: 83, Average loss:1.2783, LR: 0.001562 Top-1 Accuracy: 74.0600%, Top-5 Accuracy: 92.0360%, Time consumed:666.43s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|████████████████████████                                                               | 83/300 [16:07:10<42:00:20, 696.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Epoch: 83, Average loss:0.6645, Top-1 Accuracy: 80.6600%, Top-5 Accuracy: 96.4000%, Time consumed:29.19s\n",
      "\n",
      "새로운 최고 top-5 정확도: 96.40%\n",
      "EarlyStopping 카운터: 1 / 30\n",
      "Epoch [84], Batch [50/782], Loss: 2.4418, LR: 0.001562\n",
      "Epoch [84], Batch [100/782], Loss: 2.8605, LR: 0.001562\n",
      "Epoch [84], Batch [150/782], Loss: 0.3758, LR: 0.001562\n",
      "Epoch [84], Batch [200/782], Loss: 0.4225, LR: 0.001562\n",
      "Epoch [84], Batch [250/782], Loss: 0.2324, LR: 0.001562\n",
      "Epoch [84], Batch [300/782], Loss: 0.2680, LR: 0.001562\n",
      "Epoch [84], Batch [350/782], Loss: 2.5543, LR: 0.001562\n",
      "Epoch [84], Batch [400/782], Loss: 1.1632, LR: 0.001562\n",
      "Epoch [84], Batch [450/782], Loss: 0.4018, LR: 0.001562\n",
      "Epoch [84], Batch [500/782], Loss: 0.4340, LR: 0.001562\n",
      "Epoch [84], Batch [550/782], Loss: 2.0033, LR: 0.001562\n",
      "Epoch [84], Batch [600/782], Loss: 2.1189, LR: 0.001562\n",
      "Epoch [84], Batch [650/782], Loss: 2.6828, LR: 0.001562\n",
      "Epoch [84], Batch [700/782], Loss: 0.4718, LR: 0.001562\n",
      "Epoch [84], Batch [750/782], Loss: 0.5134, LR: 0.001562\n",
      "Train set: Epoch: 84, Average loss:1.2423, LR: 0.001562 Top-1 Accuracy: 74.8420%, Top-5 Accuracy: 92.5240%, Time consumed:668.09s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|████████████████████████▎                                                              | 84/300 [16:18:49<41:50:31, 697.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Epoch: 84, Average loss:0.6953, Top-1 Accuracy: 80.1700%, Top-5 Accuracy: 96.1700%, Time consumed:30.44s\n",
      "\n",
      "EarlyStopping 카운터: 2 / 30\n",
      "Epoch [85], Batch [50/782], Loss: 0.6666, LR: 0.001562\n",
      "Epoch [85], Batch [100/782], Loss: 2.2647, LR: 0.001562\n",
      "Epoch [85], Batch [150/782], Loss: 0.5133, LR: 0.001562\n",
      "Epoch [85], Batch [200/782], Loss: 0.3384, LR: 0.001562\n",
      "Epoch [85], Batch [250/782], Loss: 0.6430, LR: 0.001562\n",
      "Epoch [85], Batch [300/782], Loss: 0.4593, LR: 0.001562\n",
      "Epoch [85], Batch [350/782], Loss: 0.3104, LR: 0.001562\n",
      "Epoch [85], Batch [400/782], Loss: 0.3691, LR: 0.001562\n",
      "Epoch [85], Batch [450/782], Loss: 0.4014, LR: 0.001562\n",
      "Epoch [85], Batch [500/782], Loss: 1.0764, LR: 0.001562\n",
      "Epoch [85], Batch [550/782], Loss: 2.8856, LR: 0.001562\n",
      "Epoch [85], Batch [600/782], Loss: 0.3860, LR: 0.001562\n",
      "Epoch [85], Batch [650/782], Loss: 0.6048, LR: 0.001562\n",
      "Epoch [85], Batch [700/782], Loss: 2.3299, LR: 0.001562\n",
      "Epoch [85], Batch [750/782], Loss: 0.4164, LR: 0.001562\n",
      "Train set: Epoch: 85, Average loss:1.2074, LR: 0.001562 Top-1 Accuracy: 75.7380%, Top-5 Accuracy: 93.0700%, Time consumed:671.65s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|████████████████████████▋                                                              | 85/300 [16:30:30<41:42:58, 698.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Epoch: 85, Average loss:0.6809, Top-1 Accuracy: 80.5200%, Top-5 Accuracy: 96.0600%, Time consumed:29.49s\n",
      "\n",
      "EarlyStopping 카운터: 3 / 30\n",
      "Epoch [86], Batch [50/782], Loss: 2.3803, LR: 0.001562\n",
      "Epoch [86], Batch [100/782], Loss: 0.3527, LR: 0.001562\n",
      "Epoch [86], Batch [150/782], Loss: 0.8947, LR: 0.001562\n",
      "Epoch [86], Batch [200/782], Loss: 2.1666, LR: 0.001562\n",
      "Epoch [86], Batch [250/782], Loss: 0.4641, LR: 0.001562\n",
      "Epoch [86], Batch [300/782], Loss: 0.4560, LR: 0.001562\n",
      "Epoch [86], Batch [350/782], Loss: 0.3378, LR: 0.001562\n",
      "Epoch [86], Batch [400/782], Loss: 0.4146, LR: 0.001562\n",
      "Epoch [86], Batch [450/782], Loss: 0.3275, LR: 0.001562\n",
      "Epoch [86], Batch [500/782], Loss: 0.4055, LR: 0.001562\n",
      "Epoch [86], Batch [550/782], Loss: 0.4133, LR: 0.001562\n",
      "Epoch [86], Batch [600/782], Loss: 0.3503, LR: 0.001562\n",
      "Epoch [86], Batch [650/782], Loss: 1.9091, LR: 0.001562\n",
      "Epoch [86], Batch [700/782], Loss: 0.4448, LR: 0.001562\n",
      "Epoch [86], Batch [750/782], Loss: 0.4129, LR: 0.001562\n",
      "Train set: Epoch: 86, Average loss:1.2194, LR: 0.001562 Top-1 Accuracy: 75.8400%, Top-5 Accuracy: 93.1220%, Time consumed:667.89s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|████████████████████████▉                                                              | 86/300 [16:42:08<41:31:07, 698.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Epoch: 86, Average loss:0.6689, Top-1 Accuracy: 80.4000%, Top-5 Accuracy: 96.3300%, Time consumed:30.41s\n",
      "\n",
      "EarlyStopping 카운터: 4 / 30\n",
      "Epoch [87], Batch [50/782], Loss: 2.2218, LR: 0.001562\n",
      "Epoch [87], Batch [100/782], Loss: 2.4256, LR: 0.001562\n",
      "Epoch [87], Batch [150/782], Loss: 1.0141, LR: 0.001562\n",
      "Epoch [87], Batch [200/782], Loss: 0.4138, LR: 0.001562\n",
      "Epoch [87], Batch [250/782], Loss: 0.4618, LR: 0.001562\n",
      "Epoch [87], Batch [300/782], Loss: 0.2696, LR: 0.001562\n",
      "Epoch [87], Batch [350/782], Loss: 2.4627, LR: 0.001562\n",
      "Epoch [87], Batch [400/782], Loss: 1.7258, LR: 0.001562\n",
      "Epoch [87], Batch [450/782], Loss: 0.5951, LR: 0.001562\n",
      "Epoch [87], Batch [500/782], Loss: 0.2740, LR: 0.001562\n",
      "Epoch [87], Batch [550/782], Loss: 2.3819, LR: 0.001562\n",
      "Epoch [87], Batch [600/782], Loss: 0.4835, LR: 0.001562\n",
      "Epoch [87], Batch [650/782], Loss: 1.5979, LR: 0.001562\n",
      "Epoch [87], Batch [700/782], Loss: 0.4104, LR: 0.001562\n",
      "Epoch [87], Batch [750/782], Loss: 1.5561, LR: 0.001562\n",
      "Train set: Epoch: 87, Average loss:1.2627, LR: 0.001562 Top-1 Accuracy: 74.5640%, Top-5 Accuracy: 92.5540%, Time consumed:667.81s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|█████████████████████████▏                                                             | 87/300 [16:53:45<41:17:42, 697.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Epoch: 87, Average loss:0.6673, Top-1 Accuracy: 80.4400%, Top-5 Accuracy: 96.3000%, Time consumed:28.96s\n",
      "\n",
      "EarlyStopping 카운터: 5 / 30\n",
      "Epoch [88], Batch [50/782], Loss: 0.4082, LR: 0.000781\n",
      "Epoch [88], Batch [100/782], Loss: 1.8331, LR: 0.000781\n",
      "Epoch [88], Batch [150/782], Loss: 0.3313, LR: 0.000781\n",
      "Epoch [88], Batch [200/782], Loss: 1.9061, LR: 0.000781\n",
      "Epoch [88], Batch [250/782], Loss: 2.3140, LR: 0.000781\n",
      "Epoch [88], Batch [300/782], Loss: 1.0798, LR: 0.000781\n",
      "Epoch [88], Batch [350/782], Loss: 2.3529, LR: 0.000781\n",
      "Epoch [88], Batch [400/782], Loss: 0.3703, LR: 0.000781\n",
      "Epoch [88], Batch [450/782], Loss: 0.4072, LR: 0.000781\n",
      "Epoch [88], Batch [500/782], Loss: 0.3408, LR: 0.000781\n",
      "Epoch [88], Batch [550/782], Loss: 0.3529, LR: 0.000781\n",
      "Epoch [88], Batch [600/782], Loss: 0.3031, LR: 0.000781\n",
      "Epoch [88], Batch [650/782], Loss: 2.6567, LR: 0.000781\n",
      "Epoch [88], Batch [700/782], Loss: 2.0991, LR: 0.000781\n",
      "Epoch [88], Batch [750/782], Loss: 2.6166, LR: 0.000781\n",
      "Train set: Epoch: 88, Average loss:1.2311, LR: 0.000781 Top-1 Accuracy: 75.8520%, Top-5 Accuracy: 93.1280%, Time consumed:668.96s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|█████████████████████████▌                                                             | 88/300 [17:05:23<41:06:07, 697.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Epoch: 88, Average loss:0.6837, Top-1 Accuracy: 80.7200%, Top-5 Accuracy: 96.1800%, Time consumed:29.01s\n",
      "\n",
      "EarlyStopping 카운터: 6 / 30\n",
      "Epoch [89], Batch [50/782], Loss: 0.7266, LR: 0.000781\n",
      "Epoch [89], Batch [100/782], Loss: 2.4154, LR: 0.000781\n",
      "Epoch [89], Batch [150/782], Loss: 0.4127, LR: 0.000781\n",
      "Epoch [89], Batch [200/782], Loss: 0.4927, LR: 0.000781\n",
      "Epoch [89], Batch [250/782], Loss: 0.3509, LR: 0.000781\n",
      "Epoch [89], Batch [300/782], Loss: 0.2873, LR: 0.000781\n",
      "Epoch [89], Batch [350/782], Loss: 0.7658, LR: 0.000781\n",
      "Epoch [89], Batch [400/782], Loss: 1.4002, LR: 0.000781\n",
      "Epoch [89], Batch [450/782], Loss: 0.2705, LR: 0.000781\n",
      "Epoch [89], Batch [500/782], Loss: 0.3055, LR: 0.000781\n",
      "Epoch [89], Batch [550/782], Loss: 2.1328, LR: 0.000781\n",
      "Epoch [89], Batch [600/782], Loss: 0.7303, LR: 0.000781\n",
      "Epoch [89], Batch [650/782], Loss: 0.3408, LR: 0.000781\n",
      "Epoch [89], Batch [700/782], Loss: 2.2715, LR: 0.000781\n",
      "Epoch [89], Batch [750/782], Loss: 0.3575, LR: 0.000781\n",
      "Train set: Epoch: 89, Average loss:1.1443, LR: 0.000781 Top-1 Accuracy: 77.3420%, Top-5 Accuracy: 93.6600%, Time consumed:666.70s\n",
      "Test set: Epoch: 89, Average loss:0.6508, Top-1 Accuracy: 81.1700%, Top-5 Accuracy: 96.3100%, Time consumed:29.22s\n",
      "\n",
      "새로운 최고 top-1 정확도: 81.17%, top-5 정확도: 96.31%\n",
      "Accuracy improved (80.80% --> 81.17%). Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|█████████████████████████▊                                                             | 89/300 [17:17:00<40:53:02, 697.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [90], Batch [50/782], Loss: 0.2389, LR: 0.000781\n",
      "Epoch [90], Batch [100/782], Loss: 0.3505, LR: 0.000781\n",
      "Epoch [90], Batch [150/782], Loss: 0.2701, LR: 0.000781\n",
      "Epoch [90], Batch [200/782], Loss: 0.3005, LR: 0.000781\n",
      "Epoch [90], Batch [250/782], Loss: 0.5163, LR: 0.000781\n",
      "Epoch [90], Batch [300/782], Loss: 2.5181, LR: 0.000781\n",
      "Epoch [90], Batch [350/782], Loss: 2.4381, LR: 0.000781\n",
      "Epoch [90], Batch [400/782], Loss: 1.7175, LR: 0.000781\n",
      "Epoch [90], Batch [450/782], Loss: 0.4629, LR: 0.000781\n",
      "Epoch [90], Batch [500/782], Loss: 2.4321, LR: 0.000781\n",
      "Epoch [90], Batch [550/782], Loss: 0.4416, LR: 0.000781\n",
      "Epoch [90], Batch [600/782], Loss: 2.1426, LR: 0.000781\n",
      "Epoch [90], Batch [650/782], Loss: 0.2951, LR: 0.000781\n",
      "Epoch [90], Batch [700/782], Loss: 0.3542, LR: 0.000781\n",
      "Epoch [90], Batch [750/782], Loss: 0.4660, LR: 0.000781\n",
      "Train set: Epoch: 90, Average loss:1.1665, LR: 0.000781 Top-1 Accuracy: 77.4720%, Top-5 Accuracy: 93.9240%, Time consumed:667.48s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██████████████████████████                                                             | 90/300 [17:28:36<40:40:17, 697.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Epoch: 90, Average loss:0.6415, Top-1 Accuracy: 81.1200%, Top-5 Accuracy: 96.5500%, Time consumed:28.99s\n",
      "\n",
      "새로운 최고 top-5 정확도: 96.55%\n",
      "EarlyStopping 카운터: 1 / 30\n",
      "Epoch [91], Batch [50/782], Loss: 0.2573, LR: 0.000781\n",
      "Epoch [91], Batch [100/782], Loss: 2.3924, LR: 0.000781\n",
      "Epoch [91], Batch [150/782], Loss: 2.2939, LR: 0.000781\n",
      "Epoch [91], Batch [200/782], Loss: 0.4920, LR: 0.000781\n",
      "Epoch [91], Batch [250/782], Loss: 2.2606, LR: 0.000781\n",
      "Epoch [91], Batch [300/782], Loss: 2.0816, LR: 0.000781\n",
      "Epoch [91], Batch [350/782], Loss: 2.5498, LR: 0.000781\n",
      "Epoch [91], Batch [400/782], Loss: 0.3202, LR: 0.000781\n",
      "Epoch [91], Batch [450/782], Loss: 0.3445, LR: 0.000781\n",
      "Epoch [91], Batch [500/782], Loss: 0.3090, LR: 0.000781\n",
      "Epoch [91], Batch [550/782], Loss: 0.2303, LR: 0.000781\n",
      "Epoch [91], Batch [600/782], Loss: 1.3701, LR: 0.000781\n",
      "Epoch [91], Batch [650/782], Loss: 0.3107, LR: 0.000781\n",
      "Epoch [91], Batch [700/782], Loss: 0.5284, LR: 0.000781\n",
      "Epoch [91], Batch [750/782], Loss: 0.2825, LR: 0.000781\n",
      "Train set: Epoch: 91, Average loss:1.1646, LR: 0.000781 Top-1 Accuracy: 77.1820%, Top-5 Accuracy: 93.5320%, Time consumed:665.81s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██████████████████████████▍                                                            | 91/300 [17:40:13<40:27:34, 696.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Epoch: 91, Average loss:0.6430, Top-1 Accuracy: 81.1200%, Top-5 Accuracy: 96.4000%, Time consumed:30.36s\n",
      "\n",
      "EarlyStopping 카운터: 2 / 30\n",
      "Epoch [92], Batch [50/782], Loss: 0.3778, LR: 0.000781\n",
      "Epoch [92], Batch [100/782], Loss: 0.9101, LR: 0.000781\n",
      "Epoch [92], Batch [150/782], Loss: 2.3550, LR: 0.000781\n",
      "Epoch [92], Batch [200/782], Loss: 0.3311, LR: 0.000781\n",
      "Epoch [92], Batch [250/782], Loss: 2.0943, LR: 0.000781\n",
      "Epoch [92], Batch [300/782], Loss: 1.7588, LR: 0.000781\n",
      "Epoch [92], Batch [350/782], Loss: 0.4451, LR: 0.000781\n",
      "Epoch [92], Batch [400/782], Loss: 0.5517, LR: 0.000781\n",
      "Epoch [92], Batch [450/782], Loss: 0.4836, LR: 0.000781\n",
      "Epoch [92], Batch [500/782], Loss: 1.3881, LR: 0.000781\n",
      "Epoch [92], Batch [550/782], Loss: 0.3940, LR: 0.000781\n",
      "Epoch [92], Batch [600/782], Loss: 2.6289, LR: 0.000781\n",
      "Epoch [92], Batch [650/782], Loss: 2.0230, LR: 0.000781\n",
      "Epoch [92], Batch [700/782], Loss: 2.2968, LR: 0.000781\n",
      "Epoch [92], Batch [750/782], Loss: 1.3956, LR: 0.000781\n",
      "Train set: Epoch: 92, Average loss:1.1188, LR: 0.000781 Top-1 Accuracy: 77.8480%, Top-5 Accuracy: 94.0540%, Time consumed:665.75s\n",
      "Test set: Epoch: 92, Average loss:0.6345, Top-1 Accuracy: 81.8000%, Top-5 Accuracy: 96.5300%, Time consumed:28.63s\n",
      "\n",
      "새로운 최고 top-1 정확도: 81.80%, top-5 정확도: 96.53%\n",
      "Accuracy improved (81.17% --> 81.80%). Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|██████████████████████████▋                                                            | 92/300 [17:51:48<40:14:02, 696.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [93], Batch [50/782], Loss: 0.1669, LR: 0.000781\n",
      "Epoch [93], Batch [100/782], Loss: 2.2281, LR: 0.000781\n",
      "Epoch [93], Batch [150/782], Loss: 1.8917, LR: 0.000781\n",
      "Epoch [93], Batch [200/782], Loss: 0.9208, LR: 0.000781\n",
      "Epoch [93], Batch [250/782], Loss: 0.2830, LR: 0.000781\n",
      "Epoch [93], Batch [300/782], Loss: 0.6607, LR: 0.000781\n",
      "Epoch [93], Batch [350/782], Loss: 0.4033, LR: 0.000781\n",
      "Epoch [93], Batch [400/782], Loss: 0.3539, LR: 0.000781\n",
      "Epoch [93], Batch [450/782], Loss: 0.3692, LR: 0.000781\n",
      "Epoch [93], Batch [500/782], Loss: 0.3451, LR: 0.000781\n",
      "Epoch [93], Batch [550/782], Loss: 0.6675, LR: 0.000781\n",
      "Epoch [93], Batch [600/782], Loss: 1.4273, LR: 0.000781\n",
      "Epoch [93], Batch [650/782], Loss: 2.3741, LR: 0.000781\n",
      "Epoch [93], Batch [700/782], Loss: 0.9301, LR: 0.000781\n",
      "Epoch [93], Batch [750/782], Loss: 2.1330, LR: 0.000781\n",
      "Train set: Epoch: 93, Average loss:1.1461, LR: 0.000781 Top-1 Accuracy: 77.4840%, Top-5 Accuracy: 93.7800%, Time consumed:666.98s\n",
      "Test set: Epoch: 93, Average loss:0.6319, Top-1 Accuracy: 81.8600%, Top-5 Accuracy: 96.5500%, Time consumed:30.48s\n",
      "\n",
      "새로운 최고 top-1 정확도: 81.86%, top-5 정확도: 96.55%\n",
      "Accuracy improved (81.80% --> 81.86%). Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|██████████████████████████▉                                                            | 93/300 [18:03:26<40:04:17, 696.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [94], Batch [50/782], Loss: 2.7607, LR: 0.000781\n",
      "Epoch [94], Batch [100/782], Loss: 2.4908, LR: 0.000781\n",
      "Epoch [94], Batch [150/782], Loss: 2.4512, LR: 0.000781\n",
      "Epoch [94], Batch [200/782], Loss: 0.2664, LR: 0.000781\n",
      "Epoch [94], Batch [250/782], Loss: 2.6556, LR: 0.000781\n",
      "Epoch [94], Batch [300/782], Loss: 2.4187, LR: 0.000781\n",
      "Epoch [94], Batch [350/782], Loss: 2.0453, LR: 0.000781\n",
      "Epoch [94], Batch [400/782], Loss: 1.2338, LR: 0.000781\n",
      "Epoch [94], Batch [450/782], Loss: 1.7240, LR: 0.000781\n",
      "Epoch [94], Batch [500/782], Loss: 0.2627, LR: 0.000781\n",
      "Epoch [94], Batch [550/782], Loss: 2.4172, LR: 0.000781\n",
      "Epoch [94], Batch [600/782], Loss: 0.7732, LR: 0.000781\n",
      "Epoch [94], Batch [650/782], Loss: 0.3396, LR: 0.000781\n",
      "Epoch [94], Batch [700/782], Loss: 1.5198, LR: 0.000781\n",
      "Epoch [94], Batch [750/782], Loss: 2.6385, LR: 0.000781\n",
      "Train set: Epoch: 94, Average loss:1.1597, LR: 0.000781 Top-1 Accuracy: 76.9300%, Top-5 Accuracy: 93.3980%, Time consumed:666.09s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███████████████████████████▎                                                           | 94/300 [18:15:02<39:51:35, 696.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Epoch: 94, Average loss:0.6859, Top-1 Accuracy: 80.6800%, Top-5 Accuracy: 96.4100%, Time consumed:29.74s\n",
      "\n",
      "EarlyStopping 카운터: 1 / 30\n",
      "Epoch [95], Batch [50/782], Loss: 0.3606, LR: 0.000781\n",
      "Epoch [95], Batch [100/782], Loss: 1.4712, LR: 0.000781\n",
      "Epoch [95], Batch [150/782], Loss: 0.3500, LR: 0.000781\n",
      "Epoch [95], Batch [200/782], Loss: 0.4325, LR: 0.000781\n",
      "Epoch [95], Batch [250/782], Loss: 0.2837, LR: 0.000781\n",
      "Epoch [95], Batch [300/782], Loss: 2.2598, LR: 0.000781\n",
      "Epoch [95], Batch [350/782], Loss: 0.3176, LR: 0.000781\n",
      "Epoch [95], Batch [400/782], Loss: 2.5862, LR: 0.000781\n",
      "Epoch [95], Batch [450/782], Loss: 0.2762, LR: 0.000781\n",
      "Epoch [95], Batch [500/782], Loss: 2.4627, LR: 0.000781\n",
      "Epoch [95], Batch [550/782], Loss: 2.5080, LR: 0.000781\n",
      "Epoch [95], Batch [600/782], Loss: 0.3856, LR: 0.000781\n",
      "Epoch [95], Batch [650/782], Loss: 2.6763, LR: 0.000781\n",
      "Epoch [95], Batch [700/782], Loss: 0.3518, LR: 0.000781\n",
      "Epoch [95], Batch [750/782], Loss: 2.4228, LR: 0.000781\n",
      "Train set: Epoch: 95, Average loss:1.1598, LR: 0.000781 Top-1 Accuracy: 77.1020%, Top-5 Accuracy: 93.4040%, Time consumed:666.76s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███████████████████████████▌                                                           | 95/300 [18:26:37<39:39:07, 696.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Epoch: 95, Average loss:0.6667, Top-1 Accuracy: 80.9200%, Top-5 Accuracy: 96.4500%, Time consumed:28.97s\n",
      "\n",
      "EarlyStopping 카운터: 2 / 30\n",
      "Epoch [96], Batch [50/782], Loss: 2.0804, LR: 0.000391\n",
      "Epoch [96], Batch [100/782], Loss: 2.6810, LR: 0.000391\n",
      "Epoch [96], Batch [150/782], Loss: 1.2373, LR: 0.000391\n",
      "Epoch [96], Batch [200/782], Loss: 0.2512, LR: 0.000391\n",
      "Epoch [96], Batch [250/782], Loss: 0.2784, LR: 0.000391\n",
      "Epoch [96], Batch [300/782], Loss: 2.4890, LR: 0.000391\n",
      "Epoch [96], Batch [350/782], Loss: 2.0478, LR: 0.000391\n",
      "Epoch [96], Batch [400/782], Loss: 1.7002, LR: 0.000391\n",
      "Epoch [96], Batch [450/782], Loss: 0.5146, LR: 0.000391\n",
      "Epoch [96], Batch [500/782], Loss: 2.4245, LR: 0.000391\n",
      "Epoch [96], Batch [550/782], Loss: 1.9343, LR: 0.000391\n",
      "Epoch [96], Batch [600/782], Loss: 1.1167, LR: 0.000391\n",
      "Epoch [96], Batch [650/782], Loss: 0.3545, LR: 0.000391\n",
      "Epoch [96], Batch [700/782], Loss: 2.3858, LR: 0.000391\n",
      "Epoch [96], Batch [750/782], Loss: 0.3854, LR: 0.000391\n",
      "Train set: Epoch: 96, Average loss:1.1341, LR: 0.000391 Top-1 Accuracy: 77.9800%, Top-5 Accuracy: 93.7940%, Time consumed:664.28s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███████████████████████████▊                                                           | 96/300 [18:38:11<39:24:36, 695.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Epoch: 96, Average loss:0.6239, Top-1 Accuracy: 81.5300%, Top-5 Accuracy: 96.6100%, Time consumed:29.18s\n",
      "\n",
      "새로운 최고 top-5 정확도: 96.61%\n",
      "EarlyStopping 카운터: 3 / 30\n",
      "Epoch [97], Batch [50/782], Loss: 0.4153, LR: 0.000391\n",
      "Epoch [97], Batch [100/782], Loss: 0.2545, LR: 0.000391\n",
      "Epoch [97], Batch [150/782], Loss: 0.2120, LR: 0.000391\n",
      "Epoch [97], Batch [200/782], Loss: 0.4611, LR: 0.000391\n",
      "Epoch [97], Batch [250/782], Loss: 1.5980, LR: 0.000391\n",
      "Epoch [97], Batch [300/782], Loss: 2.6425, LR: 0.000391\n",
      "Epoch [97], Batch [350/782], Loss: 2.4027, LR: 0.000391\n",
      "Epoch [97], Batch [400/782], Loss: 2.2433, LR: 0.000391\n",
      "Epoch [97], Batch [450/782], Loss: 2.3208, LR: 0.000391\n",
      "Epoch [97], Batch [500/782], Loss: 2.1092, LR: 0.000391\n",
      "Epoch [97], Batch [550/782], Loss: 1.8050, LR: 0.000391\n",
      "Epoch [97], Batch [600/782], Loss: 1.7347, LR: 0.000391\n",
      "Epoch [97], Batch [650/782], Loss: 2.1863, LR: 0.000391\n",
      "Epoch [97], Batch [700/782], Loss: 1.5013, LR: 0.000391\n",
      "Epoch [97], Batch [750/782], Loss: 0.3053, LR: 0.000391\n",
      "Train set: Epoch: 97, Average loss:1.1365, LR: 0.000391 Top-1 Accuracy: 77.0980%, Top-5 Accuracy: 93.5400%, Time consumed:667.68s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|████████████████████████████▏                                                          | 97/300 [18:49:48<39:14:34, 695.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Epoch: 97, Average loss:0.6414, Top-1 Accuracy: 81.3200%, Top-5 Accuracy: 96.4600%, Time consumed:29.32s\n",
      "\n",
      "EarlyStopping 카운터: 4 / 30\n",
      "Epoch [98], Batch [50/782], Loss: 0.6181, LR: 0.000391\n",
      "Epoch [98], Batch [100/782], Loss: 2.1775, LR: 0.000391\n",
      "Epoch [98], Batch [150/782], Loss: 0.4588, LR: 0.000391\n",
      "Epoch [98], Batch [200/782], Loss: 2.4539, LR: 0.000391\n",
      "Epoch [98], Batch [250/782], Loss: 0.3365, LR: 0.000391\n",
      "Epoch [98], Batch [300/782], Loss: 2.6574, LR: 0.000391\n",
      "Epoch [98], Batch [350/782], Loss: 1.7498, LR: 0.000391\n",
      "Epoch [98], Batch [400/782], Loss: 0.3821, LR: 0.000391\n",
      "Epoch [98], Batch [450/782], Loss: 1.3175, LR: 0.000391\n",
      "Epoch [98], Batch [500/782], Loss: 0.3940, LR: 0.000391\n",
      "Epoch [98], Batch [550/782], Loss: 0.3021, LR: 0.000391\n",
      "Epoch [98], Batch [600/782], Loss: 2.4590, LR: 0.000391\n",
      "Epoch [98], Batch [650/782], Loss: 2.2032, LR: 0.000391\n",
      "Epoch [98], Batch [700/782], Loss: 0.4814, LR: 0.000391\n",
      "Epoch [98], Batch [750/782], Loss: 0.6072, LR: 0.000391\n",
      "Train set: Epoch: 98, Average loss:1.1907, LR: 0.000391 Top-1 Accuracy: 76.8880%, Top-5 Accuracy: 93.4120%, Time consumed:665.79s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|████████████████████████████▍                                                          | 98/300 [19:01:23<39:01:56, 695.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Epoch: 98, Average loss:0.6414, Top-1 Accuracy: 81.6000%, Top-5 Accuracy: 96.4600%, Time consumed:29.11s\n",
      "\n",
      "EarlyStopping 카운터: 5 / 30\n",
      "Epoch [99], Batch [50/782], Loss: 2.1481, LR: 0.000391\n",
      "Epoch [99], Batch [100/782], Loss: 0.3211, LR: 0.000391\n",
      "Epoch [99], Batch [150/782], Loss: 0.3291, LR: 0.000391\n",
      "Epoch [99], Batch [200/782], Loss: 2.4565, LR: 0.000391\n",
      "Epoch [99], Batch [250/782], Loss: 0.1952, LR: 0.000391\n",
      "Epoch [99], Batch [300/782], Loss: 0.4046, LR: 0.000391\n",
      "Epoch [99], Batch [350/782], Loss: 0.2678, LR: 0.000391\n",
      "Epoch [99], Batch [400/782], Loss: 2.5691, LR: 0.000391\n",
      "Epoch [99], Batch [450/782], Loss: 0.1791, LR: 0.000391\n",
      "Epoch [99], Batch [500/782], Loss: 1.7079, LR: 0.000391\n",
      "Epoch [99], Batch [550/782], Loss: 0.3350, LR: 0.000391\n",
      "Epoch [99], Batch [600/782], Loss: 0.2646, LR: 0.000391\n",
      "Epoch [99], Batch [650/782], Loss: 1.2677, LR: 0.000391\n",
      "Epoch [99], Batch [700/782], Loss: 0.2227, LR: 0.000391\n",
      "Epoch [99], Batch [750/782], Loss: 0.2810, LR: 0.000391\n",
      "Train set: Epoch: 99, Average loss:1.1245, LR: 0.000391 Top-1 Accuracy: 77.5320%, Top-5 Accuracy: 93.5640%, Time consumed:667.14s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|████████████████████████████▋                                                          | 99/300 [19:12:59<38:50:45, 695.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Epoch: 99, Average loss:0.6351, Top-1 Accuracy: 81.5800%, Top-5 Accuracy: 96.4300%, Time consumed:28.88s\n",
      "\n",
      "EarlyStopping 카운터: 6 / 30\n",
      "Epoch [100], Batch [50/782], Loss: 0.2153, LR: 0.000391\n",
      "Epoch [100], Batch [100/782], Loss: 2.0588, LR: 0.000391\n",
      "Epoch [100], Batch [150/782], Loss: 0.3101, LR: 0.000391\n",
      "Epoch [100], Batch [200/782], Loss: 0.4689, LR: 0.000391\n",
      "Epoch [100], Batch [250/782], Loss: 2.3454, LR: 0.000391\n",
      "Epoch [100], Batch [300/782], Loss: 1.9759, LR: 0.000391\n",
      "Epoch [100], Batch [350/782], Loss: 0.2655, LR: 0.000391\n",
      "Epoch [100], Batch [400/782], Loss: 1.7839, LR: 0.000391\n",
      "Epoch [100], Batch [450/782], Loss: 0.3043, LR: 0.000391\n",
      "Epoch [100], Batch [500/782], Loss: 0.4940, LR: 0.000391\n",
      "Epoch [100], Batch [550/782], Loss: 0.3490, LR: 0.000391\n",
      "Epoch [100], Batch [600/782], Loss: 0.2930, LR: 0.000391\n",
      "Epoch [100], Batch [650/782], Loss: 0.3416, LR: 0.000391\n",
      "Epoch [100], Batch [700/782], Loss: 0.4084, LR: 0.000391\n",
      "Epoch [100], Batch [750/782], Loss: 0.5009, LR: 0.000391\n",
      "Train set: Epoch: 100, Average loss:1.0787, LR: 0.000391 Top-1 Accuracy: 79.1500%, Top-5 Accuracy: 94.4080%, Time consumed:667.58s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|████████████████████████████▋                                                         | 100/300 [19:24:37<38:41:22, 696.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Epoch: 100, Average loss:0.6394, Top-1 Accuracy: 81.5100%, Top-5 Accuracy: 96.4200%, Time consumed:30.36s\n",
      "\n",
      "EarlyStopping 카운터: 7 / 30\n",
      "Epoch [101], Batch [50/782], Loss: 2.3510, LR: 0.000391\n",
      "Epoch [101], Batch [100/782], Loss: 0.2696, LR: 0.000391\n",
      "Epoch [101], Batch [150/782], Loss: 2.1796, LR: 0.000391\n",
      "Epoch [101], Batch [200/782], Loss: 0.9697, LR: 0.000391\n",
      "Epoch [101], Batch [250/782], Loss: 0.3917, LR: 0.000391\n",
      "Epoch [101], Batch [300/782], Loss: 0.5120, LR: 0.000391\n",
      "Epoch [101], Batch [350/782], Loss: 0.3533, LR: 0.000391\n",
      "Epoch [101], Batch [400/782], Loss: 1.4777, LR: 0.000391\n",
      "Epoch [101], Batch [450/782], Loss: 2.3701, LR: 0.000391\n",
      "Epoch [101], Batch [500/782], Loss: 0.6183, LR: 0.000391\n",
      "Epoch [101], Batch [550/782], Loss: 2.1341, LR: 0.000391\n",
      "Epoch [101], Batch [600/782], Loss: 2.5623, LR: 0.000391\n",
      "Epoch [101], Batch [650/782], Loss: 0.2428, LR: 0.000391\n",
      "Epoch [101], Batch [700/782], Loss: 0.4525, LR: 0.000391\n",
      "Epoch [101], Batch [750/782], Loss: 1.8934, LR: 0.000391\n",
      "Train set: Epoch: 101, Average loss:1.1408, LR: 0.000391 Top-1 Accuracy: 77.3000%, Top-5 Accuracy: 93.3920%, Time consumed:668.04s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|████████████████████████████▉                                                         | 101/300 [19:36:14<38:30:16, 696.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Epoch: 101, Average loss:0.6235, Top-1 Accuracy: 81.5800%, Top-5 Accuracy: 96.6500%, Time consumed:28.87s\n",
      "\n",
      "새로운 최고 top-5 정확도: 96.65%\n",
      "EarlyStopping 카운터: 8 / 30\n",
      "Epoch [102], Batch [50/782], Loss: 0.3175, LR: 0.000195\n",
      "Epoch [102], Batch [100/782], Loss: 1.2618, LR: 0.000195\n",
      "Epoch [102], Batch [150/782], Loss: 2.5878, LR: 0.000195\n",
      "Epoch [102], Batch [200/782], Loss: 0.2447, LR: 0.000195\n",
      "Epoch [102], Batch [250/782], Loss: 0.2765, LR: 0.000195\n",
      "Epoch [102], Batch [300/782], Loss: 1.9730, LR: 0.000195\n",
      "Epoch [102], Batch [350/782], Loss: 1.8686, LR: 0.000195\n",
      "Epoch [102], Batch [400/782], Loss: 2.0890, LR: 0.000195\n",
      "Epoch [102], Batch [450/782], Loss: 0.4380, LR: 0.000195\n",
      "Epoch [102], Batch [500/782], Loss: 0.3403, LR: 0.000195\n",
      "Epoch [102], Batch [550/782], Loss: 0.1749, LR: 0.000195\n",
      "Epoch [102], Batch [600/782], Loss: 0.1554, LR: 0.000195\n",
      "Epoch [102], Batch [650/782], Loss: 0.3302, LR: 0.000195\n",
      "Epoch [102], Batch [700/782], Loss: 0.3465, LR: 0.000195\n",
      "Epoch [102], Batch [750/782], Loss: 0.2112, LR: 0.000195\n",
      "Train set: Epoch: 102, Average loss:1.0636, LR: 0.000195 Top-1 Accuracy: 79.6400%, Top-5 Accuracy: 94.4540%, Time consumed:672.86s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|█████████████████████████████▏                                                        | 102/300 [19:47:57<38:25:50, 698.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Epoch: 102, Average loss:0.6556, Top-1 Accuracy: 81.5900%, Top-5 Accuracy: 96.3300%, Time consumed:30.94s\n",
      "\n",
      "EarlyStopping 카운터: 9 / 30\n",
      "Epoch [103], Batch [50/782], Loss: 2.4826, LR: 0.000195\n",
      "Epoch [103], Batch [100/782], Loss: 0.6340, LR: 0.000195\n",
      "Epoch [103], Batch [150/782], Loss: 0.3853, LR: 0.000195\n",
      "Epoch [103], Batch [200/782], Loss: 1.4850, LR: 0.000195\n",
      "Epoch [103], Batch [250/782], Loss: 0.2849, LR: 0.000195\n",
      "Epoch [103], Batch [300/782], Loss: 0.3456, LR: 0.000195\n",
      "Epoch [103], Batch [350/782], Loss: 0.3052, LR: 0.000195\n",
      "Epoch [103], Batch [400/782], Loss: 1.1959, LR: 0.000195\n",
      "Epoch [103], Batch [450/782], Loss: 1.2662, LR: 0.000195\n",
      "Epoch [103], Batch [500/782], Loss: 1.8055, LR: 0.000195\n",
      "Epoch [103], Batch [550/782], Loss: 1.7236, LR: 0.000195\n",
      "Epoch [103], Batch [600/782], Loss: 2.5012, LR: 0.000195\n",
      "Epoch [103], Batch [650/782], Loss: 2.6067, LR: 0.000195\n",
      "Epoch [103], Batch [700/782], Loss: 0.5562, LR: 0.000195\n",
      "Epoch [103], Batch [750/782], Loss: 0.3216, LR: 0.000195\n",
      "Train set: Epoch: 103, Average loss:1.1104, LR: 0.000195 Top-1 Accuracy: 78.7340%, Top-5 Accuracy: 94.2460%, Time consumed:668.04s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|█████████████████████████████▌                                                        | 103/300 [19:59:34<38:12:28, 698.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Epoch: 103, Average loss:0.6349, Top-1 Accuracy: 81.5100%, Top-5 Accuracy: 96.5100%, Time consumed:28.94s\n",
      "\n",
      "EarlyStopping 카운터: 10 / 30\n",
      "Epoch [104], Batch [50/782], Loss: 1.6126, LR: 0.000195\n",
      "Epoch [104], Batch [100/782], Loss: 0.9656, LR: 0.000195\n",
      "Epoch [104], Batch [150/782], Loss: 2.3562, LR: 0.000195\n",
      "Epoch [104], Batch [200/782], Loss: 0.4003, LR: 0.000195\n",
      "Epoch [104], Batch [250/782], Loss: 2.6362, LR: 0.000195\n",
      "Epoch [104], Batch [300/782], Loss: 2.2918, LR: 0.000195\n",
      "Epoch [104], Batch [350/782], Loss: 1.8798, LR: 0.000195\n",
      "Epoch [104], Batch [400/782], Loss: 0.4405, LR: 0.000195\n",
      "Epoch [104], Batch [450/782], Loss: 0.3154, LR: 0.000195\n",
      "Epoch [104], Batch [500/782], Loss: 0.1896, LR: 0.000195\n",
      "Epoch [104], Batch [550/782], Loss: 0.2342, LR: 0.000195\n",
      "Epoch [104], Batch [600/782], Loss: 0.3745, LR: 0.000195\n",
      "Epoch [104], Batch [650/782], Loss: 0.7579, LR: 0.000195\n",
      "Epoch [104], Batch [700/782], Loss: 1.7133, LR: 0.000195\n",
      "Epoch [104], Batch [750/782], Loss: 0.2081, LR: 0.000195\n",
      "Train set: Epoch: 104, Average loss:1.1223, LR: 0.000195 Top-1 Accuracy: 77.8700%, Top-5 Accuracy: 93.6000%, Time consumed:664.51s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|█████████████████████████████▊                                                        | 104/300 [20:11:08<37:56:04, 696.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Epoch: 104, Average loss:0.6319, Top-1 Accuracy: 81.4900%, Top-5 Accuracy: 96.5100%, Time consumed:28.84s\n",
      "\n",
      "EarlyStopping 카운터: 11 / 30\n",
      "Epoch [105], Batch [50/782], Loss: 1.6118, LR: 0.000195\n",
      "Epoch [105], Batch [100/782], Loss: 1.8997, LR: 0.000195\n",
      "Epoch [105], Batch [150/782], Loss: 0.3115, LR: 0.000195\n",
      "Epoch [105], Batch [200/782], Loss: 2.3005, LR: 0.000195\n",
      "Epoch [105], Batch [250/782], Loss: 0.2886, LR: 0.000195\n",
      "Epoch [105], Batch [300/782], Loss: 2.5141, LR: 0.000195\n",
      "Epoch [105], Batch [350/782], Loss: 1.1484, LR: 0.000195\n",
      "Epoch [105], Batch [400/782], Loss: 2.5908, LR: 0.000195\n",
      "Epoch [105], Batch [450/782], Loss: 1.4234, LR: 0.000195\n",
      "Epoch [105], Batch [500/782], Loss: 0.2883, LR: 0.000195\n",
      "Epoch [105], Batch [550/782], Loss: 0.2451, LR: 0.000195\n",
      "Epoch [105], Batch [600/782], Loss: 0.2502, LR: 0.000195\n",
      "Epoch [105], Batch [650/782], Loss: 0.3770, LR: 0.000195\n",
      "Epoch [105], Batch [700/782], Loss: 0.2402, LR: 0.000195\n",
      "Epoch [105], Batch [750/782], Loss: 2.4981, LR: 0.000195\n",
      "Train set: Epoch: 105, Average loss:1.1134, LR: 0.000195 Top-1 Accuracy: 78.5340%, Top-5 Accuracy: 93.8920%, Time consumed:666.14s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|██████████████████████████████                                                        | 105/300 [20:22:43<37:42:56, 696.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Epoch: 105, Average loss:0.6258, Top-1 Accuracy: 81.6400%, Top-5 Accuracy: 96.6400%, Time consumed:29.04s\n",
      "\n",
      "EarlyStopping 카운터: 12 / 30\n",
      "Epoch [106], Batch [50/782], Loss: 0.4456, LR: 0.000195\n",
      "Epoch [106], Batch [100/782], Loss: 0.3099, LR: 0.000195\n",
      "Epoch [106], Batch [150/782], Loss: 2.1456, LR: 0.000195\n",
      "Epoch [106], Batch [200/782], Loss: 1.9251, LR: 0.000195\n",
      "Epoch [106], Batch [250/782], Loss: 0.2754, LR: 0.000195\n",
      "Epoch [106], Batch [300/782], Loss: 0.2884, LR: 0.000195\n",
      "Epoch [106], Batch [350/782], Loss: 0.4342, LR: 0.000195\n",
      "Epoch [106], Batch [400/782], Loss: 2.3766, LR: 0.000195\n",
      "Epoch [106], Batch [450/782], Loss: 0.2659, LR: 0.000195\n",
      "Epoch [106], Batch [500/782], Loss: 2.6292, LR: 0.000195\n",
      "Epoch [106], Batch [550/782], Loss: 0.3294, LR: 0.000195\n",
      "Epoch [106], Batch [600/782], Loss: 2.3929, LR: 0.000195\n",
      "Epoch [106], Batch [650/782], Loss: 2.3288, LR: 0.000195\n",
      "Epoch [106], Batch [700/782], Loss: 0.2986, LR: 0.000195\n",
      "Epoch [106], Batch [750/782], Loss: 0.1762, LR: 0.000195\n",
      "Train set: Epoch: 106, Average loss:1.1029, LR: 0.000195 Top-1 Accuracy: 78.8420%, Top-5 Accuracy: 94.1100%, Time consumed:665.95s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|██████████████████████████████▍                                                       | 106/300 [20:34:18<37:29:51, 695.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Epoch: 106, Average loss:0.6288, Top-1 Accuracy: 81.7500%, Top-5 Accuracy: 96.6000%, Time consumed:28.80s\n",
      "\n",
      "EarlyStopping 카운터: 13 / 30\n",
      "Epoch [107], Batch [50/782], Loss: 2.3613, LR: 0.000195\n",
      "Epoch [107], Batch [100/782], Loss: 1.6054, LR: 0.000195\n",
      "Epoch [107], Batch [150/782], Loss: 1.1294, LR: 0.000195\n",
      "Epoch [107], Batch [200/782], Loss: 2.5093, LR: 0.000195\n",
      "Epoch [107], Batch [250/782], Loss: 0.2991, LR: 0.000195\n",
      "Epoch [107], Batch [300/782], Loss: 0.2063, LR: 0.000195\n",
      "Epoch [107], Batch [350/782], Loss: 2.0757, LR: 0.000195\n",
      "Epoch [107], Batch [400/782], Loss: 1.9795, LR: 0.000195\n",
      "Epoch [107], Batch [450/782], Loss: 0.2332, LR: 0.000195\n",
      "Epoch [107], Batch [500/782], Loss: 0.2115, LR: 0.000195\n",
      "Epoch [107], Batch [550/782], Loss: 0.2144, LR: 0.000195\n",
      "Epoch [107], Batch [600/782], Loss: 0.3199, LR: 0.000195\n",
      "Epoch [107], Batch [650/782], Loss: 0.2479, LR: 0.000195\n",
      "Epoch [107], Batch [700/782], Loss: 2.5261, LR: 0.000195\n",
      "Epoch [107], Batch [750/782], Loss: 2.3052, LR: 0.000195\n",
      "Train set: Epoch: 107, Average loss:1.0886, LR: 0.000195 Top-1 Accuracy: 79.1580%, Top-5 Accuracy: 94.2860%, Time consumed:664.37s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|██████████████████████████████▋                                                       | 107/300 [20:45:52<37:17:11, 695.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Epoch: 107, Average loss:0.6339, Top-1 Accuracy: 81.7200%, Top-5 Accuracy: 96.6200%, Time consumed:30.34s\n",
      "\n",
      "EarlyStopping 카운터: 14 / 30\n",
      "Epoch [108], Batch [50/782], Loss: 1.3083, LR: 0.000098\n",
      "Epoch [108], Batch [100/782], Loss: 0.2301, LR: 0.000098\n",
      "Epoch [108], Batch [150/782], Loss: 0.3117, LR: 0.000098\n",
      "Epoch [108], Batch [200/782], Loss: 1.9305, LR: 0.000098\n",
      "Epoch [108], Batch [250/782], Loss: 2.5738, LR: 0.000098\n",
      "Epoch [108], Batch [300/782], Loss: 2.2586, LR: 0.000098\n",
      "Epoch [108], Batch [350/782], Loss: 0.1778, LR: 0.000098\n",
      "Epoch [108], Batch [400/782], Loss: 0.2198, LR: 0.000098\n",
      "Epoch [108], Batch [450/782], Loss: 0.2263, LR: 0.000098\n",
      "Epoch [108], Batch [500/782], Loss: 2.4332, LR: 0.000098\n",
      "Epoch [108], Batch [550/782], Loss: 2.2019, LR: 0.000098\n",
      "Epoch [108], Batch [600/782], Loss: 2.0798, LR: 0.000098\n",
      "Epoch [108], Batch [650/782], Loss: 0.4840, LR: 0.000098\n",
      "Epoch [108], Batch [700/782], Loss: 0.2376, LR: 0.000098\n",
      "Epoch [108], Batch [750/782], Loss: 0.2599, LR: 0.000098\n",
      "Train set: Epoch: 108, Average loss:1.1246, LR: 0.000098 Top-1 Accuracy: 77.8660%, Top-5 Accuracy: 93.5120%, Time consumed:667.58s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|██████████████████████████████▉                                                       | 108/300 [20:57:29<37:06:28, 695.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Epoch: 108, Average loss:0.6502, Top-1 Accuracy: 81.3500%, Top-5 Accuracy: 96.5400%, Time consumed:28.83s\n",
      "\n",
      "EarlyStopping 카운터: 15 / 30\n",
      "Epoch [109], Batch [50/782], Loss: 0.1684, LR: 0.000098\n",
      "Epoch [109], Batch [100/782], Loss: 0.1391, LR: 0.000098\n",
      "Epoch [109], Batch [150/782], Loss: 0.2062, LR: 0.000098\n",
      "Epoch [109], Batch [200/782], Loss: 2.2065, LR: 0.000098\n",
      "Epoch [109], Batch [250/782], Loss: 0.4236, LR: 0.000098\n",
      "Epoch [109], Batch [300/782], Loss: 0.1901, LR: 0.000098\n",
      "Epoch [109], Batch [350/782], Loss: 0.1634, LR: 0.000098\n",
      "Epoch [109], Batch [400/782], Loss: 1.6370, LR: 0.000098\n",
      "Epoch [109], Batch [450/782], Loss: 0.3945, LR: 0.000098\n",
      "Epoch [109], Batch [500/782], Loss: 0.2846, LR: 0.000098\n",
      "Epoch [109], Batch [550/782], Loss: 2.3780, LR: 0.000098\n",
      "Epoch [109], Batch [600/782], Loss: 2.3661, LR: 0.000098\n",
      "Epoch [109], Batch [650/782], Loss: 2.5649, LR: 0.000098\n",
      "Epoch [109], Batch [700/782], Loss: 0.4192, LR: 0.000098\n",
      "Epoch [109], Batch [750/782], Loss: 0.4122, LR: 0.000098\n",
      "Train set: Epoch: 109, Average loss:1.0430, LR: 0.000098 Top-1 Accuracy: 79.5500%, Top-5 Accuracy: 94.3940%, Time consumed:666.79s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███████████████████████████████▏                                                      | 109/300 [21:09:06<36:55:56, 696.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Epoch: 109, Average loss:0.6323, Top-1 Accuracy: 81.5700%, Top-5 Accuracy: 96.6900%, Time consumed:30.08s\n",
      "\n",
      "새로운 최고 top-5 정확도: 96.69%\n",
      "EarlyStopping 카운터: 16 / 30\n",
      "Epoch [110], Batch [50/782], Loss: 0.5993, LR: 0.000098\n",
      "Epoch [110], Batch [100/782], Loss: 0.2133, LR: 0.000098\n",
      "Epoch [110], Batch [150/782], Loss: 1.8238, LR: 0.000098\n",
      "Epoch [110], Batch [200/782], Loss: 0.4769, LR: 0.000098\n",
      "Epoch [110], Batch [250/782], Loss: 0.1509, LR: 0.000098\n",
      "Epoch [110], Batch [300/782], Loss: 0.4763, LR: 0.000098\n",
      "Epoch [110], Batch [350/782], Loss: 1.9423, LR: 0.000098\n",
      "Epoch [110], Batch [400/782], Loss: 0.3236, LR: 0.000098\n",
      "Epoch [110], Batch [450/782], Loss: 0.2791, LR: 0.000098\n",
      "Epoch [110], Batch [500/782], Loss: 2.4613, LR: 0.000098\n",
      "Epoch [110], Batch [550/782], Loss: 0.4540, LR: 0.000098\n",
      "Epoch [110], Batch [600/782], Loss: 0.2566, LR: 0.000098\n",
      "Epoch [110], Batch [650/782], Loss: 0.2897, LR: 0.000098\n",
      "Epoch [110], Batch [700/782], Loss: 0.8047, LR: 0.000098\n",
      "Epoch [110], Batch [750/782], Loss: 0.3324, LR: 0.000098\n",
      "Train set: Epoch: 110, Average loss:1.0829, LR: 0.000098 Top-1 Accuracy: 79.4060%, Top-5 Accuracy: 94.4860%, Time consumed:666.06s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███████████████████████████████▌                                                      | 110/300 [21:20:41<36:43:12, 695.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Epoch: 110, Average loss:0.6384, Top-1 Accuracy: 81.6900%, Top-5 Accuracy: 96.4900%, Time consumed:28.85s\n",
      "\n",
      "EarlyStopping 카운터: 17 / 30\n",
      "Epoch [111], Batch [50/782], Loss: 1.4958, LR: 0.000098\n",
      "Epoch [111], Batch [100/782], Loss: 0.3184, LR: 0.000098\n",
      "Epoch [111], Batch [150/782], Loss: 0.3362, LR: 0.000098\n",
      "Epoch [111], Batch [200/782], Loss: 1.9475, LR: 0.000098\n",
      "Epoch [111], Batch [250/782], Loss: 0.3540, LR: 0.000098\n",
      "Epoch [111], Batch [300/782], Loss: 1.2603, LR: 0.000098\n",
      "Epoch [111], Batch [350/782], Loss: 2.3001, LR: 0.000098\n",
      "Epoch [111], Batch [400/782], Loss: 0.3187, LR: 0.000098\n",
      "Epoch [111], Batch [450/782], Loss: 2.2892, LR: 0.000098\n",
      "Epoch [111], Batch [500/782], Loss: 0.2032, LR: 0.000098\n",
      "Epoch [111], Batch [550/782], Loss: 0.2985, LR: 0.000098\n",
      "Epoch [111], Batch [600/782], Loss: 1.0325, LR: 0.000098\n",
      "Epoch [111], Batch [650/782], Loss: 2.2105, LR: 0.000098\n",
      "Epoch [111], Batch [700/782], Loss: 0.5018, LR: 0.000098\n",
      "Epoch [111], Batch [750/782], Loss: 1.6130, LR: 0.000098\n",
      "Train set: Epoch: 111, Average loss:1.1038, LR: 0.000098 Top-1 Accuracy: 79.4580%, Top-5 Accuracy: 94.5580%, Time consumed:665.48s\n",
      "Test set: Epoch: 111, Average loss:0.6093, Top-1 Accuracy: 81.9800%, Top-5 Accuracy: 96.6500%, Time consumed:29.10s\n",
      "\n",
      "새로운 최고 top-1 정확도: 81.98%, top-5 정확도: 96.65%\n",
      "Accuracy improved (81.86% --> 81.98%). Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███████████████████████████████▊                                                      | 111/300 [21:32:16<36:31:07, 695.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [112], Batch [50/782], Loss: 0.3195, LR: 0.000098\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.v2 as transforms_v2  # CutMix를 위한 v2 transforms 추가\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "from tools.tool import AccuracyEarlyStopping, WarmUpLR, SAM  \n",
    "from models.shake_pyramidnet import ShakePyramidNet\n",
    "\n",
    "# 메모리 초기화\n",
    "torch.cuda.empty_cache()\n",
    "torch.backends.cuda.max_memory_cached = 0\n",
    "\n",
    "wandb.login(key=\"ef091b9abcea3186341ddf8995d62bde62d7469e\")\n",
    "wandb.init(project=\"PBL-2\", name=\"shake_pyramidnet-cfc,SAM_SGD\")  \n",
    "\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "# WandB 설정\n",
    "config = {\n",
    "    \"model\": \"shake_pyramidnet\",\n",
    "    \"batch_size\": 256,\n",
    "    \"num_epochs\": 300,\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"optimizer\": \"SAM_SGD\",\n",
    "    \"seed\": 2025,\n",
    "    \"deterministic\": False,\n",
    "    \"patience\": 30,  # early stopping patience\n",
    "    \"max_epochs_wait\": float('inf'),  # 최대 30 에폭까지만 기다림\n",
    "    \"cutmix_alpha\": 1.0,  # CutMix 알파 파라미터 추가\n",
    "    \"cutmix_prob\": 0.5,   # CutMix 적용 확률 추가\n",
    "    \"crop_padding\": 4,    # RandomCrop 패딩 크기\n",
    "    \"crop_size\": 32,      # RandomCrop 크기 (CIFAR-100 이미지 크기는 32x32)\n",
    "    \"warmup_epochs\": 5,   # 웜업할 에폭 수 추가\n",
    "    \"momentum\" : 0.9,\n",
    "    \"weight_decay\" : 5e-4\n",
    "    \n",
    "}\n",
    "wandb.config.update(config)\n",
    "\n",
    "# CIFAR-100 데이터셋 로드 - 기본 train/test 분할 사용\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(config[\"crop_size\"], padding=config[\"crop_padding\"]),  # 패딩 후 랜덤 크롭\n",
    "    transforms.RandomHorizontalFlip(),  # 수평 뒤집기\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR100(\n",
    "    root='./data', train=True, download=True, transform=transform_train)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR100(\n",
    "    root='./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "# DataLoader 생성\n",
    "trainloader = DataLoader(trainset, batch_size=config[\"batch_size\"], pin_memory=True, shuffle=True, num_workers=8)\n",
    "testloader = DataLoader(testset, batch_size=config[\"batch_size\"], pin_memory=True, shuffle=False, num_workers=8)\n",
    "\n",
    "print(f\"Train set size: {len(trainset)}\")\n",
    "print(f\"Test set size: {len(testset)}\")\n",
    "\n",
    "# CutMix 변환 정의\n",
    "cutmix = transforms_v2.CutMix(alpha=config[\"cutmix_alpha\"], num_classes=100)  # CIFAR-100은 100개 클래스\n",
    "\n",
    "def train(model, trainloader, criterion, optimizer, device, epoch, warmup_scheduler=None, warmup_epochs=5):\n",
    "    \"\"\"\n",
    "    학습 함수 (CutMix 적용)\n",
    "    \"\"\"\n",
    "    model.train()   # 모델을 학습 모드로 설정\n",
    "    start_time = time.time()  # 시간 측정 시작\n",
    "    running_loss = 0.0\n",
    "    correct_top1 = 0\n",
    "    correct_top5 = 0\n",
    "    total = 0\n",
    "    \n",
    "    for i, (inputs, labels) in enumerate(trainloader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "\n",
    "        # CutMix 확률적 적용 - 한 번만 적용하고 같은 입력/레이블 유지\n",
    "        original_inputs = inputs.clone()\n",
    "        original_labels = labels.clone()\n",
    "        \n",
    "        # CutMix 확률적 적용\n",
    "        if random.random() < config[\"cutmix_prob\"]:\n",
    "            inputs, labels = cutmix(inputs, labels)\n",
    "            # 이 경우 labels은 원-핫 인코딩 형태로 변환됨\n",
    "            use_cutmix = True\n",
    "        else:\n",
    "            use_cutmix = False\n",
    "\n",
    "        # SAM의 첫 번째 스텝 (가중치 섭동)        \n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # CutMix 적용 여부에 따라 손실 함수 선택\n",
    "        if use_cutmix:\n",
    "            # CutMix가 적용된 경우 (원-핫 인코딩된 레이블)\n",
    "            loss = torch.nn.functional.cross_entropy(outputs, labels)\n",
    "        else:\n",
    "            # 일반적인 경우 (정수 인덱스 레이블)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.first_step(zero_grad=True)\n",
    "\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # 동일한 손실 계산 방식 사용\n",
    "        if use_cutmix:\n",
    "            loss_second = torch.nn.functional.cross_entropy(outputs, labels)\n",
    "        else:\n",
    "            loss_second = criterion(outputs, labels)\n",
    "\n",
    "\n",
    "        # SAM의 두 번째 스텝 (가중치 섭동)        \n",
    "        loss_second.backward()\n",
    "\n",
    "\n",
    "        # SAM의 두 번째 스텝 (실제 업데이트)\n",
    "        optimizer.second_step(zero_grad=True)\n",
    "\n",
    "\n",
    "         # 학습률 스케줄러 업데이트 - warmup 스케줄러만 여기서 업데이트\n",
    "        if epoch < warmup_epochs and warmup_scheduler is not None:\n",
    "            warmup_scheduler.step()\n",
    "        \n",
    "        running_loss += loss.item()       \n",
    "\n",
    "        \n",
    "        # 정확도 계산 - CutMix 적용 여부에 따라 다르게 처리\n",
    "        if use_cutmix:\n",
    "            _, label_idx = labels.max(1)\n",
    "        else:\n",
    "            label_idx = labels\n",
    "            \n",
    "        \n",
    "        # top-1 정확도 계산\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += inputs.size(0)\n",
    "        correct_top1 += predicted.eq(label_idx).sum().item()\n",
    "        \n",
    "        # top-5 정확도 계산\n",
    "        _, top5_idx = outputs.topk(5, 1, largest=True, sorted=True)\n",
    "        correct_top5 += sum([1 for i in range(len(label_idx)) if label_idx[i] in top5_idx[i]])\n",
    "        \n",
    "        if (i + 1) % 50 == 0:  # 50 배치마다 출력\n",
    "            print(f'Epoch [{epoch+1}], Batch [{i+1}/{len(trainloader)}], Loss: {loss.item():.4f}, LR: {optimizer.param_groups[0][\"lr\"]:.6f}')\n",
    "    \n",
    "    epoch_loss = running_loss / len(trainloader)\n",
    "    accuracy_top1 = 100.0 * correct_top1 / total\n",
    "    accuracy_top5 = 100.0 * correct_top5 / total\n",
    "    \n",
    "    train_time = time.time() - start_time\n",
    "    \n",
    "    # 학습 세트에 대한 성능 출력\n",
    "    print(f'Train set: Epoch: {epoch+1}, Average loss:{epoch_loss:.4f}, LR: {optimizer.param_groups[0][\"lr\"]:.6f} '\n",
    "          f'Top-1 Accuracy: {accuracy_top1:.4f}%, Top-5 Accuracy: {accuracy_top5:.4f}%, Time consumed:{train_time:.2f}s')\n",
    "    \n",
    "    return epoch_loss, accuracy_top1, accuracy_top5\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device, epoch, phase=\"test\"):\n",
    "    \"\"\"\n",
    "    평가 함수\n",
    "    \"\"\"\n",
    "    model.eval()  # 모델을 평가 모드로 설정\n",
    "    start_time = time.time()  # 시간 측정 시작\n",
    "    \n",
    "    eval_loss = 0.0\n",
    "    correct_top1 = 0\n",
    "    correct_top5 = 0\n",
    "    total = 0\n",
    "    \n",
    "    # 그래디언트 계산 비활성화\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # 순전파\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # 손실 계산\n",
    "            loss = criterion(outputs, labels)\n",
    "            eval_loss += loss.item()\n",
    "            \n",
    "            # top-1 정확도 계산\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct_top1 += (predicted == labels).sum().item()\n",
    "            \n",
    "            # top-5 정확도 계산\n",
    "            _, top5_idx = outputs.topk(5, 1, largest=True, sorted=True)\n",
    "            correct_top5 += top5_idx.eq(labels.view(-1, 1).expand_as(top5_idx)).sum().item()\n",
    "    \n",
    "    # 평균 손실 및 정확도 계산\n",
    "    eval_loss = eval_loss / len(dataloader)\n",
    "    accuracy_top1 = 100.0 * correct_top1 / total\n",
    "    accuracy_top5 = 100.0 * correct_top5 / total\n",
    "    \n",
    "    # 평가 시간 계산\n",
    "    eval_time = time.time() - start_time\n",
    "    \n",
    "    # 테스트 세트에 대한 성능 출력\n",
    "    print(f'{phase.capitalize()} set: Epoch: {epoch+1}, Average loss:{eval_loss:.4f}, '\n",
    "          f'Top-1 Accuracy: {accuracy_top1:.4f}%, Top-5 Accuracy: {accuracy_top5:.4f}%, Time consumed:{eval_time:.2f}s')\n",
    "    print()\n",
    "    \n",
    "    return eval_loss, accuracy_top1, accuracy_top5\n",
    "\n",
    "# 메인 학습 루프\n",
    "def main_training_loop(model, trainloader, testloader, criterion, optimizer, device, num_epochs, patience, max_epochs_wait, warmup_scheduler=None, main_scheduler=None, warmup_epochs=5):\n",
    "    \"\"\"\n",
    "    메인 학습 루프 (accuracy 기준 early stopping)\n",
    "    \"\"\"\n",
    "    # 정확도 기반 얼리 스토핑 사용\n",
    "    early_stopping = AccuracyEarlyStopping(patience=patience, verbose=True, path='checkpoint.pt', max_epochs=max_epochs_wait)\n",
    "    \n",
    "    best_test_acc_top1 = 0.0\n",
    "    best_test_acc_top5 = 0.0\n",
    "    \n",
    "    # 테스트 정확도 기록을 위한 리스트\n",
    "    test_acc_top1_history = []\n",
    "    \n",
    "    # tqdm을 사용한 진행 상황 표시\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        # 학습\n",
    "        train_loss, train_acc_top1, train_acc_top5 = train(\n",
    "            model, \n",
    "            trainloader, \n",
    "            criterion, \n",
    "            optimizer, \n",
    "            device, \n",
    "            epoch, \n",
    "            warmup_scheduler, \n",
    "            warmup_epochs\n",
    "        )\n",
    "        \n",
    "        # 테스트 데이터로 평가\n",
    "        test_loss, test_acc_top1, test_acc_top5 = evaluate(model, testloader, criterion, device, epoch, phase=\"test\")\n",
    "\n",
    "        # 웜업 이후 ReduceLROnPlateau 스케줄러 업데이트 \n",
    "        if epoch >= warmup_epochs and main_scheduler is not None:\n",
    "            main_scheduler.step(test_acc_top1)  # 테스트 정확도에 따라 학습률 업데이트       \n",
    "            \n",
    "        # 테스트 정확도 기록\n",
    "        test_acc_top1_history.append(test_acc_top1)\n",
    "        \n",
    "        # WandB에 로깅\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"learning_rate\": optimizer.param_groups[0]['lr'],\n",
    "            \"train_loss\": train_loss,\n",
    "            \"train_accuracy_top1\": train_acc_top1,\n",
    "            \"train_accuracy_top5\": train_acc_top5,\n",
    "            \"test_loss\": test_loss,\n",
    "            \"test_accuracy_top1\": test_acc_top1,\n",
    "            \"test_accuracy_top5\": test_acc_top5\n",
    "        })\n",
    "            \n",
    "        # 최고 정확도 모델 저장 (top-1 기준)\n",
    "        if test_acc_top1 > best_test_acc_top1:\n",
    "            best_test_acc_top1 = test_acc_top1\n",
    "            best_test_acc_top5_at_best_top1 = test_acc_top5\n",
    "            print(f'새로운 최고 top-1 정확도: {best_test_acc_top1:.2f}%, top-5 정확도: {best_test_acc_top5_at_best_top1:.2f}%')\n",
    "            # 모델 저장\n",
    "            model_path = f'best_model_{wandb.run.name}.pth'\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            \n",
    "            # WandB에 모델 아티팩트 저장\n",
    "            wandb.save(model_path)\n",
    "        \n",
    "        # top-5 accuracy 기록 업데이트\n",
    "        if test_acc_top5 > best_test_acc_top5:\n",
    "            best_test_acc_top5 = test_acc_top5\n",
    "            print(f'새로운 최고 top-5 정확도: {best_test_acc_top5:.2f}%')\n",
    "\n",
    "        # Early stopping 체크 (test_acc_top1 기준)\n",
    "        early_stopping(test_acc_top1, model, epoch)\n",
    "        if early_stopping.early_stop:\n",
    "            print(f\"에폭 {epoch+1}에서 학습 조기 종료. 최고 성능 에폭: {early_stopping.best_epoch+1}\")\n",
    "            break\n",
    "    \n",
    "    # 훈련 완료 후 최고 모델 로드\n",
    "    print(\"테스트 정확도 기준 최고 모델 로드 중...\")\n",
    "    model_path = f'best_model_{wandb.run.name}.pth'\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "    # 최종 테스트 세트 평가\n",
    "    final_test_loss, final_test_acc_top1, final_test_acc_top5 = evaluate(model, testloader, criterion, device, num_epochs-1, phase=\"test\")\n",
    "    \n",
    "    print(f'완료! 최고 테스트 top-1 정확도: {best_test_acc_top1:.2f}%, 최고 테스트 top-5 정확도: {best_test_acc_top5:.2f}%')\n",
    "    print(f'최종 테스트 top-1 정확도: {final_test_acc_top1:.2f}%, 최종 테스트 top-5 정확도: {final_test_acc_top5:.2f}%')\n",
    "    \n",
    "    # WandB에 최종 결과 기록\n",
    "    wandb.run.summary[\"best_test_accuracy_top1\"] = best_test_acc_top1\n",
    "    wandb.run.summary[\"best_test_accuracy_top5\"] = best_test_acc_top5\n",
    "    wandb.run.summary[\"final_test_accuracy_top1\"] = final_test_acc_top1\n",
    "    wandb.run.summary[\"final_test_accuracy_top5\"] = final_test_acc_top5\n",
    "\n",
    "    # Early stopping 정보 저장\n",
    "    if early_stopping.early_stop:\n",
    "        wandb.run.summary[\"early_stopped\"] = True\n",
    "        wandb.run.summary[\"early_stopped_epoch\"] = epoch+1\n",
    "        wandb.run.summary[\"best_epoch\"] = early_stopping.best_epoch+1\n",
    "    else:\n",
    "        wandb.run.summary[\"early_stopped\"] = False\n",
    "\n",
    "\n",
    "# 디바이스 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 모델 초기화\n",
    "# 또는 매개변수 커스터마이징\n",
    "model = ShakePyramidNet(depth=110, alpha=270, label=100).to(device)\n",
    "criterion = nn.CrossEntropyLoss()  # 기본 CrossEntropyLoss 사용 (라벨 스무딩 없음)\n",
    "base_optimizer = torch.optim.SGD\n",
    "optimizer = SAM(model.parameters(), base_optimizer, lr=config[\"learning_rate\"], momentum=config[\"momentum\"],weight_decay=config[\"weight_decay\"],nesterov=True)\n",
    "\n",
    "\n",
    "\n",
    "# WarmUpLR 스케줄러 초기화\n",
    "# 웜업할 총 iteration 수 계산 (웜업 에폭 × 배치 수)\n",
    "warmup_steps = config[\"warmup_epochs\"] * len(trainloader)\n",
    "warmup_scheduler = WarmUpLR(optimizer, total_iters=warmup_steps)\n",
    "\n",
    "# 웜업 이후 사용할 스케줄러 설정 \n",
    "main_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, \n",
    "    mode='max',           # 정확도를 모니터링하므로 'max' 모드 사용\n",
    "    factor=0.5,           # 학습률 감소 비율 (5배 감소)\n",
    "    patience=5,           # 몇 에폭 동안 개선이 없을 때 감소시킬지\n",
    "    verbose=True,         # 학습률 변경 시 출력\n",
    "    threshold=0.01,        # 개선으로 간주할 최소 변화량\n",
    "    min_lr=1e-5\n",
    ")\n",
    "\n",
    "# WandB에 모델 구조 기록\n",
    "wandb.watch(model, log=\"all\")\n",
    "\n",
    "# GPU 가속\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"{torch.cuda.device_count()}개의 GPU를 사용합니다.\")\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "# 훈련 시작 시간 기록\n",
    "start_time = time.time()\n",
    "\n",
    "# 메인 학습 루프 호출\n",
    "main_training_loop(\n",
    "    model=model,\n",
    "    trainloader=trainloader,\n",
    "    testloader=testloader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    num_epochs=config[\"num_epochs\"],\n",
    "    patience=config[\"patience\"],\n",
    "    max_epochs_wait=config[\"max_epochs_wait\"],\n",
    "    warmup_scheduler=warmup_scheduler,\n",
    "    main_scheduler=main_scheduler,\n",
    "    warmup_epochs=config[\"warmup_epochs\"]\n",
    ")\n",
    "\n",
    "# 훈련 종료 시간 기록 및 출력\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "wandb.log({\"total_training_time\": total_time})\n",
    "\n",
    "print(f\"전체 학습 시간: {total_time:.2f} 초\")\n",
    "\n",
    "# WandB 실행 종료\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57b9218-aa24-4297-bb92-473c215e6767",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
