{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516f9d5f-6361-4b03-98ee-e1ba4de86b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/guswls/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/guswls/EEN4015_Intro2ML/pbl-2/wandb/run-20250428_052156-24tufm42</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sokjh1310-hanyang-university/PBL-2/runs/24tufm42' target=\"_blank\">resnet18_cfc,lr=0.005,factor=0.5,SAM_SGD,PSKD</a></strong> to <a href='https://wandb.ai/sokjh1310-hanyang-university/PBL-2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sokjh1310-hanyang-university/PBL-2' target=\"_blank\">https://wandb.ai/sokjh1310-hanyang-university/PBL-2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sokjh1310-hanyang-university/PBL-2/runs/24tufm42' target=\"_blank\">https://wandb.ai/sokjh1310-hanyang-university/PBL-2/runs/24tufm42</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Train set size: 50000\n",
      "Test set size: 10000\n",
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2개의 GPU를 사용합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                       | 0/300 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1], Batch [50/391], Loss: 4.9732, LR: 0.000064\n",
      "Epoch [1], Batch [100/391], Loss: 4.9096, LR: 0.000128\n",
      "Epoch [1], Batch [150/391], Loss: 4.8186, LR: 0.000192\n",
      "Epoch [1], Batch [200/391], Loss: 4.7684, LR: 0.000256\n",
      "Epoch [1], Batch [250/391], Loss: 4.7386, LR: 0.000320\n",
      "Epoch [1], Batch [300/391], Loss: 4.7178, LR: 0.000384\n",
      "Epoch [1], Batch [350/391], Loss: 4.6032, LR: 0.000448\n",
      "Train set: Epoch: 1, Average loss:4.7695, LR: 0.000500 Top-1 Accuracy: 0.6040%, Top-5 Accuracy: 3.5700%, Time consumed:90.17s\n",
      "Test set: Epoch: 1, Average loss:4.2991, Top-1 Accuracy: 6.2500%, Top-5 Accuracy: 19.9600%, Time consumed:9.84s\n",
      "\n",
      "새로운 최고 top-1 정확도: 6.25%, top-5 정확도: 19.96%\n",
      "새로운 최고 top-5 정확도: 19.96%\n",
      "Accuracy improved (-inf% --> 6.25%). Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▎                                                                                           | 1/300 [01:40<8:19:28, 100.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2], Batch [50/391], Loss: 4.5279, LR: 0.000564\n",
      "Epoch [2], Batch [100/391], Loss: 4.4057, LR: 0.000628\n",
      "Epoch [2], Batch [150/391], Loss: 4.3585, LR: 0.000692\n",
      "Epoch [2], Batch [200/391], Loss: 4.3938, LR: 0.000756\n",
      "Epoch [2], Batch [250/391], Loss: 4.4378, LR: 0.000820\n",
      "Epoch [2], Batch [300/391], Loss: 4.1473, LR: 0.000884\n",
      "Epoch [2], Batch [350/391], Loss: 4.3206, LR: 0.000948\n",
      "Train set: Epoch: 2, Average loss:4.3915, LR: 0.001000 Top-1 Accuracy: 3.6400%, Top-5 Accuracy: 15.0160%, Time consumed:91.03s\n",
      "Test set: Epoch: 2, Average loss:3.8643, Top-1 Accuracy: 10.8100%, Top-5 Accuracy: 32.2900%, Time consumed:9.92s\n",
      "\n",
      "새로운 최고 top-1 정확도: 10.81%, top-5 정확도: 32.29%\n",
      "새로운 최고 top-5 정확도: 32.29%\n",
      "Accuracy improved (6.25% --> 10.81%). Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▌                                                                                           | 2/300 [03:21<8:20:34, 100.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3], Batch [50/391], Loss: 4.0792, LR: 0.001064\n",
      "Epoch [3], Batch [100/391], Loss: 4.1785, LR: 0.001128\n",
      "Epoch [3], Batch [150/391], Loss: 4.3421, LR: 0.001192\n",
      "Epoch [3], Batch [200/391], Loss: 4.4253, LR: 0.001256\n",
      "Epoch [3], Batch [250/391], Loss: 4.3922, LR: 0.001320\n",
      "Epoch [3], Batch [300/391], Loss: 4.3951, LR: 0.001384\n",
      "Epoch [3], Batch [350/391], Loss: 4.1225, LR: 0.001448\n",
      "Train set: Epoch: 3, Average loss:4.1310, LR: 0.001500 Top-1 Accuracy: 7.4560%, Top-5 Accuracy: 24.8980%, Time consumed:94.06s\n",
      "Test set: Epoch: 3, Average loss:3.6318, Top-1 Accuracy: 14.0600%, Top-5 Accuracy: 37.7900%, Time consumed:9.84s\n",
      "\n",
      "새로운 최고 top-1 정확도: 14.06%, top-5 정확도: 37.79%\n",
      "새로운 최고 top-5 정확도: 37.79%\n",
      "Accuracy improved (10.81% --> 14.06%). Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▉                                                                                           | 3/300 [05:05<8:26:27, 102.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4], Batch [50/391], Loss: 3.8114, LR: 0.001564\n",
      "Epoch [4], Batch [100/391], Loss: 4.0453, LR: 0.001628\n",
      "Epoch [4], Batch [150/391], Loss: 4.2864, LR: 0.001692\n",
      "Epoch [4], Batch [200/391], Loss: 3.9626, LR: 0.001756\n",
      "Epoch [4], Batch [250/391], Loss: 3.6952, LR: 0.001820\n",
      "Epoch [4], Batch [300/391], Loss: 3.9831, LR: 0.001884\n",
      "Epoch [4], Batch [350/391], Loss: 3.6013, LR: 0.001948\n",
      "Train set: Epoch: 4, Average loss:3.9606, LR: 0.002000 Top-1 Accuracy: 10.3080%, Top-5 Accuracy: 30.8260%, Time consumed:95.01s\n",
      "Test set: Epoch: 4, Average loss:3.3625, Top-1 Accuracy: 19.0300%, Top-5 Accuracy: 46.3100%, Time consumed:9.79s\n",
      "\n",
      "새로운 최고 top-1 정확도: 19.03%, top-5 정확도: 46.31%\n",
      "새로운 최고 top-5 정확도: 46.31%\n",
      "Accuracy improved (14.06% --> 19.03%). Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|█▏                                                                                          | 4/300 [06:50<8:30:07, 103.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5], Batch [50/391], Loss: 3.7198, LR: 0.002064\n",
      "Epoch [5], Batch [100/391], Loss: 3.5496, LR: 0.002128\n",
      "Epoch [5], Batch [150/391], Loss: 3.5384, LR: 0.002192\n",
      "Epoch [5], Batch [200/391], Loss: 4.2356, LR: 0.002256\n",
      "Epoch [5], Batch [250/391], Loss: 3.9693, LR: 0.002320\n",
      "Epoch [5], Batch [300/391], Loss: 3.3968, LR: 0.002384\n",
      "Epoch [5], Batch [350/391], Loss: 3.3714, LR: 0.002448\n",
      "Train set: Epoch: 5, Average loss:3.7787, LR: 0.002500 Top-1 Accuracy: 13.3400%, Top-5 Accuracy: 36.9760%, Time consumed:98.24s\n",
      "Test set: Epoch: 5, Average loss:3.1157, Top-1 Accuracy: 23.8900%, Top-5 Accuracy: 53.1100%, Time consumed:10.58s\n",
      "\n",
      "새로운 최고 top-1 정확도: 23.89%, top-5 정확도: 53.11%\n",
      "새로운 최고 top-5 정확도: 53.11%\n",
      "Accuracy improved (19.03% --> 23.89%). Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|█▌                                                                                          | 5/300 [08:39<8:38:27, 105.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6], Batch [50/391], Loss: 3.3444, LR: 0.002564\n",
      "Epoch [6], Batch [100/391], Loss: 4.4375, LR: 0.002628\n",
      "Epoch [6], Batch [150/391], Loss: 3.7995, LR: 0.002692\n",
      "Epoch [6], Batch [200/391], Loss: 3.3022, LR: 0.002756\n",
      "Epoch [6], Batch [250/391], Loss: 3.0831, LR: 0.002820\n",
      "Epoch [6], Batch [300/391], Loss: 3.9863, LR: 0.002884\n",
      "Epoch [6], Batch [350/391], Loss: 3.0163, LR: 0.002948\n",
      "Train set: Epoch: 6, Average loss:3.5969, LR: 0.003000 Top-1 Accuracy: 17.1120%, Top-5 Accuracy: 42.8860%, Time consumed:99.75s\n",
      "Test set: Epoch: 6, Average loss:2.8822, Top-1 Accuracy: 27.5100%, Top-5 Accuracy: 59.3600%, Time consumed:10.10s\n",
      "\n",
      "새로운 최고 top-1 정확도: 27.51%, top-5 정확도: 59.36%\n",
      "새로운 최고 top-5 정확도: 59.36%\n",
      "Accuracy improved (23.89% --> 27.51%). Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|█▊                                                                                          | 6/300 [10:29<8:44:27, 107.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7], Batch [50/391], Loss: 2.9926, LR: 0.003064\n",
      "Epoch [7], Batch [100/391], Loss: 4.0514, LR: 0.003128\n",
      "Epoch [7], Batch [150/391], Loss: 4.1094, LR: 0.003192\n",
      "Epoch [7], Batch [200/391], Loss: 2.8666, LR: 0.003256\n",
      "Epoch [7], Batch [250/391], Loss: 2.9044, LR: 0.003320\n",
      "Epoch [7], Batch [300/391], Loss: 2.8718, LR: 0.003384\n",
      "Epoch [7], Batch [350/391], Loss: 3.4970, LR: 0.003448\n",
      "Train set: Epoch: 7, Average loss:3.3792, LR: 0.003500 Top-1 Accuracy: 21.0480%, Top-5 Accuracy: 48.9500%, Time consumed:91.52s\n",
      "Test set: Epoch: 7, Average loss:2.6941, Top-1 Accuracy: 30.9600%, Top-5 Accuracy: 63.0800%, Time consumed:11.14s\n",
      "\n",
      "새로운 최고 top-1 정확도: 30.96%, top-5 정확도: 63.08%\n",
      "새로운 최고 top-5 정확도: 63.08%\n",
      "Accuracy improved (27.51% --> 30.96%). Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|██▏                                                                                         | 7/300 [12:12<8:36:04, 105.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8], Batch [50/391], Loss: 3.0321, LR: 0.003564\n",
      "Epoch [8], Batch [100/391], Loss: 3.4205, LR: 0.003628\n",
      "Epoch [8], Batch [150/391], Loss: 3.6028, LR: 0.003692\n",
      "Epoch [8], Batch [200/391], Loss: 3.7201, LR: 0.003756\n",
      "Epoch [8], Batch [250/391], Loss: 3.8437, LR: 0.003820\n",
      "Epoch [8], Batch [300/391], Loss: 2.7368, LR: 0.003884\n",
      "Epoch [8], Batch [350/391], Loss: 3.9526, LR: 0.003948\n",
      "Train set: Epoch: 8, Average loss:3.2319, LR: 0.004000 Top-1 Accuracy: 24.5780%, Top-5 Accuracy: 53.9800%, Time consumed:90.46s\n",
      "Test set: Epoch: 8, Average loss:2.3894, Top-1 Accuracy: 37.8600%, Top-5 Accuracy: 70.0000%, Time consumed:9.95s\n",
      "\n",
      "새로운 최고 top-1 정확도: 37.86%, top-5 정확도: 70.00%\n",
      "새로운 최고 top-5 정확도: 70.00%\n",
      "Accuracy improved (30.96% --> 37.86%). Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|██▍                                                                                         | 8/300 [13:53<8:26:31, 104.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9], Batch [50/391], Loss: 2.6943, LR: 0.004064\n",
      "Epoch [9], Batch [100/391], Loss: 2.6861, LR: 0.004128\n",
      "Epoch [9], Batch [150/391], Loss: 2.7790, LR: 0.004192\n",
      "Epoch [9], Batch [200/391], Loss: 3.7634, LR: 0.004256\n",
      "Epoch [9], Batch [250/391], Loss: 2.5081, LR: 0.004320\n",
      "Epoch [9], Batch [300/391], Loss: 2.4775, LR: 0.004384\n",
      "Epoch [9], Batch [350/391], Loss: 2.7157, LR: 0.004448\n",
      "Train set: Epoch: 9, Average loss:3.0385, LR: 0.004500 Top-1 Accuracy: 28.7160%, Top-5 Accuracy: 58.8100%, Time consumed:91.25s\n",
      "Test set: Epoch: 9, Average loss:2.1976, Top-1 Accuracy: 41.6500%, Top-5 Accuracy: 74.1600%, Time consumed:9.82s\n",
      "\n",
      "새로운 최고 top-1 정확도: 41.65%, top-5 정확도: 74.16%\n",
      "새로운 최고 top-5 정확도: 74.16%\n",
      "Accuracy improved (37.86% --> 41.65%). Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|██▊                                                                                         | 9/300 [15:34<8:20:35, 103.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10], Batch [50/391], Loss: 2.4351, LR: 0.004564\n",
      "Epoch [10], Batch [100/391], Loss: 3.3036, LR: 0.004628\n",
      "Epoch [10], Batch [150/391], Loss: 3.7438, LR: 0.004692\n",
      "Epoch [10], Batch [200/391], Loss: 3.1072, LR: 0.004756\n",
      "Epoch [10], Batch [250/391], Loss: 2.5074, LR: 0.004820\n",
      "Epoch [10], Batch [300/391], Loss: 3.6719, LR: 0.004884\n",
      "Epoch [10], Batch [350/391], Loss: 2.3149, LR: 0.004948\n",
      "Train set: Epoch: 10, Average loss:3.0364, LR: 0.005000 Top-1 Accuracy: 29.4840%, Top-5 Accuracy: 59.5780%, Time consumed:90.81s\n",
      "Test set: Epoch: 10, Average loss:2.0735, Top-1 Accuracy: 45.4500%, Top-5 Accuracy: 76.4000%, Time consumed:9.83s\n",
      "\n",
      "새로운 최고 top-1 정확도: 45.45%, top-5 정확도: 76.40%\n",
      "새로운 최고 top-5 정확도: 76.40%\n",
      "Accuracy improved (41.65% --> 45.45%). Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|███                                                                                        | 10/300 [17:15<8:15:25, 102.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11], Batch [50/391], Loss: 2.4716, LR: 0.005000\n",
      "Epoch [11], Batch [100/391], Loss: 2.1357, LR: 0.005000\n",
      "Epoch [11], Batch [150/391], Loss: 2.3024, LR: 0.005000\n",
      "Epoch [11], Batch [200/391], Loss: 2.3214, LR: 0.005000\n",
      "Epoch [11], Batch [250/391], Loss: 2.4612, LR: 0.005000\n",
      "Epoch [11], Batch [300/391], Loss: 2.2858, LR: 0.005000\n",
      "Epoch [11], Batch [350/391], Loss: 2.0669, LR: 0.005000\n",
      "Train set: Epoch: 11, Average loss:2.8274, LR: 0.005000 Top-1 Accuracy: 34.3600%, Top-5 Accuracy: 64.7940%, Time consumed:90.66s\n",
      "Test set: Epoch: 11, Average loss:1.9058, Top-1 Accuracy: 48.6900%, Top-5 Accuracy: 78.9300%, Time consumed:9.79s\n",
      "\n",
      "새로운 최고 top-1 정확도: 48.69%, top-5 정확도: 78.93%\n",
      "새로운 최고 top-5 정확도: 78.93%\n",
      "Accuracy improved (45.45% --> 48.69%). Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|███▎                                                                                       | 11/300 [18:56<8:11:03, 101.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12], Batch [50/391], Loss: 2.9009, LR: 0.005000\n",
      "Epoch [12], Batch [100/391], Loss: 3.8773, LR: 0.005000\n",
      "Epoch [12], Batch [150/391], Loss: 2.2622, LR: 0.005000\n",
      "Epoch [12], Batch [200/391], Loss: 2.3592, LR: 0.005000\n",
      "Epoch [12], Batch [250/391], Loss: 2.3454, LR: 0.005000\n",
      "Epoch [12], Batch [300/391], Loss: 2.9598, LR: 0.005000\n",
      "Epoch [12], Batch [350/391], Loss: 3.7301, LR: 0.005000\n",
      "Train set: Epoch: 12, Average loss:2.7885, LR: 0.005000 Top-1 Accuracy: 35.8960%, Top-5 Accuracy: 65.8100%, Time consumed:90.63s\n",
      "Test set: Epoch: 12, Average loss:1.8192, Top-1 Accuracy: 50.6300%, Top-5 Accuracy: 80.9800%, Time consumed:9.75s\n",
      "\n",
      "새로운 최고 top-1 정확도: 50.63%, top-5 정확도: 80.98%\n",
      "새로운 최고 top-5 정확도: 80.98%\n",
      "Accuracy improved (48.69% --> 50.63%). Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|███▋                                                                                       | 12/300 [20:36<8:07:25, 101.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13], Batch [50/391], Loss: 1.8094, LR: 0.005000\n",
      "Epoch [13], Batch [100/391], Loss: 2.2016, LR: 0.005000\n",
      "Epoch [13], Batch [150/391], Loss: 2.0627, LR: 0.005000\n",
      "Epoch [13], Batch [200/391], Loss: 1.9166, LR: 0.005000\n",
      "Epoch [13], Batch [250/391], Loss: 1.8641, LR: 0.005000\n",
      "Epoch [13], Batch [300/391], Loss: 2.1169, LR: 0.005000\n",
      "Epoch [13], Batch [350/391], Loss: 2.1987, LR: 0.005000\n",
      "Train set: Epoch: 13, Average loss:2.6104, LR: 0.005000 Top-1 Accuracy: 38.5560%, Top-5 Accuracy: 68.6940%, Time consumed:93.58s\n",
      "Test set: Epoch: 13, Average loss:1.7395, Top-1 Accuracy: 52.5900%, Top-5 Accuracy: 82.1100%, Time consumed:9.74s\n",
      "\n",
      "새로운 최고 top-1 정확도: 52.59%, top-5 정확도: 82.11%\n",
      "새로운 최고 top-5 정확도: 82.11%\n",
      "Accuracy improved (50.63% --> 52.59%). Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|███▉                                                                                       | 13/300 [22:20<8:08:38, 102.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14], Batch [50/391], Loss: 3.5123, LR: 0.005000\n",
      "Epoch [14], Batch [100/391], Loss: 1.9120, LR: 0.005000\n",
      "Epoch [14], Batch [150/391], Loss: 3.4605, LR: 0.005000\n",
      "Epoch [14], Batch [200/391], Loss: 1.8357, LR: 0.005000\n",
      "Epoch [14], Batch [250/391], Loss: 3.1437, LR: 0.005000\n",
      "Epoch [14], Batch [300/391], Loss: 3.2711, LR: 0.005000\n",
      "Epoch [14], Batch [350/391], Loss: 1.7658, LR: 0.005000\n",
      "Train set: Epoch: 14, Average loss:2.5546, LR: 0.005000 Top-1 Accuracy: 41.0760%, Top-5 Accuracy: 70.9820%, Time consumed:90.51s\n",
      "Test set: Epoch: 14, Average loss:1.6520, Top-1 Accuracy: 54.9700%, Top-5 Accuracy: 83.3100%, Time consumed:10.19s\n",
      "\n",
      "새로운 최고 top-1 정확도: 54.97%, top-5 정확도: 83.31%\n",
      "새로운 최고 top-5 정확도: 83.31%\n",
      "Accuracy improved (52.59% --> 54.97%). Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|████▏                                                                                      | 14/300 [24:01<8:05:12, 101.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15], Batch [50/391], Loss: 1.6628, LR: 0.005000\n",
      "Epoch [15], Batch [100/391], Loss: 1.8655, LR: 0.005000\n",
      "Epoch [15], Batch [150/391], Loss: 3.8524, LR: 0.005000\n",
      "Epoch [15], Batch [200/391], Loss: 1.6760, LR: 0.005000\n",
      "Epoch [15], Batch [250/391], Loss: 1.7980, LR: 0.005000\n",
      "Epoch [15], Batch [300/391], Loss: 2.6826, LR: 0.005000\n",
      "Epoch [15], Batch [350/391], Loss: 1.6335, LR: 0.005000\n",
      "Train set: Epoch: 15, Average loss:2.4410, LR: 0.005000 Top-1 Accuracy: 43.6600%, Top-5 Accuracy: 73.1760%, Time consumed:90.92s\n",
      "Test set: Epoch: 15, Average loss:1.5463, Top-1 Accuracy: 57.5200%, Top-5 Accuracy: 84.8500%, Time consumed:9.86s\n",
      "\n",
      "새로운 최고 top-1 정확도: 57.52%, top-5 정확도: 84.85%\n",
      "새로운 최고 top-5 정확도: 84.85%\n",
      "Accuracy improved (54.97% --> 57.52%). Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|████▌                                                                                      | 15/300 [25:42<8:02:25, 101.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16], Batch [50/391], Loss: 3.6402, LR: 0.005000\n",
      "Epoch [16], Batch [100/391], Loss: 3.6492, LR: 0.005000\n",
      "Epoch [16], Batch [150/391], Loss: 1.5948, LR: 0.005000\n",
      "Epoch [16], Batch [200/391], Loss: 3.3127, LR: 0.005000\n",
      "Epoch [16], Batch [250/391], Loss: 3.5580, LR: 0.005000\n",
      "Epoch [16], Batch [300/391], Loss: 1.7533, LR: 0.005000\n",
      "Epoch [16], Batch [350/391], Loss: 1.7419, LR: 0.005000\n",
      "Train set: Epoch: 16, Average loss:2.3662, LR: 0.005000 Top-1 Accuracy: 45.3740%, Top-5 Accuracy: 74.8120%, Time consumed:89.16s\n",
      "Test set: Epoch: 16, Average loss:1.5168, Top-1 Accuracy: 58.6300%, Top-5 Accuracy: 85.4400%, Time consumed:9.70s\n",
      "\n",
      "새로운 최고 top-1 정확도: 58.63%, top-5 정확도: 85.44%\n",
      "새로운 최고 top-5 정확도: 85.44%\n",
      "Accuracy improved (57.52% --> 58.63%). Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|████▊                                                                                      | 16/300 [27:21<7:57:13, 100.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17], Batch [50/391], Loss: 3.6055, LR: 0.005000\n",
      "Epoch [17], Batch [100/391], Loss: 1.8238, LR: 0.005000\n",
      "Epoch [17], Batch [150/391], Loss: 2.6977, LR: 0.005000\n",
      "Epoch [17], Batch [200/391], Loss: 2.1473, LR: 0.005000\n",
      "Epoch [17], Batch [250/391], Loss: 3.2212, LR: 0.005000\n",
      "Epoch [17], Batch [300/391], Loss: 3.5358, LR: 0.005000\n",
      "Epoch [17], Batch [350/391], Loss: 1.6744, LR: 0.005000\n",
      "Train set: Epoch: 17, Average loss:2.3261, LR: 0.005000 Top-1 Accuracy: 46.4820%, Top-5 Accuracy: 75.7220%, Time consumed:95.41s\n",
      "Test set: Epoch: 17, Average loss:1.4425, Top-1 Accuracy: 59.8200%, Top-5 Accuracy: 86.8000%, Time consumed:9.88s\n",
      "\n",
      "새로운 최고 top-1 정확도: 59.82%, top-5 정확도: 86.80%\n",
      "새로운 최고 top-5 정확도: 86.80%\n",
      "Accuracy improved (58.63% --> 59.82%). Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|█████▏                                                                                     | 17/300 [29:07<8:02:14, 102.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18], Batch [50/391], Loss: 1.6645, LR: 0.005000\n",
      "Epoch [18], Batch [100/391], Loss: 1.5558, LR: 0.005000\n",
      "Epoch [18], Batch [150/391], Loss: 3.4983, LR: 0.005000\n",
      "Epoch [18], Batch [200/391], Loss: 1.5626, LR: 0.005000\n",
      "Epoch [18], Batch [250/391], Loss: 2.8552, LR: 0.005000\n",
      "Epoch [18], Batch [300/391], Loss: 3.2791, LR: 0.005000\n",
      "Epoch [18], Batch [350/391], Loss: 3.4837, LR: 0.005000\n",
      "Train set: Epoch: 18, Average loss:2.3286, LR: 0.005000 Top-1 Accuracy: 46.7600%, Top-5 Accuracy: 75.2440%, Time consumed:89.57s\n",
      "Test set: Epoch: 18, Average loss:1.4274, Top-1 Accuracy: 60.3000%, Top-5 Accuracy: 87.0200%, Time consumed:9.57s\n",
      "\n",
      "새로운 최고 top-1 정확도: 60.30%, top-5 정확도: 87.02%\n",
      "새로운 최고 top-5 정확도: 87.02%\n",
      "Accuracy improved (59.82% --> 60.30%). Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|█████▍                                                                                     | 18/300 [30:46<7:56:29, 101.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19], Batch [50/391], Loss: 1.4915, LR: 0.005000\n",
      "Epoch [19], Batch [100/391], Loss: 2.7134, LR: 0.005000\n",
      "Epoch [19], Batch [150/391], Loss: 2.8871, LR: 0.005000\n",
      "Epoch [19], Batch [200/391], Loss: 1.2730, LR: 0.005000\n",
      "Epoch [19], Batch [250/391], Loss: 2.2721, LR: 0.005000\n",
      "Epoch [19], Batch [300/391], Loss: 3.2194, LR: 0.005000\n",
      "Epoch [19], Batch [350/391], Loss: 1.5917, LR: 0.005000\n",
      "Train set: Epoch: 19, Average loss:2.2189, LR: 0.005000 Top-1 Accuracy: 49.5340%, Top-5 Accuracy: 78.1260%, Time consumed:94.22s\n",
      "Test set: Epoch: 19, Average loss:1.4018, Top-1 Accuracy: 61.5900%, Top-5 Accuracy: 87.2800%, Time consumed:9.72s\n",
      "\n",
      "새로운 최고 top-1 정확도: 61.59%, top-5 정확도: 87.28%\n",
      "새로운 최고 top-5 정확도: 87.28%\n",
      "Accuracy improved (60.30% --> 61.59%). Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|█████▊                                                                                     | 19/300 [32:30<7:58:50, 102.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20], Batch [50/391], Loss: 1.3804, LR: 0.005000\n",
      "Epoch [20], Batch [100/391], Loss: 3.7189, LR: 0.005000\n",
      "Epoch [20], Batch [150/391], Loss: 2.5071, LR: 0.005000\n",
      "Epoch [20], Batch [200/391], Loss: 3.3978, LR: 0.005000\n",
      "Epoch [20], Batch [250/391], Loss: 1.3433, LR: 0.005000\n",
      "Epoch [20], Batch [300/391], Loss: 2.3419, LR: 0.005000\n",
      "Epoch [20], Batch [350/391], Loss: 2.6478, LR: 0.005000\n",
      "Train set: Epoch: 20, Average loss:2.2245, LR: 0.005000 Top-1 Accuracy: 50.0780%, Top-5 Accuracy: 78.4640%, Time consumed:88.91s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|██████                                                                                     | 20/300 [34:09<7:52:26, 101.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Epoch: 20, Average loss:1.4332, Top-1 Accuracy: 60.7400%, Top-5 Accuracy: 86.6200%, Time consumed:9.96s\n",
      "\n",
      "EarlyStopping 카운터: 1 / 30\n",
      "Epoch [21], Batch [50/391], Loss: 1.3213, LR: 0.005000\n",
      "Epoch [21], Batch [100/391], Loss: 1.3450, LR: 0.005000\n",
      "Epoch [21], Batch [150/391], Loss: 1.3688, LR: 0.005000\n",
      "Epoch [21], Batch [200/391], Loss: 3.4119, LR: 0.005000\n",
      "Epoch [21], Batch [250/391], Loss: 1.2165, LR: 0.005000\n",
      "Epoch [21], Batch [300/391], Loss: 3.5421, LR: 0.005000\n",
      "Epoch [21], Batch [350/391], Loss: 1.5751, LR: 0.005000\n",
      "Train set: Epoch: 21, Average loss:2.1649, LR: 0.005000 Top-1 Accuracy: 50.8240%, Top-5 Accuracy: 78.7600%, Time consumed:90.49s\n",
      "Test set: Epoch: 21, Average loss:1.3560, Top-1 Accuracy: 62.2400%, Top-5 Accuracy: 88.1200%, Time consumed:9.69s\n",
      "\n",
      "새로운 최고 top-1 정확도: 62.24%, top-5 정확도: 88.12%\n",
      "새로운 최고 top-5 정확도: 88.12%\n",
      "Accuracy improved (61.59% --> 62.24%). Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|██████▎                                                                                    | 21/300 [35:49<7:49:35, 100.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22], Batch [50/391], Loss: 1.3518, LR: 0.005000\n",
      "Epoch [22], Batch [100/391], Loss: 1.6357, LR: 0.005000\n",
      "Epoch [22], Batch [150/391], Loss: 1.3852, LR: 0.005000\n",
      "Epoch [22], Batch [200/391], Loss: 2.6832, LR: 0.005000\n",
      "Epoch [22], Batch [250/391], Loss: 3.5016, LR: 0.005000\n",
      "Epoch [22], Batch [300/391], Loss: 1.2707, LR: 0.005000\n",
      "Epoch [22], Batch [350/391], Loss: 1.4165, LR: 0.005000\n",
      "Train set: Epoch: 22, Average loss:2.1677, LR: 0.005000 Top-1 Accuracy: 51.2020%, Top-5 Accuracy: 79.1220%, Time consumed:89.27s\n",
      "Test set: Epoch: 22, Average loss:1.2986, Top-1 Accuracy: 63.8700%, Top-5 Accuracy: 88.9500%, Time consumed:9.68s\n",
      "\n",
      "새로운 최고 top-1 정확도: 63.87%, top-5 정확도: 88.95%\n",
      "새로운 최고 top-5 정확도: 88.95%\n",
      "Accuracy improved (62.24% --> 63.87%). Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|██████▋                                                                                    | 22/300 [37:29<7:45:26, 100.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [23], Batch [50/391], Loss: 3.0101, LR: 0.005000\n",
      "Epoch [23], Batch [100/391], Loss: 1.3623, LR: 0.005000\n",
      "Epoch [23], Batch [150/391], Loss: 3.4831, LR: 0.005000\n",
      "Epoch [23], Batch [200/391], Loss: 2.1887, LR: 0.005000\n",
      "Epoch [23], Batch [250/391], Loss: 2.1569, LR: 0.005000\n",
      "Epoch [23], Batch [300/391], Loss: 1.3070, LR: 0.005000\n",
      "Epoch [23], Batch [350/391], Loss: 1.5682, LR: 0.005000\n",
      "Train set: Epoch: 23, Average loss:2.0993, LR: 0.005000 Top-1 Accuracy: 52.5940%, Top-5 Accuracy: 79.6580%, Time consumed:88.43s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|███████                                                                                     | 23/300 [39:08<7:41:36, 99.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Epoch: 23, Average loss:1.3566, Top-1 Accuracy: 62.9400%, Top-5 Accuracy: 87.8700%, Time consumed:10.46s\n",
      "\n",
      "EarlyStopping 카운터: 1 / 30\n",
      "Epoch [24], Batch [50/391], Loss: 3.2307, LR: 0.005000\n",
      "Epoch [24], Batch [100/391], Loss: 1.4392, LR: 0.005000\n",
      "Epoch [24], Batch [150/391], Loss: 1.3372, LR: 0.005000\n",
      "Epoch [24], Batch [200/391], Loss: 1.3143, LR: 0.005000\n",
      "Epoch [24], Batch [250/391], Loss: 1.0722, LR: 0.005000\n",
      "Epoch [24], Batch [300/391], Loss: 1.2841, LR: 0.005000\n",
      "Epoch [24], Batch [350/391], Loss: 1.2196, LR: 0.005000\n",
      "Train set: Epoch: 24, Average loss:2.0411, LR: 0.005000 Top-1 Accuracy: 54.6500%, Top-5 Accuracy: 81.6500%, Time consumed:88.73s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|███████▎                                                                                    | 24/300 [40:47<7:38:46, 99.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Epoch: 24, Average loss:1.3345, Top-1 Accuracy: 63.1200%, Top-5 Accuracy: 88.5700%, Time consumed:10.39s\n",
      "\n",
      "EarlyStopping 카운터: 2 / 30\n",
      "Epoch [25], Batch [50/391], Loss: 1.1477, LR: 0.005000\n",
      "Epoch [25], Batch [100/391], Loss: 1.1746, LR: 0.005000\n",
      "Epoch [25], Batch [150/391], Loss: 3.2898, LR: 0.005000\n",
      "Epoch [25], Batch [200/391], Loss: 1.1942, LR: 0.005000\n",
      "Epoch [25], Batch [250/391], Loss: 3.2334, LR: 0.005000\n",
      "Epoch [25], Batch [300/391], Loss: 2.7761, LR: 0.005000\n",
      "Epoch [25], Batch [350/391], Loss: 1.2146, LR: 0.005000\n",
      "Train set: Epoch: 25, Average loss:1.9957, LR: 0.005000 Top-1 Accuracy: 55.4220%, Top-5 Accuracy: 81.8980%, Time consumed:88.63s\n",
      "Test set: Epoch: 25, Average loss:1.2015, Top-1 Accuracy: 65.7500%, Top-5 Accuracy: 90.3200%, Time consumed:10.73s\n",
      "\n",
      "새로운 최고 top-1 정확도: 65.75%, top-5 정확도: 90.32%\n",
      "새로운 최고 top-5 정확도: 90.32%\n",
      "Accuracy improved (63.87% --> 65.75%). Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|███████▋                                                                                    | 25/300 [42:26<7:37:01, 99.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [26], Batch [50/391], Loss: 3.2261, LR: 0.002500\n",
      "Epoch [26], Batch [100/391], Loss: 2.5954, LR: 0.002500\n",
      "Epoch [26], Batch [150/391], Loss: 1.1639, LR: 0.002500\n",
      "Epoch [26], Batch [200/391], Loss: 1.2031, LR: 0.002500\n",
      "Epoch [26], Batch [250/391], Loss: 3.2869, LR: 0.002500\n",
      "Epoch [26], Batch [300/391], Loss: 0.9375, LR: 0.002500\n",
      "Epoch [26], Batch [350/391], Loss: 1.1475, LR: 0.002500\n",
      "Train set: Epoch: 26, Average loss:1.9515, LR: 0.002500 Top-1 Accuracy: 57.5020%, Top-5 Accuracy: 83.2640%, Time consumed:88.59s\n",
      "Test set: Epoch: 26, Average loss:1.1582, Top-1 Accuracy: 67.5900%, Top-5 Accuracy: 90.8900%, Time consumed:9.52s\n",
      "\n",
      "새로운 최고 top-1 정확도: 67.59%, top-5 정확도: 90.89%\n",
      "새로운 최고 top-5 정확도: 90.89%\n",
      "Accuracy improved (65.75% --> 67.59%). Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|███████▉                                                                                    | 26/300 [44:05<7:33:32, 99.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [27], Batch [50/391], Loss: 1.8439, LR: 0.002500\n",
      "Epoch [27], Batch [100/391], Loss: 3.4057, LR: 0.002500\n",
      "Epoch [27], Batch [150/391], Loss: 1.1350, LR: 0.002500\n",
      "Epoch [27], Batch [200/391], Loss: 2.9566, LR: 0.002500\n",
      "Epoch [27], Batch [250/391], Loss: 3.0022, LR: 0.002500\n",
      "Epoch [27], Batch [300/391], Loss: 0.9356, LR: 0.002500\n",
      "Epoch [27], Batch [350/391], Loss: 2.6067, LR: 0.002500\n",
      "Train set: Epoch: 27, Average loss:1.8642, LR: 0.002500 Top-1 Accuracy: 59.7300%, Top-5 Accuracy: 85.0220%, Time consumed:88.73s\n",
      "Test set: Epoch: 27, Average loss:1.1270, Top-1 Accuracy: 68.4700%, Top-5 Accuracy: 91.3900%, Time consumed:9.59s\n",
      "\n",
      "새로운 최고 top-1 정확도: 68.47%, top-5 정확도: 91.39%\n",
      "새로운 최고 top-5 정확도: 91.39%\n",
      "Accuracy improved (67.59% --> 68.47%). Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|████████▎                                                                                   | 27/300 [45:43<7:30:50, 99.09s/it]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.v2 as transforms_v2  # CutMix를 위한 v2 transforms 추가\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "from tools.tool import AccuracyEarlyStopping, WarmUpLR, SAM  # 수정된 AccuracyEarlyStopping 클래스 임포트\n",
    "from models.resnet import resnet18\n",
    "\n",
    "wandb.login(key=\"ef091b9abcea3186341ddf8995d62bde62d7469e\")\n",
    "wandb.init(project=\"PBL-2\", name=\"resnet18_cfc,lr=0.005,factor=0.5,SAM_SGD,crossentropy\")  \n",
    "\n",
    "# WandB 설정 - ResNeXt 코드의 파라미터 사용하되 모델만 resnet18로 변경\n",
    "wandb.config = {\n",
    "    # 모델 설정\n",
    "    \"model\": \"resnet18\",  # 모델만 resnet18로 유지\n",
    "    \"batch_size\": 128,    # ResNeXt와 동일하게 설정\n",
    "    \"num_epochs\": 300,\n",
    "    \n",
    "    \"learning_rate\": 0.005,  # ResNeXt와 동일하게 설정\n",
    "    \"optimizer\": \"SAM_SGD\",\n",
    "    \"momentum\": 0.9,\n",
    "    \"weight_decay\": 5e-4,\n",
    "    \"nesterov\": True,\n",
    "    \n",
    "    # SAM 옵티마이저 설정\n",
    "    \"rho\": 0.05,\n",
    "    \"adaptive\": False,\n",
    "    \n",
    "    # 학습 과정 설정\n",
    "    \"seed\": 2025,\n",
    "    \"deterministic\": False,\n",
    "    \"patience\": 30,  # early stopping patience\n",
    "    \"max_epochs_wait\": float('inf'),\n",
    "    \n",
    "    # 데이터 증강 설정\n",
    "    \"cutmix_alpha\": 1.0,  # CutMix 알파 파라미터\n",
    "    \"cutmix_prob\": 0.5,   # CutMix 적용 확률\n",
    "    \"crop_padding\": 4,    # RandomCrop 패딩 크기\n",
    "    \"crop_size\": 32,      # RandomCrop 크기 (CIFAR-100 이미지 크기는 32x32)\n",
    "    \n",
    "    # 스케줄러 설정 - ResNeXt 파라미터 적용\n",
    "    \"warmup_epochs\": 10,  # ResNeXt와 동일하게 설정\n",
    "    \"lr_scheduler\": \"ReduceLROnPlateau\",\n",
    "    \"lr_factor\": 0.5,\n",
    "    \"lr_patience\": 5,\n",
    "    \"lr_threshold\": 0.1,\n",
    "    \"min_lr\": 1e-6,\n",
    "    \n",
    "    # 시스템 설정\n",
    "    \"num_workers\": 32,\n",
    "    \"pin_memory\": True,\n",
    "}\n",
    "\n",
    "wandb.config.update(config)\n",
    "\n",
    "# CIFAR-100 데이터셋 로드 - 기본 train/test 분할 사용\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(config[\"crop_size\"], padding=config[\"crop_padding\"]),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR100(\n",
    "    root='./data', train=True, download=True, transform=transform_train)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR100(\n",
    "    root='./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "# DataLoader 생성 - config 사용\n",
    "trainloader = DataLoader(\n",
    "    trainset, \n",
    "    batch_size=config[\"batch_size\"], \n",
    "    shuffle=True, \n",
    "    pin_memory=config[\"pin_memory\"], \n",
    "    num_workers=config[\"num_workers\"]\n",
    ")\n",
    "\n",
    "testloader = DataLoader(\n",
    "    testset, \n",
    "    batch_size=config[\"batch_size\"], \n",
    "    shuffle=False, \n",
    "    pin_memory=config[\"pin_memory\"], \n",
    "    num_workers=config[\"num_workers\"]\n",
    ")\n",
    "\n",
    "print(f\"Train set size: {len(trainset)}\")\n",
    "print(f\"Test set size: {len(testset)}\")\n",
    "\n",
    "# CutMix 변환 정의\n",
    "cutmix = transforms_v2.CutMix(alpha=config[\"cutmix_alpha\"], num_classes=100)  # CIFAR-100은 100개 클래스\n",
    "\n",
    "def train(model, trainloader, criterion, optimizer, device, epoch, warmup_scheduler=None, warmup_epochs=None):\n",
    "    \"\"\"\n",
    "    학습 함수 (CutMix 및 PSKD 적용)\n",
    "    \"\"\"\n",
    "    # config에서 warmup_epochs 가져오기 (None이면)\n",
    "    if warmup_epochs is None:\n",
    "        warmup_epochs = config[\"warmup_epochs\"]\n",
    "        \n",
    "    model.train()   # 모델을 학습 모드로 설정\n",
    "    start_time = time.time()  # 시간 측정 시작\n",
    "    running_loss = 0.0\n",
    "    correct_top1 = 0\n",
    "    correct_top5 = 0\n",
    "    total = 0\n",
    "    \n",
    "    for i, (inputs, labels) in enumerate(trainloader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # CutMix 확률적 적용\n",
    "        if random.random() < config[\"cutmix_prob\"]:\n",
    "            inputs, labels = cutmix(inputs, labels)\n",
    "            # 이 경우 labels은 원-핫 인코딩 형태로 변환됨\n",
    "            use_cutmix = True\n",
    "        else:\n",
    "            # 일반 레이블을 원-핫 인코딩으로 변환 (PSKD에 필요)\n",
    "            batch_size = labels.size(0)\n",
    "            one_hot_labels = torch.zeros(batch_size, 100).to(device)  # CIFAR-100은 100개 클래스\n",
    "            one_hot_labels.scatter_(1, labels.unsqueeze(1), 1)\n",
    "            use_cutmix = False\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # SAM 첫 번째 스텝을 위한 손실 계산\n",
    "        if use_cutmix:\n",
    "            # CutMix가 적용된 경우 (이미 원-핫 인코딩된 레이블)\n",
    "            loss = criterion(outputs, labels)\n",
    "        else:\n",
    "            # 일반적인 경우 (원-핫 인코딩으로 변환한 레이블)\n",
    "            loss = criterion(outputs, one_hot_labels)\n",
    "            \n",
    "        loss.backward()\n",
    "        optimizer.first_step(zero_grad=True)\n",
    "        \n",
    "        # SAM 두 번째 스텝을 위한 손실 계산\n",
    "        outputs = model(inputs)\n",
    "        if use_cutmix:\n",
    "            loss = criterion(outputs, labels)\n",
    "        else:\n",
    "            loss = criterion(outputs, one_hot_labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.second_step(zero_grad=True)\n",
    "        \n",
    "        # 학습률 스케줄러 업데이트 - warmup 스케줄러만 여기서 업데이트\n",
    "        if epoch < warmup_epochs and warmup_scheduler is not None:\n",
    "            warmup_scheduler.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # 정확도 계산 - CutMix 적용 여부에 따라 다르게 처리\n",
    "        if use_cutmix:\n",
    "            # 원-핫 인코딩된 레이블에서 argmax를 사용해 가장 큰 값의 인덱스 추출\n",
    "            _, label_idx = labels.max(1)\n",
    "        else:\n",
    "            # 정수 인덱스 레이블 그대로 사용\n",
    "            label_idx = labels\n",
    "            \n",
    "        # top-1 정확도 계산\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += inputs.size(0)\n",
    "        correct_top1 += predicted.eq(label_idx).sum().item()\n",
    "        \n",
    "        # top-5 정확도 계산\n",
    "        _, top5_idx = outputs.topk(5, 1, largest=True, sorted=True)\n",
    "        correct_top5 += sum([1 for i in range(len(label_idx)) if label_idx[i] in top5_idx[i]])\n",
    "        \n",
    "        if (i + 1) % 50 == 0:  # 50 배치마다 출력\n",
    "            print(f'Epoch [{epoch+1}], Batch [{i+1}/{len(trainloader)}], Loss: {loss.item():.4f}, LR: {optimizer.param_groups[0][\"lr\"]:.6f}')\n",
    "    \n",
    "    epoch_loss = running_loss / len(trainloader)\n",
    "    accuracy_top1 = 100.0 * correct_top1 / total\n",
    "    accuracy_top5 = 100.0 * correct_top5 / total\n",
    "    \n",
    "    train_time = time.time() - start_time\n",
    "    \n",
    "    # 학습 세트에 대한 성능 출력\n",
    "    print(f'Train set: Epoch: {epoch+1}, Average loss:{epoch_loss:.4f}, LR: {optimizer.param_groups[0][\"lr\"]:.6f} '\n",
    "          f'Top-1 Accuracy: {accuracy_top1:.4f}%, Top-5 Accuracy: {accuracy_top5:.4f}%, Time consumed:{train_time:.2f}s')\n",
    "    \n",
    "    return epoch_loss, accuracy_top1, accuracy_top5\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device, epoch, phase=\"test\"):\n",
    "    \"\"\"\n",
    "    평가 함수 (PSKD 손실 함수 적용)\n",
    "    \"\"\"\n",
    "    model.eval()  # 모델을 평가 모드로 설정\n",
    "    start_time = time.time()  # 시간 측정 시작\n",
    "    \n",
    "    eval_loss = 0.0\n",
    "    correct_top1 = 0\n",
    "    correct_top5 = 0\n",
    "    total = 0\n",
    "    \n",
    "    # 그래디언트 계산 비활성화\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # 원-핫 인코딩 변환 (PSKD에 필요)\n",
    "            batch_size = labels.size(0)\n",
    "            one_hot_labels = torch.zeros(batch_size, 100).to(device)  # CIFAR-100은 100개 클래스\n",
    "            one_hot_labels.scatter_(1, labels.unsqueeze(1), 1)\n",
    "            \n",
    "            # 순전파\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # PSKD 손실 계산\n",
    "            loss = criterion(outputs, one_hot_labels)\n",
    "            eval_loss += loss.item()\n",
    "            \n",
    "            # top-1 정확도 계산\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct_top1 += (predicted == labels).sum().item()\n",
    "            \n",
    "            # top-5 정확도 계산\n",
    "            _, top5_idx = outputs.topk(5, 1, largest=True, sorted=True)\n",
    "            correct_top5 += top5_idx.eq(labels.view(-1, 1).expand_as(top5_idx)).sum().item()\n",
    "    \n",
    "    # 평균 손실 및 정확도 계산\n",
    "    eval_loss = eval_loss / len(dataloader)\n",
    "    accuracy_top1 = 100.0 * correct_top1 / total\n",
    "    accuracy_top5 = 100.0 * correct_top5 / total\n",
    "    \n",
    "    # 평가 시간 계산\n",
    "    eval_time = time.time() - start_time\n",
    "    \n",
    "    # 테스트 세트에 대한 성능 출력\n",
    "    print(f'{phase.capitalize()} set: Epoch: {epoch+1}, Average loss:{eval_loss:.4f}, '\n",
    "          f'Top-1 Accuracy: {accuracy_top1:.4f}%, Top-5 Accuracy: {accuracy_top5:.4f}%, Time consumed:{eval_time:.2f}s')\n",
    "    print()\n",
    "    \n",
    "    return eval_loss, accuracy_top1, accuracy_top5\n",
    "\n",
    "# 메인 학습 루프\n",
    "def main_training_loop(model, trainloader, testloader, criterion, optimizer, device, num_epochs=None, patience=None, max_epochs_wait=None, warmup_scheduler=None, main_scheduler=None, warmup_epochs=None):\n",
    "    \"\"\"\n",
    "    메인 학습 루프 (accuracy 기준 early stopping) - config에서 기본값 가져오기\n",
    "    \"\"\"\n",
    "    # config에서 값 가져오기 (None이면)\n",
    "    if num_epochs is None:\n",
    "        num_epochs = config[\"num_epochs\"]\n",
    "    if patience is None:\n",
    "        patience = config[\"patience\"]\n",
    "    if max_epochs_wait is None:\n",
    "        max_epochs_wait = config[\"max_epochs_wait\"]\n",
    "    if warmup_epochs is None:\n",
    "        warmup_epochs = config[\"warmup_epochs\"]\n",
    "        \n",
    "    # 정확도 기반 얼리 스토핑 사용\n",
    "    early_stopping = AccuracyEarlyStopping(patience=patience, verbose=True, path='checkpoint.pt', max_epochs=max_epochs_wait)\n",
    "    \n",
    "    best_test_acc_top1 = 0.0\n",
    "    best_test_acc_top5 = 0.0\n",
    "    \n",
    "    # 테스트 정확도 기록을 위한 리스트\n",
    "    test_acc_top1_history = []\n",
    "    \n",
    "    # tqdm을 사용한 진행 상황 표시\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        # 학습\n",
    "        train_loss, train_acc_top1, train_acc_top5 = train(\n",
    "            model, \n",
    "            trainloader, \n",
    "            criterion, \n",
    "            optimizer, \n",
    "            device, \n",
    "            epoch, \n",
    "            warmup_scheduler, \n",
    "            warmup_epochs\n",
    "        )\n",
    "        \n",
    "        # 테스트 데이터로 평가\n",
    "        test_loss, test_acc_top1, test_acc_top5 = evaluate(model, testloader, criterion, device, epoch, phase=\"test\")\n",
    "\n",
    "        # 웜업 이후 스케줄러 업데이트 \n",
    "        if epoch >= warmup_epochs and main_scheduler is not None:\n",
    "            if isinstance(main_scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                main_scheduler.step(test_acc_top1)  # 테스트 정확도에 따라 학습률 업데이트\n",
    "            else:\n",
    "                main_scheduler.step()  # 다른 스케줄러 (예: CosineAnnealingLR)\n",
    "            \n",
    "        # 테스트 정확도 기록\n",
    "        test_acc_top1_history.append(test_acc_top1)\n",
    "        \n",
    "        # WandB에 로깅\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"learning_rate\": optimizer.param_groups[0]['lr'],\n",
    "            \"train_loss\": train_loss,\n",
    "            \"train_accuracy_top1\": train_acc_top1,\n",
    "            \"train_accuracy_top5\": train_acc_top5,\n",
    "            \"test_loss\": test_loss,\n",
    "            \"test_accuracy_top1\": test_acc_top1,\n",
    "            \"test_accuracy_top5\": test_acc_top5\n",
    "        })\n",
    "            \n",
    "        # 최고 정확도 모델 저장 (top-1 기준)\n",
    "        if test_acc_top1 > best_test_acc_top1:\n",
    "            best_test_acc_top1 = test_acc_top1\n",
    "            best_test_acc_top5_at_best_top1 = test_acc_top5\n",
    "            print(f'새로운 최고 top-1 정확도: {best_test_acc_top1:.2f}%, top-5 정확도: {best_test_acc_top5_at_best_top1:.2f}%')\n",
    "            # 모델 저장\n",
    "            model_path = f'best_model_{wandb.run.name}.pth'\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            \n",
    "            # WandB에 모델 아티팩트 저장\n",
    "            wandb.save(model_path)\n",
    "        \n",
    "        # top-5 accuracy 기록 업데이트\n",
    "        if test_acc_top5 > best_test_acc_top5:\n",
    "            best_test_acc_top5 = test_acc_top5\n",
    "            print(f'새로운 최고 top-5 정확도: {best_test_acc_top5:.2f}%')\n",
    "\n",
    "        # Early stopping 체크 (test_acc_top1 기준)\n",
    "        early_stopping(test_acc_top1, model, epoch)\n",
    "        if early_stopping.early_stop:\n",
    "            print(f\"에폭 {epoch+1}에서 학습 조기 종료. 최고 성능 에폭: {early_stopping.best_epoch+1}\")\n",
    "            break\n",
    "    \n",
    "    # 훈련 완료 후 최고 모델 로드\n",
    "    print(\"테스트 정확도 기준 최고 모델 로드 중...\")\n",
    "    model_path = f'best_model_{wandb.run.name}.pth'\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "    # 최종 테스트 세트 평가\n",
    "    final_test_loss, final_test_acc_top1, final_test_acc_top5 = evaluate(model, testloader, criterion, device, num_epochs-1, phase=\"test\")\n",
    "    \n",
    "    print(f'완료! 최고 테스트 top-1 정확도: {best_test_acc_top1:.2f}%, 최고 테스트 top-5 정확도: {best_test_acc_top5:.2f}%')\n",
    "    print(f'최종 테스트 top-1 정확도: {final_test_acc_top1:.2f}%, 최종 테스트 top-5 정확도: {final_test_acc_top5:.2f}%')\n",
    "    \n",
    "    # WandB에 최종 결과 기록\n",
    "    wandb.run.summary[\"best_test_accuracy_top1\"] = best_test_acc_top1\n",
    "    wandb.run.summary[\"best_test_accuracy_top5\"] = best_test_acc_top5\n",
    "    wandb.run.summary[\"final_test_accuracy_top1\"] = final_test_acc_top1\n",
    "    wandb.run.summary[\"final_test_accuracy_top5\"] = final_test_acc_top5\n",
    "\n",
    "    # Early stopping 정보 저장\n",
    "    if early_stopping.early_stop:\n",
    "        wandb.run.summary[\"early_stopped\"] = True\n",
    "        wandb.run.summary[\"early_stopped_epoch\"] = epoch+1\n",
    "        wandb.run.summary[\"best_epoch\"] = early_stopping.best_epoch+1\n",
    "    else:\n",
    "        wandb.run.summary[\"early_stopped\"] = False\n",
    "\n",
    "\n",
    "# 디바이스 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 모델 초기화 - config 기반\n",
    "if config[\"model\"] == \"resnet18\":\n",
    "    model = resnet18().to(device)\n",
    "else:\n",
    "    raise ValueError(f\"지원되지 않는 모델: {config['model']}\")\n",
    "\n",
    "# CrossEntropyLoss 손실 함수 설정 (두 번째 코드의 손실 함수 유지)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 옵티마이저 설정 - ResNeXt와 동일한 파라미터 적용\n",
    "if config[\"optimizer\"] == \"SAM_SGD\":\n",
    "    base_optimizer = optim.SGD\n",
    "    optimizer = SAM(\n",
    "        model.parameters(), \n",
    "        base_optimizer, \n",
    "        lr=config[\"learning_rate\"],\n",
    "        momentum=config[\"momentum\"],\n",
    "        weight_decay=config[\"weight_decay\"],\n",
    "        nesterov=config[\"nesterov\"],\n",
    "        rho=config[\"rho\"],\n",
    "        adaptive=config[\"adaptive\"]\n",
    "    )\n",
    "else:\n",
    "    raise ValueError(f\"지원되지 않는 옵티마이저: {config['optimizer']}\")\n",
    "\n",
    "# WarmUpLR 스케줄러 초기화 - ResNeXt와 동일하게 10 에폭으로 설정\n",
    "warmup_steps = config[\"warmup_epochs\"] * len(trainloader)\n",
    "warmup_scheduler = WarmUpLR(optimizer, total_iters=warmup_steps)\n",
    "\n",
    "if config[\"lr_scheduler\"] == \"ReduceLROnPlateau\":\n",
    "    main_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, \n",
    "        mode='max',\n",
    "        factor=config[\"lr_factor\"],\n",
    "        patience=config[\"lr_patience\"],\n",
    "        verbose=True,\n",
    "        threshold=config[\"lr_threshold\"],\n",
    "        min_lr=config[\"min_lr\"]\n",
    "    )\n",
    "else:\n",
    "    raise ValueError(f\"지원되지 않는 스케줄러: {config['lr_scheduler']}\")\n",
    "\n",
    "# WandB에 모델 구조 기록\n",
    "wandb.watch(model, log=\"all\")\n",
    "\n",
    "# GPU 가속 - 여러 GPU 사용\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"{torch.cuda.device_count()}개의 GPU를 사용합니다.\")\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "# 훈련 시작 시간 기록\n",
    "start_time = time.time()\n",
    "\n",
    "# 메인 학습 루프 호출 - 명시적인 파라미터 전달 대신 기본값(config에서 자동으로 가져옴) 사용\n",
    "main_training_loop(\n",
    "    model=model,\n",
    "    trainloader=trainloader,\n",
    "    testloader=testloader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    warmup_scheduler=warmup_scheduler,\n",
    "    main_scheduler=main_scheduler\n",
    ")\n",
    "\n",
    "# 훈련 종료 시간 기록 및 출력\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "wandb.log({\"total_training_time\": total_time})\n",
    "\n",
    "print(f\"전체 학습 시간: {total_time:.2f} 초\")\n",
    "\n",
    "# WandB 실행 종료\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57b9218-aa24-4297-bb92-473c215e6767",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
